{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64ace7f",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "\n",
    "For this problem, you will be implementing a Decision Tree classifier that works on discrete (categorical) features. Although a relatively simple learning algorithm, the Decision Tree is often used as a fundamental building block for more powerful (and popular) models such as Random Forest and Gradient Boosted ensembles. \n",
    "\n",
    "You should base your solution on the [ID3](https://en.wikipedia.org/wiki/ID3_algorithmhttps://en.wikipedia.org/wiki/ID3_algorithm) algorithm. This is a basic tree-learning algorithm that greedly grows a tree based on _information gain_ (reduction in entropy). Please refer to Chapter 3 of _Machine Learning_ by Tom M. Mitchell for more details. \n",
    "\n",
    "\n",
    "We have provided some skeleton code for the classifier, along with a couple of utility functions in the [decision_tree.py](./decision_tree.py) module. Please fill out the functions marked with `TODO` and feel free to add extra constructor arguments as you see fit (just make sure the default constructor solves the first dataset).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2595a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f2427",
   "metadata": {},
   "source": [
    "We begin by loading necessary packages. Below follows a short description of the imported modules:\n",
    "\n",
    "- `numpy` is the defacto python package for numerical calculation. Most other numerical libraries (including pandas) is based on numpy.\n",
    "- `pandas` is a widely used package for manipulating (mostly) tabular data\n",
    "- `decision_tree` refers to the module in this folder that should be further implemented by you\n",
    "\n",
    "Note: The `%autoreload` statement is an [IPython magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html) that automatically reloads the newest version of all imported modules within the cell. This means that you can edit the `decision_tree.py` file and just rerun this cell to get the updated version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdb81000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import decision_tree as dt  # <-- Your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97031f7",
   "metadata": {},
   "source": [
    "## [1] First Dataset\n",
    "\n",
    "The first dataset is a toy problem lifted from Table 3.2 in the Machine Learning textbook. The objective is to predict whether a given day is suitable for playing tennis based on several weather conditions. \n",
    "\n",
    "### [1.1] Load Data\n",
    "\n",
    "We begin by loading data from the .csv file located in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbdf4b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play Tennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook Temperature Humidity    Wind Play Tennis\n",
       "0      Sunny         Hot     High    Weak          No\n",
       "1      Sunny         Hot     High  Strong          No\n",
       "2   Overcast         Hot     High    Weak         Yes\n",
       "3       Rain        Mild     High    Weak         Yes\n",
       "4       Rain        Cool   Normal    Weak         Yes\n",
       "5       Rain        Cool   Normal  Strong          No\n",
       "6   Overcast        Cool   Normal  Strong         Yes\n",
       "7      Sunny        Mild     High    Weak          No\n",
       "8      Sunny        Cool   Normal    Weak         Yes\n",
       "9       Rain        Mild   Normal    Weak         Yes\n",
       "10     Sunny        Mild   Normal  Strong         Yes\n",
       "11  Overcast        Mild     High  Strong         Yes\n",
       "12  Overcast         Hot   Normal    Weak         Yes\n",
       "13      Rain        Mild     High  Strong          No"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = pd.read_csv('data_1.csv')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a43ffe9",
   "metadata": {},
   "source": [
    "### [1.2] Fit and Evaluate Model\n",
    "\n",
    "Next we fit and evaluate a Decision Tree over the dataset. We first partition the data into the dependent (`y` = Play Tennis) and independent (`X` = everything else) variables. We then initialize a Decision Tree learner and fit it to all the data. Finally, we evaluate the model over the same data by calculating its accuracy, i.e. the fraction of correctly classified samples.\n",
    "\n",
    "Note that `.fit` and `.predict` will crash until you implement these two methods in [decision_tree.py](./decision_tree.py).\n",
    "\n",
    "Assuming that you've correctly implemented the ID3 algorithm as described in the course textbook, you should expect the model to perfectly fit the training data. That is, you should get a classification accuracy of 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "524486cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D470> <decision_tree.TreeNode object at 0x0000012D9650D438> <decision_tree.TreeNode object at 0x0000012D9650D3C8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D470> <decision_tree.TreeNode object at 0x0000012D9650D438> <decision_tree.TreeNode object at 0x0000012D9650D3C8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D470> <decision_tree.TreeNode object at 0x0000012D9650D438> <decision_tree.TreeNode object at 0x0000012D9650D3C8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D3C8> <decision_tree.TreeNode object at 0x0000012D9650D320> <decision_tree.TreeNode object at 0x0000012D9650D2E8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D630> <decision_tree.TreeNode object at 0x0000012D9650D5F8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D630> <decision_tree.TreeNode object at 0x0000012D9650D5F8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D5F8> <decision_tree.TreeNode object at 0x0000012D9650D5C0> <decision_tree.TreeNode object at 0x0000012D9650D588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D470> <decision_tree.TreeNode object at 0x0000012D9650D438> <decision_tree.TreeNode object at 0x0000012D9650D3C8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D630> <decision_tree.TreeNode object at 0x0000012D9650D5F8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D630> <decision_tree.TreeNode object at 0x0000012D9650D5F8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D630> <decision_tree.TreeNode object at 0x0000012D9650D5F8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D5F8> <decision_tree.TreeNode object at 0x0000012D9650D5C0> <decision_tree.TreeNode object at 0x0000012D9650D588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6D8> <decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D943BC588>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D6A0> <decision_tree.TreeNode object at 0x0000012D9650D668> <decision_tree.TreeNode object at 0x0000012D9650D470>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D470> <decision_tree.TreeNode object at 0x0000012D9650D438> <decision_tree.TreeNode object at 0x0000012D9650D3C8>\n",
      "<decision_tree.TreeNode object at 0x0000012D9650D3C8> <decision_tree.TreeNode object at 0x0000012D9650D320> <decision_tree.TreeNode object at 0x0000012D9650D2E8>\n",
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Separate independent (X) and dependent (y) variables\n",
    "X = data_1.drop(columns=['Play Tennis'])\n",
    "y = data_1['Play Tennis']\n",
    "\n",
    "# Create and fit a Decrision Tree classifier\n",
    "model_1 = dt.DecisionTree()  # <-- Should work with default constructor\n",
    "model_1.fit(X, y)\n",
    "\n",
    "# Verify that it perfectly fits the training set\n",
    "print(f'Accuracy: {dt.accuracy(y_true=y, y_pred=model_1.predict(X)) * 100 :.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a8df96",
   "metadata": {},
   "source": [
    "### [1.3] Inspect Classification Rules\n",
    "\n",
    "A big advantage of Decision Trees is that they are relatively transparent learners. By this we mean that it is easy for an outside observer to analyse and understand how the model makes its decisions. The problem of being able to reason about how a machine learning model reasons is known as _Explainable AI_ and is often a desirable property of machine learning systems.\n",
    "\n",
    "Every time a Decision Tree is evaluated, the datapoint is compared against a set of nodes starting at the root of the tree and (typically) ending at one of the leaf nodes. An equivalent way to view this reasoning is as an implication rule ($A \\rightarrow B$) where the antecedent ($A$) is a conjunction of of attribute values and the consequent ($B$) is the predicted label. For instance, if a path down the tree first checks if Outlook=Rain, then checks if Wind=Strong, and then predicts Play Tennis=No, this line of reasoning can be represented as:\n",
    "\n",
    "- If $Outlook=Rain \\cap Wind=Strong \\rightarrow$ then predict $Play Tennis = No$\n",
    "\n",
    "We will leverage this property to export the decision tree you just created as a set of rules. For the subsequent cell to work, you must also have implemented the `.get_rules()` method in the provided boilerplate code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b66149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Humidity=High ∩ Outlook=Sunny => No\n",
      "✅ Humidity=High ∩ Outlook=Rain ∩ Wind=Weak => Yes\n",
      "✅ Humidity=Normal ∩ Wind=Weak => Yes\n",
      "❌ Humidity=Normal ∩ Wind=Strong ∩ Outlook=Rain => No\n",
      "✅ Humidity=Normal ∩ Wind=Strong ∩ Outlook=Sunny => Yes\n",
      "❌ Humidity=High ∩ Outlook=Rain ∩ Wind=Strong => No\n"
     ]
    }
   ],
   "source": [
    "for rules, label in model_1.get_rules():\n",
    "    conjunction = ' ∩ '.join(f'{attr}={value}' for attr, value in rules)\n",
    "    print(f'{\"✅\" if label == \"Yes\" else \"❌\"} {conjunction} => {label}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fe7e14",
   "metadata": {},
   "source": [
    "## [2] Second Dataset\n",
    "\n",
    "The second dataset involves predicting whether an investment opportunity will result in a successful `Outcome` or not. To make this prediction, you are given a dataset of 200 historical$^1$ business ventures and their outcome, along with the following observed features:\n",
    "\n",
    "- Whether the business oportunity is in a lucurative market or not \n",
    "- Whether the presented business idea has a competitive advantage\n",
    "- Whether the second opinion from another investor is positive or not \n",
    "- The founder's previous experience with startups\n",
    "- The founder's [Zodiac Sign](https://en.wikipedia.org/wiki/Astrologyhttps://en.wikipedia.org/wiki/Astrology)\n",
    "\n",
    "---\n",
    "[1] Disclaimer: The dataset is not based on real-world business ventures. It is synthetic and generated by us. Also, it should not be considered financial advice.\n",
    "\n",
    "### [2.1] Load Data\n",
    "\n",
    "This dataset can also be found in a .csv file in the same folder as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6782e477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Founder Zodiac</th>\n",
       "      <th>Founder Experience</th>\n",
       "      <th>Second Opinion</th>\n",
       "      <th>Competitive Advantage</th>\n",
       "      <th>Lucurative Market</th>\n",
       "      <th>Outcome</th>\n",
       "      <th>Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cancer</td>\n",
       "      <td>moderate</td>\n",
       "      <td>negative</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>success</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancer</td>\n",
       "      <td>high</td>\n",
       "      <td>positive</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>failure</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>scorpio</td>\n",
       "      <td>low</td>\n",
       "      <td>negative</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>failure</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cancer</td>\n",
       "      <td>low</td>\n",
       "      <td>negative</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>failure</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aquarius</td>\n",
       "      <td>low</td>\n",
       "      <td>positive</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>success</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>capricorn</td>\n",
       "      <td>moderate</td>\n",
       "      <td>positive</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>failure</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>aquarius</td>\n",
       "      <td>low</td>\n",
       "      <td>negative</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>failure</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>cancer</td>\n",
       "      <td>moderate</td>\n",
       "      <td>negative</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>failure</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>virgo</td>\n",
       "      <td>moderate</td>\n",
       "      <td>negative</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>failure</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>capricorn</td>\n",
       "      <td>moderate</td>\n",
       "      <td>negative</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>success</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Founder Zodiac Founder Experience Second Opinion Competitive Advantage  \\\n",
       "0           cancer           moderate       negative                   yes   \n",
       "1           cancer               high       positive                   yes   \n",
       "2          scorpio                low       negative                    no   \n",
       "3           cancer                low       negative                    no   \n",
       "4         aquarius                low       positive                   yes   \n",
       "..             ...                ...            ...                   ...   \n",
       "195      capricorn           moderate       positive                    no   \n",
       "196       aquarius                low       negative                    no   \n",
       "197         cancer           moderate       negative                    no   \n",
       "198          virgo           moderate       negative                    no   \n",
       "199      capricorn           moderate       negative                   yes   \n",
       "\n",
       "    Lucurative Market  Outcome  Split  \n",
       "0                  no  success  train  \n",
       "1                  no  failure  train  \n",
       "2                  no  failure  train  \n",
       "3                  no  failure  train  \n",
       "4                 yes  success  train  \n",
       "..                ...      ...    ...  \n",
       "195               yes  failure   test  \n",
       "196               yes  failure   test  \n",
       "197               yes  failure   test  \n",
       "198                no  failure   test  \n",
       "199                no  success   test  \n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = pd.read_csv('data_2.csv')\n",
    "data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95680e85",
   "metadata": {},
   "source": [
    "### [2.2] Split Data\n",
    "\n",
    "We've also taken the liberty to pre-split the dataset into three different sets:\n",
    "\n",
    "- `train` contains 50 samples that you should use to generate the tree\n",
    "- `valid` contains 50 samples that you can use to evaluate different preprocessing methods and variations to the tree-learning algorithm.\n",
    "- `test` contains 100 samples and should only be used to evaluate the final model once you're done experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad217f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test     100\n",
       "valid     50\n",
       "train     50\n",
       "Name: Split, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2_train = data_2.query('Split == \"train\"')\n",
    "data_2_valid = data_2.query('Split == \"valid\"')\n",
    "data_2_test = data_2.query('Split == \"test\"')\n",
    "X_train, y_train = data_2_train.drop(columns=[\"Founder Zodiac\", 'Outcome', 'Split']), data_2_train.Outcome\n",
    "X_valid, y_valid = data_2_valid.drop(columns=[\"Founder Zodiac\",'Outcome', 'Split']), data_2_valid.Outcome\n",
    "X_test, y_test = data_2_test.drop(columns=[\"Founder Zodiac\", 'Outcome', 'Split']), data_2_test.Outcome\n",
    "\n",
    "\n",
    "\n",
    "#X_train, y_train = data_2_train.drop(columns=['Outcome', 'Split']), data_2_train.Outcome\n",
    "#X_valid, y_valid = data_2_valid.drop(columns=['Outcome', 'Split']), data_2_valid.Outcome\n",
    "#X_test, y_test = data_2_test.drop(columns=['Outcome', 'Split']), data_2_test.Outcome\n",
    "#Let us be real, we dont really believe that Zodiac signs do something for the success of business ventures.\n",
    "\n",
    "\n",
    "\n",
    "data_2.Split.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a3efc",
   "metadata": {},
   "source": [
    "### [2.3] Fit and Evaluate Model\n",
    "\n",
    "You may notice that the basic ID3 algorithm you developed for the first dataset does not generalize well when applied straight to this problem. Feel free to add extra functionality to it and/or the data preprocessing pipeline that might improve performance on the validation (and ultimately test set). As a debugging reference; it is highly possible to obtain accuracies over the validation and test set ranging from mid ~80% to low ~90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b41951e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 86.0%\n",
      "Valid: 72.0%\n",
      "Test: 62.0%\n"
     ]
    }
   ],
   "source": [
    "# Fit model (TO TRAIN SET ONLY)\n",
    "%autoreload\n",
    "model_2 = dt.DecisionTree(max_tree_depth=200)  # <-- Feel free to add hyperparameters \n",
    "model_2.fit(X_train, y_train)\n",
    "\n",
    "print(f'Train: {dt.accuracy(y_train, model_2.predict(X_train)) * 100 :.1f}%')\n",
    "print(f'Valid: {dt.accuracy(y_valid, model_2.predict(X_valid)) * 100 :.1f}%')\n",
    "print(f'Test: {dt.accuracy(y_test, model_2.predict(X_test)) * 100 :.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "3ebd7bc7-82ac-484b-9664-6789b3dccae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 92.0%\n",
      "Valid: 86.0%\n",
      "Test: 81.0%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "# Define calculate_entropy function to make things easier\n",
    "def calculate_entropy(labels):\n",
    "    n_labels = len(labels)\n",
    "    if n_labels <= 1:\n",
    "        return 0\n",
    "    value, counts = np.unique(labels, return_counts=True)\n",
    "    probs = counts/n_labels\n",
    "    n_classes = len(value)\n",
    "    if n_classes <= 1:\n",
    "        return 0\n",
    "    entropy = 0\n",
    "    for i in probs:\n",
    "        entropy -= i*log(i,2)\n",
    "    return entropy\n",
    "\n",
    "def split_by_feature(X, feature_idx, threshold):\n",
    "    # if the feature is numerical\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        X_true = X[X.iloc[:,feature_idx] >= threshold]\n",
    "        X_false = X[~X.iloc[:,feature_idx] >= threshold]\n",
    "    # if the feature is categorical\n",
    "    else:\n",
    "        X_true = X[X.iloc[:,feature_idx] == threshold]\n",
    "        X_false = X[~(X.iloc[:,feature_idx] == threshold)]\n",
    "    return X_true, X_false\n",
    "\n",
    "class DecisionNode():\n",
    "    def __init__(self, feature_idx=None, threshold=None, value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_idx = feature_idx # index of the feature that is used\n",
    "        self.threshold = threshold # threshold value for feature when making the decision\n",
    "        self.value = value # value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch # the node we go to if decision returns True\n",
    "        self.false_branch = false_branch # the node we go to if decision returns False\n",
    "        \n",
    "class DecisionTree():\n",
    "    def __init__(self, min_info_gain=1e-7, max_depth=float(\"inf\")):\n",
    "        self.root = None # root of this tree\n",
    "        self.min_info_gain = min_info_gain # minimum information gain to allow splitting\n",
    "        self.max_depth = max_depth # maximum depth the tree grows to\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "    def build_tree(self, X, y, current_depth=0):\n",
    "        decision = None\n",
    "        subtrees = None\n",
    "        largest_info_gain = 0\n",
    "        # add y as last column of X\n",
    "        df = pd.concat((X, y), axis=1)\n",
    "        n_rows, n_features = X.shape\n",
    "        if current_depth <= self.max_depth:\n",
    "            # iterate through every feature\n",
    "            for feature_idx in range(n_features):\n",
    "                # values of that column\n",
    "                feature_values = X.iloc[:, feature_idx]\n",
    "                unique_values = feature_values.unique()\n",
    "                for threshold in unique_values:\n",
    "                    X_true, X_false = split_by_feature(df, feature_idx, threshold)\n",
    "                    if len(X_true) > 0 and len(X_false) > 0:\n",
    "                        y_true = X_true.iloc[:,-1]\n",
    "                        y_false = X_false.iloc[:,-1]\n",
    "                        # Calculate impurity\n",
    "                        info_gain = self.calculate_information_gain(y, y_true, y_false)\n",
    "                        # Keep track of which feature gave the largest information gain\n",
    "                        if info_gain > largest_info_gain:\n",
    "                            largest_info_gain = info_gain\n",
    "                            decision = {\"feature_idx\":feature_idx, \"threshold\":threshold}\n",
    "                            subtrees = {\"X_true\":X_true.iloc[:,:-1],\n",
    "                                        \"y_true\":y_true,\n",
    "                                        \"X_false\":X_false.iloc[:,:-1],\n",
    "                                        \"y_false\":y_false}\n",
    "                            \n",
    "                  \n",
    "        # we will construct new branch if the information gain is larger than minimum information gain that we've defined\n",
    "        \n",
    "        if largest_info_gain > self.min_info_gain:\n",
    "            true_branch = self.build_tree(subtrees[\"X_true\"], subtrees[\"y_true\"], current_depth+1)\n",
    "            false_branch = self.build_tree(subtrees[\"X_false\"], subtrees[\"y_false\"], current_depth+1)\n",
    "            return DecisionNode(feature_idx=decision[\"feature_idx\"], threshold=decision[\"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # at leaf\n",
    "        leaf_value = self.majority_vote(y)\n",
    "        return DecisionNode(value=leaf_value)\n",
    "                        \n",
    "    def calculate_information_gain(self, y, y_true, y_false):\n",
    "        # probability of choosing left subtree \n",
    "        p = len(y_true) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p*calculate_entropy(y_true) - (1-p)*calculate_entropy(y_false)\n",
    "        return info_gain\n",
    "                \n",
    "    def majority_vote(self, y):\n",
    "        # this is for calculating values for the leaf nodes\n",
    "        return y.value_counts().idxmax()\n",
    "                \n",
    "    def predict_value(self, x, tree=None):\n",
    "        # recursive method to find the leaf node that corresponds to prediction\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature_idx]\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "        return self.predict_value(x, branch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for idx, row in X.iterrows():\n",
    "            y_pred.append(self.predict_value(row.values))\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "    \n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes discrete classification accuracy\n",
    "    \n",
    "    Args:\n",
    "        y_true (array<m>): a length m vector of ground truth labels\n",
    "        y_pred (array<m>): a length m vector of predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        The average number of correct predictions\n",
    "    \"\"\"\n",
    "    return (y_true == y_pred).mean()\n",
    "\n",
    "tree = DecisionTree(max_depth=5)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print(f'Train: {accuracy(y_train, tree.predict(X_train)) * 100 :.1f}%')\n",
    "print(f'Valid: {accuracy(y_valid, tree.predict(X_valid)) * 100 :.1f}%')\n",
    "print(f'Test: {accuracy(y_test, tree.predict(X_test)) * 100 :.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d31985-b312-45d6-8e28-a774f7b1bd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decccb1d",
   "metadata": {},
   "source": [
    "## [3] Further steps (optional)\n",
    "\n",
    "If you're done with the assignment but want to some more challenges; consider the following:\n",
    "\n",
    "- Make a Decision Tree learner that can handle numerical attributes\n",
    "- Make a Decision Tree learner that can handle numerical targets (regresion tree)\n",
    "- Try implementing [Random Forest](https://en.wikipedia.org/wiki/Random_forest) on top of your Decision Tree algorithm\n",
    "\n",
    "If you need more data for experimenting, UC Irvine hosts a [large repository](https://archive.ics.uci.edu/ml/datasets.php) of machine learning datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "9b7b45a9-152d-4626-9798-1ec0704d641f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Params: {'criterion': 'gini', 'max_depth': 10}\n",
      "Train: 92.0%\n",
      "Valid: 86.0%\n",
      "Test: 81.0%\n"
     ]
    }
   ],
   "source": [
    "#SKLEARN METHOD TO PURSUE:\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "data_2_train = data_2.query('Split == \"train\"')\n",
    "data_2_valid = data_2.query('Split == \"valid\"')\n",
    "data_2_test = data_2.query('Split == \"test\"')\n",
    "\n",
    "data_2_train = np.array(data_2_train)\n",
    "data_2_valid = np.array(data_2_valid)\n",
    "data_2_test = np.array(data_2_test)\n",
    "\n",
    "#Sklearn only apparently takes one-hot encoded categories into tree methods\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in range(6):\n",
    "    data_2_train[:,i] = le.fit_transform(data_2_train[:,i])\n",
    "    data_2_valid[:,i] = le.fit_transform(data_2_valid[:,i])\n",
    "    data_2_test[:,i] = le.fit_transform(data_2_test[:,i])\n",
    "\n",
    "\n",
    "data_2_train = pd.DataFrame(data_2_train)\n",
    "data_2_valid = pd.DataFrame(data_2_valid)\n",
    "data_2_test = pd.DataFrame(data_2_test)\n",
    "\n",
    "\n",
    "X_train, y_train = data_2_train.drop(columns=[0,5,6]), data_2_train[5]\n",
    "X_valid, y_valid = data_2_valid.drop(columns=[0,5,6]), data_2_valid[5]\n",
    "X_test, y_test = data_2_test.drop(columns=[0,5,6]), data_2_test[5]\n",
    "\n",
    "\n",
    "\n",
    "#GridSearch of Tree Classifier Hyper Parameters\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def dtree_grid_search(X,y,nfolds):\n",
    "    #create a dictionary of all values we want to test\n",
    "    param_grid = { 'criterion':['gini','entropy'],'max_depth': [10,20,30,40,50,100]}\n",
    "    # decision tree model\n",
    "    dtree_model=DecisionTreeClassifier()\n",
    "    #use gridsearch to test all values\n",
    "    dtree_gscv = GridSearchCV(dtree_model, param_grid, cv=nfolds)\n",
    "    #fit model to data\n",
    "    dtree_gscv.fit(X, y)\n",
    "    return dtree_gscv.best_params_\n",
    "\n",
    "best_params = dtree_grid_search(X_train,y_train.astype(\"int\"),5)\n",
    "print(\"Best Params:\", best_params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fit model (TO TRAIN SET ONLY)\n",
    "model_3 = DecisionTreeClassifier(**best_params)  # <-- Feel free to add hyperparameters \n",
    "\n",
    "model_3.fit(X_train, y_train.astype('int'))\n",
    "\n",
    "print(f'Train: {dt.accuracy(y_train, model_3.predict(np.array(X_train))) * 100 :.1f}%')\n",
    "print(f'Valid: {dt.accuracy(y_valid, model_3.predict(np.array(X_valid))) * 100 :.1f}%')\n",
    "print(f'Test: {dt.accuracy(y_test, model_3.predict(np.array(X_test))) * 100 :.1f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "81288aa9-257e-414a-8948-34e131a9a65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 92.0%\n",
      "Valid: 86.0%\n",
      "Test: 81.0%\n",
      "Best Criterion: gini\n",
      "Best max_depth: 3\n",
      "Best Number Of Components: 1\n",
      "\n",
      "DecisionTreeClassifier(max_depth=3)\n"
     ]
    }
   ],
   "source": [
    "#Tring Pipeline with GridSearchCV\n",
    "from sklearn import decomposition, datasets\n",
    "from sklearn import tree\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Using pipeline and GridSearch to find best solution\n",
    "std_slc = StandardScaler()\n",
    "\n",
    "pca = decomposition.PCA()\n",
    "\n",
    "dec_tree = tree.DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "pipe = Pipeline(steps=[('std_slc', std_slc),\n",
    "                           ('pca', pca),\n",
    "                           ('dec_tree', dec_tree)])\n",
    "\n",
    "\n",
    "n_components = list(range(1,X.shape[1]+1,1))\n",
    "\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [3,4,5,6,7,8,9,10]\n",
    "\n",
    "\n",
    "\n",
    "parameters = dict(pca__n_components=n_components,\n",
    "                      dec_tree__criterion=criterion,\n",
    "                      dec_tree__max_depth=max_depth)\n",
    "\n",
    "\n",
    "\n",
    "clf_GS = GridSearchCV(pipe, parameters)\n",
    "clf_GS.fit(X_train, y_train.astype('int'))\n",
    "\n",
    "model_3 = DecisionTreeClassifier(criterion=\"gini\", max_depth=8)  # <-- Feel free to add hyperparameters \n",
    "\n",
    "model_3.fit(X_train, y_train.astype('int'))\n",
    "\n",
    "print(f'Train: {dt.accuracy(y_train, model_3.predict(np.array(X_train))) * 100 :.1f}%')\n",
    "print(f'Valid: {dt.accuracy(y_valid, model_3.predict(np.array(X_valid))) * 100 :.1f}%')\n",
    "print(f'Test: {dt.accuracy(y_test, model_3.predict(np.array(X_test))) * 100 :.1f}%')\n",
    "\n",
    "\n",
    "\n",
    "print('Best Criterion:', clf_GS.best_estimator_.get_params()['dec_tree__criterion'])\n",
    "print('Best max_depth:', clf_GS.best_estimator_.get_params()['dec_tree__max_depth'])\n",
    "print('Best Number Of Components:', clf_GS.best_estimator_.get_params()['pca__n_components'])\n",
    "print(); print(clf_GS.best_estimator_.get_params()['dec_tree'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "e8301b14-d2fd-4c4c-a05a-0a8dabf38a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 92.0%\n",
      "Valid: 82.0%\n",
      "Test: 76.0%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #Random forest did a little worse?\n",
    "\n",
    "    \n",
    "# Fit model (TO TRAIN SET ONLY)\n",
    "model_3 = RandomForestClassifier(criterion=\"gini\")  # <-- Feel free to add hyperparameters \n",
    "\n",
    "model_3.fit(np.array(X_train), np.array(y_train).astype(\"int\"))\n",
    "\n",
    "print(f'Train: {dt.accuracy(y_train, model_3.predict(np.array(X_train))) * 100 :.1f}%')\n",
    "print(f'Valid: {dt.accuracy(y_valid, model_3.predict(np.array(X_valid))) * 100 :.1f}%')\n",
    "print(f'Test: {dt.accuracy(y_test, model_3.predict(np.array(X_test))) * 100 :.1f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
