{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96522da-cfef-4e66-a0eb-809938aa90dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play Tennis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook Temperature Humidity    Wind Play Tennis\n",
       "0      Sunny         Hot     High    Weak          No\n",
       "1      Sunny         Hot     High  Strong          No\n",
       "2   Overcast         Hot     High    Weak         Yes\n",
       "3       Rain        Mild     High    Weak         Yes\n",
       "4       Rain        Cool   Normal    Weak         Yes\n",
       "5       Rain        Cool   Normal  Strong          No\n",
       "6   Overcast        Cool   Normal  Strong         Yes\n",
       "7      Sunny        Mild     High    Weak          No\n",
       "8      Sunny        Cool   Normal    Weak         Yes\n",
       "9       Rain        Mild   Normal    Weak         Yes\n",
       "10     Sunny        Mild   Normal  Strong         Yes\n",
       "11  Overcast        Mild     High  Strong         Yes\n",
       "12  Overcast         Hot   Normal    Weak         Yes\n",
       "13      Rain        Mild     High  Strong          No"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "data_1 = pd.read_csv('data_1.csv')\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c6fc3f35-58be-4c52-ab5f-3f820f44f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "\n",
    "# Define calculate_entropy function to make things easier\n",
    "def entropy(counts):\n",
    "    \"\"\"\n",
    "    Computes the entropy of a partitioning\n",
    "    \n",
    "    Args:\n",
    "        counts (array<k>): a lenth k int array >= 0. For instance,\n",
    "            an array [3, 4, 1] implies that you have a total of 8\n",
    "            datapoints where 3 are in the first group, 4 in the second,\n",
    "            and 1 one in the last. This will result in entropy > 0.\n",
    "            In contrast, a perfect partitioning like [8, 0, 0] will\n",
    "            result in a (minimal) entropy of 0.0\n",
    "            \n",
    "    Returns:\n",
    "        A positive float scalar corresponding to the (log2) entropy\n",
    "        of the partitioning.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert (counts >= 0).all()\n",
    "    probs = counts / counts.sum()\n",
    "    probs = probs[probs > 0]  # Avoid log(0)\n",
    "    return - np.sum(probs * np.log2(probs))\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def split_by_feature(X, feature_idx, threshold):\n",
    "    # if the feature is numerical\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        X_true = X[X.iloc[:,feature_idx] >= threshold]\n",
    "        X_false = X[~X.iloc[:,feature_idx] >= threshold]\n",
    "    # if the feature is categorical\n",
    "    else:\n",
    "        X_true = X[X.iloc[:,feature_idx] == threshold]\n",
    "        X_false = X[~(X.iloc[:,feature_idx] == threshold)]\n",
    "        \n",
    "    return X_true, X_false\n",
    "\n",
    "\n",
    "\n",
    "class DecisionNode():\n",
    "    def __init__(self, feature_idx=None, threshold=None, value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_idx = feature_idx # index of the feature that is used\n",
    "        self.threshold = threshold # threshold value for feature when making the decision\n",
    "        self.value = value # value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch # the node we go to if decision returns True\n",
    "        self.false_branch = false_branch # the node we go to if decision returns False\n",
    "        \n",
    "class DecisionTree():\n",
    "    def __init__(self, min_info_gain=1e-7, max_depth=float(\"inf\")):\n",
    "        self.root = None # root of this tree\n",
    "        self.min_info_gain = min_info_gain # minimum information gain to allow splitting\n",
    "        self.max_depth = max_depth # maximum depth the tree grows to\n",
    "    def fit(self, X, y):\n",
    "        self.root = self.build_tree(X, y)\n",
    "    def build_tree(self, X, y, current_depth=0):\n",
    "        decision = None\n",
    "        subtrees = None\n",
    "        largest_info_gain = 0\n",
    "        # add y as last column of X\n",
    "        df = pd.concat((X, y), axis=1)\n",
    "        n_rows, n_features = X.shape\n",
    "        if current_depth <= self.max_depth:\n",
    "            # iterate through every feature\n",
    "            for feature_idx in range(n_features):\n",
    "                # values of that column\n",
    "                feature_values = X.iloc[:, feature_idx]\n",
    "                unique_values = feature_values.unique()\n",
    "                for threshold in unique_values:\n",
    "                    X_true, X_false = split_by_feature(df, feature_idx, threshold)\n",
    "                    if len(X_true) > 0 and len(X_false) > 0:\n",
    "                        y_true = X_true.iloc[:,-1]\n",
    "                        y_false = X_false.iloc[:,-1]\n",
    "                        # Calculate impurity\n",
    "                        info_gain = self.calculate_information_gain(y, y_true, y_false)\n",
    "                        # Keep track of which feature gave the largest information gain\n",
    "                        if info_gain > largest_info_gain:\n",
    "                            largest_info_gain = info_gain\n",
    "                            decision = {\"feature_idx\":feature_idx, \"threshold\":threshold}\n",
    "                            subtrees = {\"X_true\":X_true.iloc[:,:-1],\n",
    "                                        \"y_true\":y_true,\n",
    "                                        \"X_false\":X_false.iloc[:,:-1],\n",
    "                                        \"y_false\":y_false}\n",
    "        # we will construct new branch if the information gain is larger than minimum information gain that we've defined\n",
    "        if largest_info_gain > self.min_info_gain:\n",
    "            true_branch = self.build_tree(subtrees[\"X_true\"], subtrees[\"y_true\"], current_depth+1)\n",
    "            false_branch = self.build_tree(subtrees[\"X_false\"], subtrees[\"y_false\"], current_depth+1)\n",
    "            return DecisionNode(feature_idx=decision[\"feature_idx\"], threshold=decision[\"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # at leaf\n",
    "        leaf_value = self.majority_vote(y)\n",
    "        return DecisionNode(value=leaf_value)\n",
    "                        \n",
    "    def calculate_information_gain(self, y, y_true, y_false):\n",
    "        # probability of choosing left subtree \n",
    "        p = len(y_true) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p*calculate_entropy(y_true) - (1-p)*calculate_entropy(y_false)\n",
    "        return info_gain\n",
    "                \n",
    "    def majority_vote(self, y):\n",
    "        # this is for calculating values for the leaf nodes\n",
    "        return y.value_counts().idxmax()\n",
    "                \n",
    "    def predict_value(self, x, tree=None):\n",
    "        # recursive method to find the leaf node that corresponds to prediction\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        feature_value = x[tree.feature_idx]\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "        return self.predict_value(x, branch)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for idx, row in X.iterrows():\n",
    "            y_pred.append(self.predict_value(row.values))\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes discrete classification accuracy\n",
    "    \n",
    "    Args:\n",
    "        y_true (array<m>): a length m vector of ground truth labels\n",
    "        y_pred (array<m>): a length m vector of predicted labels\n",
    "        \n",
    "    Returns:\n",
    "        The average number of correct predictions\n",
    "    \"\"\"\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    return (y_true == y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67a46452-b953-4a58-9601-1baaa97b6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Separate independent (X) and dependent (y) variables\n",
    "X = data_1.drop(columns=['Play Tennis'])\n",
    "y = data_1['Play Tennis']\n",
    "\n",
    "# Create and fit a Decrision Tree classifier\n",
    "model_1 = DecisionTree()  # <-- Should work with default constructor\n",
    "model_1.fit(X, y)\n",
    "\n",
    "# Verify that it perfectly fits the training set\n",
    "print(f'Accuracy: {accuracy(y_true=np.array(y), y_pred=np.array(model_1.predict(X))) * 100 :.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e341d1c2-8f58-4993-845a-d66e74b261cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#My attempt at an implementation:\n",
    "class TreeNode():\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None): #asterix to force caller to explictily write value=\"something\"\n",
    "        self.feature   = feature\n",
    "        self.threshold = threshold\n",
    "        self.left      = left\n",
    "        self.right     = right\n",
    "        self.value     = value\n",
    "        \n",
    "    def isLeafNode(self):\n",
    "        return self.value is not None\n",
    "    \n",
    "class DecisionTree():\n",
    "    def __init__(self, max_tree_depth=np.inf):\n",
    "        self.root = None\n",
    "        self.max_tree_depth = max_tree_depth\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Generates a decision tree for classification\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): a matrix with discrete value where\n",
    "                each row is a sample and the columns correspond\n",
    "                to the features.\n",
    "            y (pd.Series): a vector of discrete ground-truth labels\n",
    "        \"\"\"\n",
    "        #start at the top node and at each node select the best split based on the best\n",
    "        #information gain\n",
    "        \n",
    "        #Greedy search: Loop over all features and over all thresholds and\n",
    "        #save the best split feature and split threshold at each node\n",
    "        \n",
    "        #Build the tree recursively\n",
    "        \n",
    "        #apply some stopping criteria to stop growing:\n",
    "        #For example, maximum depth, minimum samples at node, no more class\n",
    "        #distribution in node\n",
    "        \n",
    "        #when we have a leaf node store the most frequent class label of this node\n",
    "        \n",
    "        \n",
    "        \n",
    "        # TODO: Implement \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Generates predictions\n",
    "        \n",
    "        Note: should be called after .fit()\n",
    "        \n",
    "        Args:\n",
    "            X (pd.DataFrame): an mxn discrete matrix where\n",
    "                each row is a sample and the columns correspond\n",
    "                to the features.\n",
    "            \n",
    "        Returns:\n",
    "            A length m vector with predictions\n",
    "        \"\"\"\n",
    "        \n",
    "        #Traverse the tree recursively\n",
    "        #At each node look at the best split feature of the test feature vector x\n",
    "        #and go left or right depending on x[feature_idx] <= threshold\n",
    "        \n",
    "        #when we reach the leaf node we return the stored most common class label\n",
    "        \n",
    "        \n",
    "        # TODO: Implement \n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "def entropy(counts):\n",
    "    \"\"\"\n",
    "    Computes the entropy of a partitioning\n",
    "    \n",
    "    Args:\n",
    "        counts (array<k>): a lenth k int array >= 0. For instance,\n",
    "            an array [3, 4, 1] implies that you have a total of 8\n",
    "            datapoints where 3 are in the first group, 4 in the second,\n",
    "            and 1 one in the last. This will result in entropy > 0.\n",
    "            In contrast, a perfect partitioning like [8, 0, 0] will\n",
    "            result in a (minimal) entropy of 0.0\n",
    "            \n",
    "    Returns:\n",
    "        A positive float scalar corresponding to the (log2) entropy\n",
    "        of the partitioning.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert (counts >= 0).all()\n",
    "    probs = counts / counts.sum()\n",
    "    probs = probs[probs > 0]  # Avoid log(0)\n",
    "    return - np.sum(probs * np.log2(probs))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
