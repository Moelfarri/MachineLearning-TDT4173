{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group 14\n",
    "## Competition Name : Moscow Housing\n",
    "## Elias Elfarri    , ID: 473700\n",
    "## Nora Valen       , ID: 490606\n",
    "## Muhammad Sarmad  , ID: 190729"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "1. [Introduction](#1)\n",
    "1. [Importing Libraries and Reading the Dataset](#2)\n",
    "1. [Exploratory Data Analysis](#3) \n",
    "    * [3.1 General Property Valuation Domain Knowledge](#3.1)\n",
    "    * [3.2 Moscow Housing Dataset Domain Knowledge](#3.2)\n",
    "    * [3.3 Descriptive statistics](#3.3)\n",
    "    * [3.4 Distribution of outcome variable](#3.4)\n",
    "    * [3.5 Distance from city center](#3.5)\n",
    "    * [3.6 Preventing Data Leakage](#3.6)\n",
    "1. [Data Cleaning](#4)\n",
    "    * [4.1 LongLat Outliers & NaN Values](#4.1)\n",
    "    * [4.2 District Outliers & NaN Values](#4.2)\n",
    "    * [4.3 General NaN values](#4.3)\n",
    "    * [4.4 Seller Outliers & NaN Values](#4.4)\n",
    "    * [4.5 Area Kitchen and Area Living Outliers & NaN Values](#4.5)\n",
    "    * [4.6 Ceiling Outliers & NaN Values](#4.6)\n",
    "    * [4.7 Condition Outliers & NaN Values](#4.7)\n",
    "    * [4.8 Floor and Stories Outliers & NaN Values](#4.8)\n",
    "    * [4.9 Material Outliers & NaN Values](#4.9)\n",
    "    * [4.10 Constructed and New Outliers & NaN Values](#4.10) \n",
    "    * [4.11 Balconies and Loggias NaN Values](#4.11) \n",
    "    * [4.12 Elevator NaN Values](#4.12) \n",
    "    * [4.13 Remaining NaN Values After Cleaning](#4.13) \n",
    "1. [Feature Engineering](#5)\n",
    "    * [5.1 Euclidean Distance From City Center](#5.1)\n",
    "    * [5.2 Combining Elevator Features](#5.2)\n",
    "    * [5.3 Projection-based Distance From City Center](#5.3)\n",
    "    * [5.4 Area_per_room](#5.4)\n",
    "    * [5.5 Bathrooms_total](#5.5)\n",
    "    * [5.6 Balconies_total](#5.6)\n",
    "    * [5.7 Distance to Financial City Center](#5.7)\n",
    "    * [5.8 Floor_per_stories Ratio](#5.8)\n",
    "    * [5.9 Log Features](#5.10)\n",
    "    * [5.10 Squared Features](#5.11)\n",
    "    * [5.11 Other Possible Features](#5.12)\n",
    "    \n",
    "1. [Feature Extraction](#6)\n",
    "    * [6.1 Principal Component Analysis (PCA)](#6.1)\n",
    "    * [6.2 Target Encoding](#6.2)\n",
    "1. [Feature Selection](#7)\n",
    "    * [7.1 Mutual Information (MI)](#7.1)\n",
    "    * [7.2 ANOVA F-value](#7.2)\n",
    "    * [7.3 Variance Threshold](#7.3)\n",
    "    * [7.4 Conclusion Of Feature Selection](#7.4)\n",
    "1. [Models](#8)\n",
    "    * [8.1 Model Overview](#8.1)\n",
    "    * [8.2 Choosing Prediction Target](#8.2)\n",
    "    * [8.3 Validation Split](#8.3)\n",
    "    * [8.4 Xgboost](#8.4)\n",
    "    * [8.5 Lightgbm](#8.5)\n",
    "    * [8.6 Catboost](#8.6)\n",
    "    * [8.7 Stacking Model](#8.7)\n",
    "    * [8.8 Weight Averaging](#8.8)\n",
    "    * [8.9 Other Tried Models](#8.9)\n",
    "1. [Optuna Optimization](#9)\n",
    "    * [9.1 Hyperparameter Tuning Xgboost](#9.1)\n",
    "    * [9.2 Hyperparameter Tuning Lgbm](#9.2)\n",
    "    * [9.3 Hyperparameter Tuning Catboost](#9.3)\n",
    "    * [9.4 Other Model Tuning](#9.4)\n",
    "1. [Attempted and Documented Model Pipelines](#10)\n",
    "    * [10.1 Attempt 1](#10.1)\n",
    "    * [10.2 Attempt 2](#10.2)\n",
    "    * [10.3 Attempt 3](#10.3)\n",
    "    * [10.4 Attempt 4](#10.4)\n",
    "    * [10.5 Attempt 5](#10.5)\n",
    "    * [10.6 Group Kfolding Based on Building Split](#10.6)\n",
    "\n",
    "1. [Model Interpretation](#11)\n",
    "    * [11.1 LIME Interpretation](#11.1)\n",
    "    * [11.2 Model Feature Importance](#11.2)\n",
    "\n",
    "1. [Second Final Submisison - Short Notebook](#12)\n",
    "\n",
    "1. [Conclusion](#13)\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "# 1. Introduction\n",
    "\n",
    "This is the Final Long Notebook for TDT4173 Machine Learning. In this notebook we explore and document things that we have done during the lifespan of the project. In this notebook you will find our EDA, Data cleaning, Feature Engineering, Models, Optimization strategy as well as the final submissions and model interpretations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "# 2. Importing Libraries and Reading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:23:44.924988Z",
     "iopub.status.busy": "2021-11-16T12:23:44.924677Z",
     "iopub.status.idle": "2021-11-16T12:23:48.372694Z",
     "shell.execute_reply": "2021-11-16T12:23:48.371770Z",
     "shell.execute_reply.started": "2021-11-16T12:23:44.924950Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "np.random.seed(123)\n",
    "sns.set_style('darkgrid')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "#Model imports\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#for Feature Extraction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#for Feature Selection\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "#Model Optimization\n",
    "import optuna\n",
    "\n",
    "#Sklearn misc\n",
    "import sklearn.model_selection as model_selection\n",
    "from numpy.random import choice\n",
    "\n",
    "#Other misc\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:23:49.994627Z",
     "iopub.status.busy": "2021-11-16T12:23:49.993809Z",
     "iopub.status.idle": "2021-11-16T12:23:52.012905Z",
     "shell.execute_reply": "2021-11-16T12:23:52.011795Z",
     "shell.execute_reply.started": "2021-11-16T12:23:49.994571Z"
    }
   },
   "outputs": [],
   "source": [
    "!ln -s /kaggle/input/moscow-housing-tdt4173 ./data\n",
    "!ls ./data | sort\n",
    "\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:23:52.014739Z",
     "iopub.status.busy": "2021-11-16T12:23:52.014472Z",
     "iopub.status.idle": "2021-11-16T12:23:52.028284Z",
     "shell.execute_reply": "2021-11-16T12:23:52.027434Z",
     "shell.execute_reply.started": "2021-11-16T12:23:52.014709Z"
    }
   },
   "outputs": [],
   "source": [
    "def describe_column(meta):\n",
    "    \"\"\"\n",
    "    Utility function for describing a dataset column (see below for usage)\n",
    "    \"\"\"\n",
    "    def f(x):\n",
    "        d = pd.Series(name=x.name, dtype=object)\n",
    "        m = next(m for m in meta if m['name'] == x.name)\n",
    "        d['Type'] = m['type']\n",
    "        d['#NaN'] = x.isna().sum()\n",
    "        d['Description'] = m['desc']\n",
    "        if m['type'] == 'categorical':\n",
    "            counts = x.dropna().map(dict(enumerate(m['cats']))).value_counts().sort_index()\n",
    "            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n",
    "        elif m['type'] == 'real' or m['type'] == 'integer':\n",
    "            stats = x.dropna().agg(['mean', 'std', 'min', 'max'])\n",
    "            d['Statistics'] = ', '.join(f'{s}={v :.1f}' for s, v in stats.items())\n",
    "        elif m['type'] == 'boolean':\n",
    "            counts = x.dropna().astype(bool).value_counts().sort_index()\n",
    "            d['Statistics'] = ', '.join(f'{c}({n})' for c, n in counts.items())\n",
    "        else:\n",
    "            d['Statistics'] = f'#unique={x.nunique()}'\n",
    "        return d\n",
    "    return f\n",
    "\n",
    "def describe_data(data, meta):\n",
    "    desc = data.apply(describe_column(meta)).T\n",
    "    desc = desc.style.set_properties(**{'text-align': 'left'})\n",
    "    desc = desc.set_table_styles([ dict(selector='th', props=[('text-align', 'left')])])\n",
    "    return desc \n",
    "\n",
    "\n",
    "\n",
    "def plot_mi_scores(scores):\n",
    "    scores = scores.sort_values(ascending=True)\n",
    "    width = np.arange(len(scores))\n",
    "    ticks = list(scores.index)\n",
    "    plt.barh(width, scores)\n",
    "    plt.yticks(width, ticks)\n",
    "    plt.title(\"Mutual Information Scores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:23:52.218674Z",
     "iopub.status.busy": "2021-11-16T12:23:52.218201Z",
     "iopub.status.idle": "2021-11-16T12:23:52.421353Z",
     "shell.execute_reply": "2021-11-16T12:23:52.420389Z",
     "shell.execute_reply.started": "2021-11-16T12:23:52.218629Z"
    }
   },
   "outputs": [],
   "source": [
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "print(f'Loaded {len(buildings)} buildings')\n",
    "with open('data/buildings_meta.json') as f: \n",
    "    buildings_meta = json.load(f)\n",
    "buildings.head()\n",
    "describe_data(buildings, buildings_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:23:53.504218Z",
     "iopub.status.busy": "2021-11-16T12:23:53.503916Z",
     "iopub.status.idle": "2021-11-16T12:23:53.521292Z",
     "shell.execute_reply": "2021-11-16T12:23:53.520207Z",
     "shell.execute_reply.started": "2021-11-16T12:23:53.504182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all() \n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5\n",
    "\n",
    "\n",
    "def plot_map(data, ax=None, s=5, a=0.75, q_lo=0.0, q_hi=0.9, cmap='autumn', column='price', title='Moscow apartment price by location'):\n",
    "    data = data[[\"latitude\", \"longitude\", column]].sort_values(by=column, ascending=True)\n",
    "    backdrop = plt.imread('data/moscow.png')\n",
    "    backdrop = np.einsum('hwc, c -> hw', backdrop, [0, 1, 0, 0]) ** 2\n",
    "    if ax is None:\n",
    "        plt.figure(figsize=(12, 8), dpi=100)\n",
    "        ax = plt.gca()\n",
    "    discrete = data[column].nunique() <= 20\n",
    "    if not discrete:\n",
    "        lo, hi = data[column].quantile([q_lo, q_hi])\n",
    "        hue_norm = plt.Normalize(lo, hi)\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(lo, hi))\n",
    "        sm.set_array([])\n",
    "    else:\n",
    "        hue_norm = None \n",
    "    ax.imshow(backdrop, alpha=0.5, extent=[37, 38, 55.5, 56], aspect='auto', cmap='bone', norm=plt.Normalize(0.0, 2))\n",
    "    sns.scatterplot(x=\"longitude\", y=\"latitude\", hue=data[column].tolist(), ax=ax, s=s, alpha=a, palette=cmap,linewidth=0, hue_norm=hue_norm, data=data)\n",
    "    ax.set_xlim(37, 38)    # min/max longitude of image \n",
    "    ax.set_ylim(55.5, 56)  # min/max latitude of image\n",
    "    if not discrete:\n",
    "        ax.legend().remove()\n",
    "        ax.figure.colorbar(sm)\n",
    "    ax.set_title(title)\n",
    "    return ax, hue_norm\n",
    "\n",
    "\n",
    "#helper functions for PCA\n",
    "def make_mi_scores(X, y, discrete_features):\n",
    "    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "    return mi_scores\n",
    "\n",
    "\n",
    "def plot_variance(pca, width=8, dpi=100):\n",
    "    # Create figure\n",
    "    fig, axs = plt.subplots(1, 2)\n",
    "    n = pca.n_components_\n",
    "    grid = np.arange(1, n + 1)\n",
    "    # Explained variance\n",
    "    evr = pca.explained_variance_ratio_\n",
    "    axs[0].bar(grid, evr)\n",
    "    axs[0].set(\n",
    "        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Cumulative Variance\n",
    "    cv = np.cumsum(evr)\n",
    "    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n",
    "    axs[1].set(\n",
    "        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n",
    "    )\n",
    "    # Set up figure\n",
    "    fig.set(figwidth=8, dpi=100)\n",
    "    return axs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:23:54.188499Z",
     "iopub.status.busy": "2021-11-16T12:23:54.187838Z",
     "iopub.status.idle": "2021-11-16T12:23:54.195531Z",
     "shell.execute_reply": "2021-11-16T12:23:54.193688Z",
     "shell.execute_reply.started": "2021-11-16T12:23:54.188406Z"
    }
   },
   "outputs": [],
   "source": [
    "#installing packages\n",
    "# !pip install lazypredict\n",
    "# !pip install snapml\n",
    "\n",
    "# import lazypredict\n",
    "# from lazypredict import Supervised\n",
    "# from lazypredict.Supervised import LazyRegressor\n",
    "# from snapml import BoostingMachineRegressor \n",
    "\n",
    "# !pip install pandas --upgrade #to fix dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "# 3. Exploratory Data Analysis\n",
    "We wish to predict the price of a set of apartments. Hence, our target is the variable 'price'. In this section we will inspect this variable more closely, its correlation with other variables, and overall get a better understanding of the data set we are working with.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.1\"></a> <br>\n",
    "## 3.1 General Property Valuation Domain Knowledge \n",
    "\n",
    "In order to set out on the journey of analyzing the Moscow Housing dataset first we wanted to understand elements that are important in the domain of property valuation. There are many economic factors that go into why housing prices are the way they are, obviously going into the domain of micro economics one should look at how the economy is doing, supply and demand levels in the particular region where the predictions are done, track consumer behavior, current state of mortgage interest rates and much more. While these factors are very important in general house price predictions, for us it is more important to look into domain knowledge that will help us predict house prices based on a set of specific features related to said house. Therefore to simplify our task we will only emphasize Fair Market Value (FMV) in order to better understand feature importance of the Moscow Housing Dataset and how to generate new potential features that can better help our predictions. \n",
    "\n",
    "In real estate FMV is defined as the determined price that a property will sell for in an open market. Usually these house value are estimated by a professional real estate appraiser. A property's appraisal value is influenced in general by the visual inspection but there are key factors that the professionals usually look for that could be summed up in the following:\n",
    "* Recent neighborhood sales of similar properties (in a 6 to 12 month period)\n",
    "* Number of bedrooms\n",
    "* Number of bathrooms\n",
    "* Floor plan functionality / Layout\n",
    "* Size and square footage\n",
    "* Property conditions such as potentially needed repairs, aesthetics, etc\n",
    "* Location of the property (Location, Location, Location!)\n",
    "* Materials used in floors, walls, trims, exterior-walls, roof, windows\n",
    "* On-site and nearby property characteristics\n",
    "* Appraisers dont consider moveable features or decor\n",
    "* Heating and cooling systems and quality\n",
    "* Energy-efficient features (for example energy-efficiency certifications, tankless water heaters, insulated ducts)\n",
    "* If the home sits in some risk zone (nearby environmental hazards, flood zones, earthquake zones, clay soil under foundation, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.2\"></a> <br>\n",
    "## 3.2 Moscow Housing Dataset Domain Knowledge\n",
    "\n",
    "The domain knowledge in this project is required to perform feature engineering. Some of the domain knowledge was give as is while the rest was derived after careful observation of data and online search. Therefore, based on this analysis we have features that might have an impact on the price while we also have the features that likely do not have any effect on price.\n",
    "Here we create an enumeration of the most important oberservations we have:\n",
    "1.\tThe building close to the city center should have an increased price due to location.\n",
    "2.\tThe apartments with the largest area should be in general pricier than one with lesser area.\n",
    "3.\tThe apartments with more room per area should be pricier than ones with lesser area per room.\n",
    "4.\tThe apartments with more bathrooms, balconies and windows facing the street should be more expensive.\n",
    "5.\tThe apartments located on the top of a particular a high-rise building should be more expensive than the ones located on the lower floors.\n",
    "6.\tThe apartments that are new and constructed recently should be more expensive than old apartments.\n",
    "7.\tThe apartments with unusually high ceiling should be more expensive.\n",
    "8.\tThe apartment located in a building with unusually high number of stories should be expensive.\n",
    "9.\tThe condition of the apartment should be a feature that helps in the assessment of the price.\n",
    "10.\tThe access to parking, heating and elevator can also be important contributor towards the price.\n",
    "11.\tThe accurate location of the apartment e.g., precise, and easy to interpret location of the apartment can be very important factor in predicting the price.\n",
    "12.\tThe material used for the construction can also be important contributor towards the price of the apartment. \n",
    "13.\tThe type of seller can be important. e.g., property dealers tend to sell the property at a higher cost than private buyers since property dealers need to keep their margins. \n",
    "14.\tSince this is Moscow it does get cold in the winter and for that reason a heating and isolation matters as well for price predictions in general for a place that can become very cold.\n",
    "\n",
    "Unimportant Features\n",
    "On the other hand, many items can be do not care e.g.\n",
    "1.\tPresence of phone in the building might not be a deciding factor in the price as phone connections are easy to install and sometimes even not needed.\n",
    "2.\tThe name of the street should have no impact on the price of the apartment. Since the location information is already clearly indicated by using the long and lat.\n",
    "3.\tAddress of the building might also have no impact on the price due to the inclusion of the long and lat.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.3\"></a> <br>\n",
    "## 3.3. Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:23:57.115182Z",
     "iopub.status.busy": "2021-11-16T12:23:57.114846Z",
     "iopub.status.idle": "2021-11-16T12:23:57.161817Z",
     "shell.execute_reply": "2021-11-16T12:23:57.160505Z",
     "shell.execute_reply.started": "2021-11-16T12:23:57.115132Z"
    }
   },
   "outputs": [],
   "source": [
    "significant_features = [\"price\",\"area_total\",\"rooms\",\"stories\"]\n",
    "data[significant_features].describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tell from this summary that the 'price' variable has a very large range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a> <br>\n",
    "## 3.4. Distribution of outcome variable\n",
    "We want to examine the distribution of the outcome variable. With the spread of the 'price' variable in mind, we apply a log transform to the variable and plot the transformed variable as well. In the plot with the raw prices from the training set, we can se a sharp peak close to zero, and a long tail. In plot showing the transformed variable, there is a clear peak but a much more tapered tail. The distribution is deviates from te normal distribution as it is celarly positively skewed, and has a relatively sharp peak.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:35.161977Z",
     "iopub.status.busy": "2021-11-16T12:24:35.161647Z",
     "iopub.status.idle": "2021-11-16T12:24:36.395696Z",
     "shell.execute_reply": "2021-11-16T12:24:36.394188Z",
     "shell.execute_reply.started": "2021-11-16T12:24:35.161947Z"
    }
   },
   "outputs": [],
   "source": [
    "data['log(price)'] = np.log10(data['price'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(figsize=(16, 4), ncols=2, dpi=100)\n",
    "sns.distplot(data['price'], ax=ax1);\n",
    "ax1.set_title('Distribution of raw train set prices');\n",
    "sns.distplot(data['log(price)'], ax=ax2);\n",
    "ax2.set_title('Distribution of train set prices after log transform');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to examine the shape of the distibution. A normal distribution has skewness equal to 0, and kurtosis equal to 3. Kurtosis is a measure of the weight of the tails relative to the center of the distribution, and can give an impression of how extreme values are in the tail. \n",
    "\n",
    "We see that the skew is positive for both distributions, as observed. However, the kurtosis of 'price' is extremely high, meaning that we are likely to observe extreme values. This corresponds to the large range of the variable, which we have already observed. On the other hand, the kurtosis of 'log(price)' is lower than that of the normal distribution. This means that we are not liekly to see a lot of extreme values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:38.192663Z",
     "iopub.status.busy": "2021-11-16T12:24:38.192387Z",
     "iopub.status.idle": "2021-11-16T12:24:38.204124Z",
     "shell.execute_reply": "2021-11-16T12:24:38.203251Z",
     "shell.execute_reply.started": "2021-11-16T12:24:38.192635Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Skewness of 'log(price)': %f\" % data['log(price)'].skew())\n",
    "print(\"Kurtosis of 'log(price)': %f \\n\" % data['log(price)'].kurt())\n",
    "\n",
    "print(\"Skewness of 'price': %f\" % data['price'].skew())\n",
    "print(\"Kurtosis of 'price': %f\" % data['price'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.4\"></a> <br>\n",
    "## 3.4. Distance from city center\n",
    "The dataset includes the variables 'latitude' and 'longitude'. We want to visualize the relationship between location and price. From experience, we know that licing costs tend to be higher close to the city centre, and our assumption is that this is true for the Moscow dataset as well. The plot below plots apartments against their longitude at latitude, and color codes according to their price. We can easily see that apartments close to the city center are in a higher price range(yellow), and apartments further out are in a lower price range(red). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:40.182390Z",
     "iopub.status.busy": "2021-11-16T12:24:40.182046Z",
     "iopub.status.idle": "2021-11-16T12:24:41.446402Z",
     "shell.execute_reply": "2021-11-16T12:24:41.445428Z",
     "shell.execute_reply.started": "2021-11-16T12:24:40.182353Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_map(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the plot above does visualize an increasing price when moving away from the city center, we also see some high price(yellow) outliers in areas that has otherwise low price(red) apartments. In addition, the apartments are mostly high or low price, and there are few orange(mid-range) dots. Because the plot assigns color based on the total price, it does not take into the account that a very large apartment has a high price regardless of its geographical location. In reality, we know that apartment prices are relative to the size of the apartment. We repeat the plot, but this time colorcode based on price per square meter. We now see a more gradual decrease in price – moving from yellow, through orange, and into red. We observe that there are a lot more more mid-range apartments that were not missed by the first plot. This plot correlates well with our prior knowledge of apartment pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:43.117364Z",
     "iopub.status.busy": "2021-11-16T12:24:43.117041Z",
     "iopub.status.idle": "2021-11-16T12:24:44.292417Z",
     "shell.execute_reply": "2021-11-16T12:24:44.291454Z",
     "shell.execute_reply.started": "2021-11-16T12:24:43.117333Z"
    }
   },
   "outputs": [],
   "source": [
    "data['price/area'] = data['price'] / data['area_total']\n",
    "\n",
    "plot_map(data, column='price/area', title='Moscow apartment price per square meter by location');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.5\"></a> <br>\n",
    "## 3.5. Correlation with outcome variable\n",
    "We would also like to examine the correlation between features in the dataset and the target. This may give us an indication on which features are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:46.080575Z",
     "iopub.status.busy": "2021-11-16T12:24:46.080240Z",
     "iopub.status.idle": "2021-11-16T12:24:47.183637Z",
     "shell.execute_reply": "2021-11-16T12:24:47.182706Z",
     "shell.execute_reply.started": "2021-11-16T12:24:46.080542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Should probably reduce this slightly and not include all variables \n",
    "ignore_cols = ['id', 'latitude', 'longitude', 'street', 'address', 'building_id']\n",
    "corrmat = (data.drop(ignore_cols, axis=1)).corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:47.186195Z",
     "iopub.status.busy": "2021-11-16T12:24:47.185265Z",
     "iopub.status.idle": "2021-11-16T12:24:47.923039Z",
     "shell.execute_reply": "2021-11-16T12:24:47.922228Z",
     "shell.execute_reply.started": "2021-11-16T12:24:47.186153Z"
    }
   },
   "outputs": [],
   "source": [
    "k = 10\n",
    "cols = corrmat.nlargest(k, 'price')['price'].index\n",
    "corrmat_price = data[cols].drop(['price/area','log(price)'], axis=1).corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat_price, vmax=.8, square=True,annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:47.925655Z",
     "iopub.status.busy": "2021-11-16T12:24:47.924674Z",
     "iopub.status.idle": "2021-11-16T12:24:48.696008Z",
     "shell.execute_reply": "2021-11-16T12:24:48.695160Z",
     "shell.execute_reply.started": "2021-11-16T12:24:47.925581Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = corrmat.nlargest(k, 'price/area')['price/area'].index\n",
    "\n",
    "corrmat_reduced = data[cols].drop(['price', 'log(price)'], axis=1).corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corrmat_reduced, vmax=.8, square=True,annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap is reduced to only the top 10 most correlated features. Although 'area_total' and 'area_living' are highly correlated to the outcome, there are not a lot of features that appear to be very correlated. However, the correlation measures the strength of the linear relationship. There may, however, be multicollinearity between variables. This does, however, imply that a non-linear model is better suited for the prediction. In addition, it encourages some feature engineering in order to get features with a strong correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:24:49.627261Z",
     "iopub.status.busy": "2021-11-16T12:24:49.626160Z",
     "iopub.status.idle": "2021-11-16T12:24:59.886333Z",
     "shell.execute_reply": "2021-11-16T12:24:59.885374Z",
     "shell.execute_reply.started": "2021-11-16T12:24:49.627180Z"
    }
   },
   "outputs": [],
   "source": [
    "# scatterplot to see relationship between variables\n",
    "lon1 =  37.621390\n",
    "lat1 = 55.753098\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "for i in range(len(data[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data[\"longitude\"][i], data[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "\n",
    "\n",
    "data['distance'] = distance_arr\n",
    "\n",
    "sns.set()\n",
    "columns = ['price/area', 'area_total', 'area_living', 'area_kitchen','distance']\n",
    "sns.pairplot(data[columns], size = 2.5)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there apepars to be a linear relationship between 'area_total' and 'area_living', which is sensible as an increase in total area of an apartment also increases the total living area. We see a somewhat similar trend in area_kitchen with both of the mentioned variables. However, there is a clear cone shape to the plot. \n",
    "\n",
    "The top right graph between price / square meter and distance, we can see that the price decreases as we increase the distance. A similar trend can also be seens from the map.\n",
    "Overall from domain knowledge we notice that distance from city center, area total etc. are all important features and it is not surprising that the most centeral apartments are going to be the most expensive one. We will now further closely consider each feature and either clean or perform feature engineering. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3.6\"></a> <br>\n",
    "## 3.6 Preventing Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice early on that the apartments in the test set and the training set do not share the same bulding ids. Therefore, we have to be careful in creating the validation and training split inorder to prevent data leakage. \n",
    "\n",
    "We therefore decide that it is important to split based on building ids and not apartment. To demonstrate this we look at a simple model\n",
    "\n",
    "<b>First we split a simple model with an apartments split:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:25:26.391627Z",
     "iopub.status.busy": "2021-11-16T12:25:26.391352Z",
     "iopub.status.idle": "2021-11-16T12:25:27.910095Z",
     "shell.execute_reply": "2021-11-16T12:25:27.909224Z",
     "shell.execute_reply.started": "2021-11-16T12:25:26.391597Z"
    }
   },
   "outputs": [],
   "source": [
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    " \n",
    "\n",
    "data_train, data_valid = model_selection.train_test_split(data, test_size=0.33, stratify=np.log(data.price).round())\n",
    "\n",
    "features = [\"ceiling\",  \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"new\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"material\", \"stories\"]\n",
    "\n",
    "train_x = data_train[features]\n",
    "train_y = np.log1p(data_train['price'])\n",
    "test_x = data_valid[features]\n",
    "test_y = np.log1p(data_valid['price'])\n",
    "\n",
    "\n",
    "model =XGBRegressor()\n",
    "\n",
    "model.fit(train_x,train_y, verbose=False)\n",
    "\n",
    "preds = model.predict(test_x)\n",
    "rmsle_apartments = root_mean_squared_log_error(y_true=np.expm1(test_y), y_pred=np.expm1(preds))\n",
    "\n",
    "print(\"RMSLE:\", rmsle_apartments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Split model based on Building_id:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:26:45.849379Z",
     "iopub.status.busy": "2021-11-16T12:26:45.849049Z",
     "iopub.status.idle": "2021-11-16T12:26:47.038839Z",
     "shell.execute_reply": "2021-11-16T12:26:47.037938Z",
     "shell.execute_reply.started": "2021-11-16T12:26:45.849346Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = model_selection.GroupShuffleSplit(n_splits=2, test_size=.33, random_state=0)\n",
    "train_index, valid_index = next(gs.split(data, groups=data.building_id))\n",
    "\n",
    "data_train = data.loc[train_index]\n",
    "data_valid = data.loc[valid_index]\n",
    "\n",
    "\n",
    "train_x = data_train[features]\n",
    "train_y = np.log1p(data_train['price'])\n",
    "test_x = data_valid[features]\n",
    "test_y = np.log1p(data_valid['price'])\n",
    "\n",
    "\n",
    "model =XGBRegressor()\n",
    "\n",
    "model.fit(train_x,train_y, verbose=False)\n",
    "\n",
    "preds = model.predict(test_x)\n",
    "rmsle_building = root_mean_squared_log_error(y_true=np.expm1(test_y), y_pred=np.expm1(preds))\n",
    "\n",
    "print(\"RMSLE:\", rmsle_building)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSLE from the building split is much higher but also much more realistic as this shows how the model would perform if there arent apartments in the same building between the training and validation set. This makes more sense as in the test set this is how the data is split and using building split might therefore close the gap between validation set and test set performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "# 4. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the data cleaning it is important to get a perspective of how many NaN values we have in the dataset as well as proceed with trying to understand what types of problems we might expect to encounter. Important questions to ask during cleaning:\n",
    "\n",
    "* <b>Is a feature and its data intuitive on its own?</b>\n",
    "* <b>Is a feature consistent in regards to closely related features?</b> For example area_kitchen, area_living and area_total\n",
    "* <b>How prevalent are the NaN values?</b>\n",
    "* <b>Are NaN values placed randomly or does it have a significant pattern?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:03.409359Z",
     "iopub.status.busy": "2021-11-16T12:27:03.409025Z",
     "iopub.status.idle": "2021-11-16T12:27:04.541486Z",
     "shell.execute_reply": "2021-11-16T12:27:04.540597Z",
     "shell.execute_reply.started": "2021-11-16T12:27:03.409325Z"
    }
   },
   "outputs": [],
   "source": [
    "nan_percentage = data.isna().sum()/len(data) *100\n",
    "nan_percentage = nan_percentage.sort_values(ascending=False)\n",
    "\n",
    "nan_percentage_test = data_test.isna().sum()/len(data) *100\n",
    "nan_percentage_test = nan_percentage_test.sort_values(ascending=False)\n",
    "\n",
    "# Percent missing data by feature\n",
    "f, ax = plt.subplots(1, 2, sharex=True, figsize=(20,10))\n",
    "sns.barplot(ax=ax[0], x=nan_percentage.index, y=nan_percentage)\n",
    "ax[0].set_xlabel('Features', fontsize=15)\n",
    "ax[0].set_ylabel('Percent of missing values', fontsize=15)\n",
    "ax[0].set_title('Percent missing data by feature - TRAINING DATA', fontsize=10)\n",
    "ax[0].set_xticklabels(nan_percentage.sort_values(ascending=False).index, rotation=90)\n",
    "\n",
    "\n",
    "\n",
    "# Percent missing data by feature\n",
    "sns.barplot(ax=ax[1], x=nan_percentage_test.index, y=nan_percentage_test)\n",
    "ax[1].set_xlabel('Features', fontsize=15)\n",
    "ax[1].set_ylabel('Percent of missing values', fontsize=15)\n",
    "ax[1].set_title('Percent missing data by feature - TEST DATA', fontsize=10)\n",
    "ax[1].set_xticklabels(nan_percentage_test.sort_values(ascending=False).index, rotation=90)\n",
    "\n",
    "print(\"histograms of NaN values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to see that for the NaN values in both the Training and Test set. That there is a pattern in terms of missing values. And the distribution of these missing values is fairly similar overall. This means that the training and test set was not split in a particular way to make the test set necessarily more difficult than the training set, but rather that the test set is supposed to represent a wide variety of housing and their features much like the training set. From this alone we can conclude that there is a significant pattern in the missing values, where the the train and test where collected and composed in the same fashion and therefore it is safe to say that NaN values can be treated equally in both datasets. \n",
    "\n",
    "Most NaN values could be set to some mode or mean if not another significant way of imputation is found, but looking at \"Layout\" we have concluded that while it could have its significance during predictions, it can safely be dropped as over 70% of the training set for said feature is missing.\n",
    "\n",
    "In the next sections of this chapter we are going to take you through the handling of different outliers by analyzing if the data is intuitve on its own and/or if it is consistent with closely related features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a> <br>\n",
    "## 4.1 LongLat Outliers & NaN Values\n",
    "\n",
    "There are as observed by us, three different anomalies in the Longitude and Latitude test data, specifically:\n",
    "* NaN Values\n",
    "* Negative Values\n",
    "* Blown up values outside of moscow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:06.508223Z",
     "iopub.status.busy": "2021-11-16T12:27:06.507235Z",
     "iopub.status.idle": "2021-11-16T12:27:06.535172Z",
     "shell.execute_reply": "2021-11-16T12:27:06.534082Z",
     "shell.execute_reply.started": "2021-11-16T12:27:06.508174Z"
    }
   },
   "outputs": [],
   "source": [
    "data_test[[\"latitude\",\"longitude\", \"district\", \"building_id\", \"address\",\"street\"]].query(\"longitude < 36.8 | longitude > 38.88 | latitude < 55.18 | latitude > 56.082 | longitude != longitude | latitude != latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the address and street and find the correct longitude and latitude values in google maps and adjust the Longitude and Latitude accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:07.031676Z",
     "iopub.status.busy": "2021-11-16T12:27:07.031165Z",
     "iopub.status.idle": "2021-11-16T12:27:07.052270Z",
     "shell.execute_reply": "2021-11-16T12:27:07.051406Z",
     "shell.execute_reply.started": "2021-11-16T12:27:07.031639Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fixing the NaN values\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#Fixing negative values\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Fixing blown up values\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a> <br>\n",
    "## 4.2 District Outliers & NaN Values\n",
    "\n",
    "In this section we look into handling District Outliers and NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:07.533120Z",
     "iopub.status.busy": "2021-11-16T12:27:07.532650Z",
     "iopub.status.idle": "2021-11-16T12:27:09.368678Z",
     "shell.execute_reply": "2021-11-16T12:27:09.367946Z",
     "shell.execute_reply.started": "2021-11-16T12:27:07.533073Z"
    }
   },
   "outputs": [],
   "source": [
    "data_central = data.copy().drop(data[(data['longitude'] < 37.1) & (data['latitude'] < 55.6)].index)\n",
    "sns.set(rc={'figure.figsize':(30,20)})\n",
    "ax = sns.scatterplot(data= data_central[[\"latitude\",\"longitude\"]],x=\"longitude\", y=\"latitude\",  hue=data_central[\"district\"], palette=\"Paired\")\n",
    "\n",
    "ax.set_title(\"tdt4173 dataset districts - WITH OUTLIERS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can observe that there are some districts assigned in the middle of other districts and as such we decided to look at these as possible outliers. Specifically a green dot (district category 3) in a cluster of light blue dots (district category 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:09.370389Z",
     "iopub.status.busy": "2021-11-16T12:27:09.370013Z",
     "iopub.status.idle": "2021-11-16T12:27:09.384796Z",
     "shell.execute_reply": "2021-11-16T12:27:09.384170Z",
     "shell.execute_reply.started": "2021-11-16T12:27:09.370358Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:09.386115Z",
     "iopub.status.busy": "2021-11-16T12:27:09.385882Z",
     "iopub.status.idle": "2021-11-16T12:27:09.408672Z",
     "shell.execute_reply": "2021-11-16T12:27:09.407758Z",
     "shell.execute_reply.started": "2021-11-16T12:27:09.386087Z"
    }
   },
   "outputs": [],
   "source": [
    "#Note: value != value in query is the way to identify NaN values\n",
    "data_test[[\"latitude\",\"longitude\", \"district\", \"building_id\", \"address\",\"street\"]].query(\"district != district\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filled all the districts that contained nans by using their longitude and latitude coordinates and trying to find out what cluster of district it might be a part off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:09.693237Z",
     "iopub.status.busy": "2021-11-16T12:27:09.692528Z",
     "iopub.status.idle": "2021-11-16T12:27:09.733367Z",
     "shell.execute_reply": "2021-11-16T12:27:09.732525Z",
     "shell.execute_reply.started": "2021-11-16T12:27:09.693192Z"
    }
   },
   "outputs": [],
   "source": [
    "# fixing test_data districts:\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:10.047053Z",
     "iopub.status.busy": "2021-11-16T12:27:10.046767Z",
     "iopub.status.idle": "2021-11-16T12:27:11.295207Z",
     "shell.execute_reply": "2021-11-16T12:27:11.294363Z",
     "shell.execute_reply.started": "2021-11-16T12:27:10.047019Z"
    }
   },
   "outputs": [],
   "source": [
    "data_central = data_test.copy().drop(data_test[(data_test['longitude'] < 37.1) & (data_test['latitude'] < 55.6)].index)\n",
    "sns.set(rc={'figure.figsize':(10,10)})\n",
    "ax = sns.scatterplot(data= data_central[[\"latitude\",\"longitude\"]],x=\"longitude\", y=\"latitude\",  hue=data_central[\"district\"], palette=\"Paired\")\n",
    "ax.set_title(\"tdt4173 dataset districts - FIXED\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the resulting cleaned version of the test set. Obviously theere are still overlaps here and there in the district clusters but we weren't confident enough about the district lines of Moscow to make decisions about if these are outliers or correct values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.3\"></a> <br>\n",
    "## 4.3 General NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bathroom_shared and bathroom_private features contain NaN values. After some analysis we decided to fill the NaN in both features with value 1. While many old apartments mgiht not have any bathrooms we ended up thinking it is more valid to just guess it to be atleast one bathroom. Another way to do it would've been just to fill the NaN values with 99999 in order to do the Kaggle trick to communicate to the model that the NaN values is its own category. \n",
    "\n",
    "For parking we decided to create a new class called 'No Parking' and filled all the NaN with that.\n",
    "For heating we assume that Moscow is really cold and all apartments probably have some form of heating. Therefore, we filled all the NaN with the most common class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:26.483965Z",
     "iopub.status.busy": "2021-11-16T12:27:26.483434Z",
     "iopub.status.idle": "2021-11-16T12:27:26.494175Z",
     "shell.execute_reply": "2021-11-16T12:27:26.493425Z",
     "shell.execute_reply.started": "2021-11-16T12:27:26.483930Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.4\"></a> <br>\n",
    "## 4.4 Seller Outliers & NaN Values\n",
    "\n",
    "In this section we look into handling Seller Outliers and NaN values. We decided to fill all the NaN values such that the distribution of this variable does not get affected.\n",
    "\n",
    "Essentially, we used the frequency distribution of each category and filled all the NaNs at random based on the probablity distribution. For seller there is no dependeable feature from which we can derive \n",
    "missing data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:41.374433Z",
     "iopub.status.busy": "2021-11-16T12:27:41.374057Z",
     "iopub.status.idle": "2021-11-16T12:27:42.024864Z",
     "shell.execute_reply": "2021-11-16T12:27:42.023825Z",
     "shell.execute_reply.started": "2021-11-16T12:27:41.374399Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(figsize=(12, 4), ncols=2, dpi=100)\n",
    "print(f'Split dataset into {len(data)} training samples and {len(data_test)} validation samples')\n",
    "\n",
    "sns.histplot((data.seller).rename('Seller'), ax=ax1);\n",
    "sns.histplot((data_test.seller).rename('Seller'), ax=ax2);\n",
    "ax1.set_title('Training set sellers'); ax2.set_title('Validation set sellers');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to split the NaN values based on the probability distribution of the sellers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:43.161292Z",
     "iopub.status.busy": "2021-11-16T12:27:43.160477Z",
     "iopub.status.idle": "2021-11-16T12:27:43.173984Z",
     "shell.execute_reply": "2021-11-16T12:27:43.173020Z",
     "shell.execute_reply.started": "2021-11-16T12:27:43.161238Z"
    }
   },
   "outputs": [],
   "source": [
    "#getting distribution of categories based on the non NaN values\n",
    "for i in range(4):\n",
    "    print(i ,\" \" , len(data[\"seller\"][data.seller == i])/len(data[\"seller\"][data.seller == data.seller]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:44.011976Z",
     "iopub.status.busy": "2021-11-16T12:27:44.011673Z",
     "iopub.status.idle": "2021-11-16T12:27:44.026384Z",
     "shell.execute_reply": "2021-11-16T12:27:44.025369Z",
     "shell.execute_reply.started": "2021-11-16T12:27:44.011939Z"
    }
   },
   "outputs": [],
   "source": [
    "list_of_candidates = [0,1,2,3]\n",
    "# 14455 , owener 0, \n",
    "probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "number_of_items_to_pick = data['seller'].isna().sum()\n",
    "number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "              p=probability_distribution)\n",
    "draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "              p=probability_distribution)\n",
    "\n",
    "data['seller'][data.seller.isna()] = draw\n",
    "data_test['seller'][data_test.seller.isna()] = draw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.5\"></a> <br>\n",
    "## 4.5 Area Kitchen and Area Living Outliers & NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe that area_living and area_kitchen do not only contain NaN values but also have multiple outliers. It does not for instance make sense that the sum of the living and kitchen are greater than or equal to total_area. Especiall if we need to account for loggias, balconies, shared_bathrooms and private_bathrooms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:45.590106Z",
     "iopub.status.busy": "2021-11-16T12:27:45.589845Z",
     "iopub.status.idle": "2021-11-16T12:27:45.611643Z",
     "shell.execute_reply": "2021-11-16T12:27:45.610828Z",
     "shell.execute_reply.started": "2021-11-16T12:27:45.590079Z"
    }
   },
   "outputs": [],
   "source": [
    "#Outliers\n",
    "print(data[[\"area_total\",\"area_living\", \"area_kitchen\"]].query(\"area_living + area_kitchen >= area_total\"))\n",
    "\n",
    "#NaN values\n",
    "print(data[[\"area_total\",\"area_living\", \"area_kitchen\"]].query(\"area_living != area_living | area_kitchen != area_kitchen\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to fix this issue by looking at the mean kitchen and living areas of the non-outlier values. Note that we are using the mean of these values contained in the training set to populate the outlier values both for the training and test set. In many kaggle competitions if there is access to data from test set, it is usually used for applications as these, where one looks at the total mean of a feature from both the training and test set. Even though this might give a potential boost in performance we decided not to do this, as in a real world application test data is usually not accessable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:46.936670Z",
     "iopub.status.busy": "2021-11-16T12:27:46.936175Z",
     "iopub.status.idle": "2021-11-16T12:27:46.952152Z",
     "shell.execute_reply": "2021-11-16T12:27:46.951188Z",
     "shell.execute_reply.started": "2021-11-16T12:27:46.936621Z"
    }
   },
   "outputs": [],
   "source": [
    "percentage_area_data = pd.DataFrame()\n",
    "percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "print(\"Mean kitchen area of the Non-outlier values:\", mean_kitchen)\n",
    "print(\"Mean living area of the Non-outlier values:\", mean_living)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:48.051856Z",
     "iopub.status.busy": "2021-11-16T12:27:48.051579Z",
     "iopub.status.idle": "2021-11-16T12:27:48.088909Z",
     "shell.execute_reply": "2021-11-16T12:27:48.088145Z",
     "shell.execute_reply.started": "2021-11-16T12:27:48.051829Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#to omit bugs\n",
    "data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "#test_set\n",
    "data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.6\"></a> <br>\n",
    "## 4.6 Ceiling Outliers & NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could observe certain outliers in the ceiling data, where the ceiling is less than 1 meter or much greater than 9 meters. Obviously there might be some incredibly high ceilings but it is very unlikely that they are in the range of above 9 meters. For these outliers as wel as nan values we set it equal to the mode which was 2.64m, it is safe to say that it is probably the best value to use as replacement for NaN and outlier values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:50.754269Z",
     "iopub.status.busy": "2021-11-16T12:27:50.753419Z",
     "iopub.status.idle": "2021-11-16T12:27:50.773091Z",
     "shell.execute_reply": "2021-11-16T12:27:50.772494Z",
     "shell.execute_reply.started": "2021-11-16T12:27:50.754225Z"
    }
   },
   "outputs": [],
   "source": [
    "data[['ceiling']].query(\"ceiling < 1 | ceiling > 9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:51.314785Z",
     "iopub.status.busy": "2021-11-16T12:27:51.314463Z",
     "iopub.status.idle": "2021-11-16T12:27:51.989328Z",
     "shell.execute_reply": "2021-11-16T12:27:51.988440Z",
     "shell.execute_reply.started": "2021-11-16T12:27:51.314750Z"
    }
   },
   "outputs": [],
   "source": [
    "maxc = 9\n",
    "minc = 1\n",
    "data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)  \n",
    "\n",
    "data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.7\"></a> <br>\n",
    "## 4.7 Condition Outliers & NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition is feature split into 4 different categories: Decorated, Euro_repair, Undecorated, Special Design. We decidd to fill the NaN values with a new category as the condition is unknown and deriving it from other features is not as clear cut as it might have been for some of the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:54.103601Z",
     "iopub.status.busy": "2021-11-16T12:27:54.103189Z",
     "iopub.status.idle": "2021-11-16T12:27:54.109119Z",
     "shell.execute_reply": "2021-11-16T12:27:54.108385Z",
     "shell.execute_reply.started": "2021-11-16T12:27:54.103572Z"
    }
   },
   "outputs": [],
   "source": [
    "unknown =  4.0\n",
    "data['condition'] = data['condition'].fillna(unknown)\n",
    "data_test['condition'] = data_test['condition'].fillna(unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.8\"></a> <br>\n",
    "## 4.8 Floor and Stories Outliers & NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We collect the building IDs of all the buildings where any floor values of a particular apartment is greater that the total number of\n",
    "stories in that building. We then replace the stories of each such bulding ID with the max floor number associated with the apartment.\n",
    "In short we chose to trust the floor number of each apartment as compared to the stories values of each building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:56.433092Z",
     "iopub.status.busy": "2021-11-16T12:27:56.432100Z",
     "iopub.status.idle": "2021-11-16T12:27:56.446718Z",
     "shell.execute_reply": "2021-11-16T12:27:56.445863Z",
     "shell.execute_reply.started": "2021-11-16T12:27:56.433044Z"
    }
   },
   "outputs": [],
   "source": [
    "data[[\"floor\", \"stories\"]][data.floor > data.stories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:57.142030Z",
     "iopub.status.busy": "2021-11-16T12:27:57.141349Z",
     "iopub.status.idle": "2021-11-16T12:27:57.442056Z",
     "shell.execute_reply": "2021-11-16T12:27:57.441181Z",
     "shell.execute_reply.started": "2021-11-16T12:27:57.141985Z"
    }
   },
   "outputs": [],
   "source": [
    "idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "#data['storiesnew'] = data['stories'].copy()\n",
    "for i in range(idss.size):\n",
    "    max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "    data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "\n",
    "idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "#data['storiesnew'] = data['stories'].copy()\n",
    "for i in range(idss_test.size):\n",
    "    max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "    data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.9\"></a> <br>\n",
    "## 4.9 Material Outliers & NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hypothesize that the class Monolith brick is the same as Monolith therefore we merge them together as part of the cleaning process,\n",
    "So we merge class 5 with class 2. We also move Stalin from class 6 to 5 as class 5 is empty after merging class 5 with 2. Then we take the mode of the material as the replacement for NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:27:59.000516Z",
     "iopub.status.busy": "2021-11-16T12:27:58.999672Z",
     "iopub.status.idle": "2021-11-16T12:27:59.022229Z",
     "shell.execute_reply": "2021-11-16T12:27:59.021397Z",
     "shell.execute_reply.started": "2021-11-16T12:27:59.000479Z"
    }
   },
   "outputs": [],
   "source": [
    "data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "data.material[data.material==6] = 5.0 #stalin to 5\n",
    "\n",
    "data_test.material[data_test.material==5] = 2.0\n",
    "data_test.material[data_test.material==6] = 5.0\n",
    "\n",
    "data['material'][data.material.isna()] = data['material'].mode()[0]\n",
    "data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.10\"></a> <br>\n",
    "## 4.10 Constructed and New Outliers & NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the apartment in question is new and the coloum new is not a NaN but coloum constructed is a NAN then we fill the constructed coloum with year 2019. Since 2019 is the most prevalant year for apartments. Otherwise, we fill the coloumns with 2021. And after filling the all the NaN of constructed we fill the NaNs of feature new. If the apartment is constructed earlier than 2020 we fill the NaNs in new by 0.0 (Old) otherwise we fill it with 1.0. If it is not NaN we do not change it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:01.752974Z",
     "iopub.status.busy": "2021-11-16T12:28:01.752710Z",
     "iopub.status.idle": "2021-11-16T12:28:03.135851Z",
     "shell.execute_reply": "2021-11-16T12:28:03.134996Z",
     "shell.execute_reply.started": "2021-11-16T12:28:01.752949Z"
    }
   },
   "outputs": [],
   "source": [
    "data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "data['new'] = data.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")      \n",
    "\n",
    "\n",
    "data_test['constructed'] = data_test.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data_test['new'] = data_test.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.11\"></a> <br>\n",
    "## 4.11 Balconies and Loggias NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data does not contain any apartments for which there are not balconies and loggias, it made sense to fill the NaN values to satisfy this condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:04.226242Z",
     "iopub.status.busy": "2021-11-16T12:28:04.225507Z",
     "iopub.status.idle": "2021-11-16T12:28:04.240644Z",
     "shell.execute_reply": "2021-11-16T12:28:04.239580Z",
     "shell.execute_reply.started": "2021-11-16T12:28:04.226205Z"
    }
   },
   "outputs": [],
   "source": [
    "data[[\"balconies\",\"loggias\"]].query(\"balconies == 0 & loggias == 0 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:04.849551Z",
     "iopub.status.busy": "2021-11-16T12:28:04.849237Z",
     "iopub.status.idle": "2021-11-16T12:28:04.858484Z",
     "shell.execute_reply": "2021-11-16T12:28:04.857508Z",
     "shell.execute_reply.started": "2021-11-16T12:28:04.849522Z"
    }
   },
   "outputs": [],
   "source": [
    "# Therefore, we fill all the NANs with 0\n",
    "data['balconies'] = data['balconies'].fillna(0)\n",
    "data['loggias'] = data['loggias'].fillna(0)\n",
    "\n",
    "data_test['balconies'] = data_test['balconies'].fillna(0)\n",
    "data_test['loggias'] = data_test['loggias'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.12\"></a> <br>\n",
    "## 4.12 Elevator NaN Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elevators features are interesting to look at together as they are connected to eachother. Therefore we decided to clean te elevator missing values in the feature engineering as it made more sense to treat the NaN Values for the engineered elevator feature. In this section we will how ever look into possible outliers and data combinations that could possibly be unintuitive/illogical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:08.581722Z",
     "iopub.status.busy": "2021-11-16T12:28:08.581214Z",
     "iopub.status.idle": "2021-11-16T12:28:08.598918Z",
     "shell.execute_reply": "2021-11-16T12:28:08.597907Z",
     "shell.execute_reply.started": "2021-11-16T12:28:08.581672Z"
    }
   },
   "outputs": [],
   "source": [
    "d = data[[\"elevator_without\", \"elevator_passenger\", \"elevator_service\"]]\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:09.456570Z",
     "iopub.status.busy": "2021-11-16T12:28:09.456272Z",
     "iopub.status.idle": "2021-11-16T12:28:09.470025Z",
     "shell.execute_reply": "2021-11-16T12:28:09.468995Z",
     "shell.execute_reply.started": "2021-11-16T12:28:09.456536Z"
    }
   },
   "outputs": [],
   "source": [
    "d.query(\"elevator_without == 0 & elevator_passenger == 0 & elevator_service == 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to see that the data here is intuitive as the combination <b>\"elevator_without == 0 & elevator_passenger == 0 & elevator_service == 0\"</b> does not exist. If we would dissect what this could mean, it is basically that an apartment does not a passenger or a service elevator but elevator_without is saying that it does as it is 0. So as this features does not make sense it is good that there are any apartments that contain this combination. All the other binary combinations do how ever make sense and are broken down into categories and treated in section 5.2 under Feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4.13\"></a> <br>\n",
    "## 4.13 Remaining NaN Values After Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:12.140637Z",
     "iopub.status.busy": "2021-11-16T12:28:12.140352Z",
     "iopub.status.idle": "2021-11-16T12:28:12.155424Z",
     "shell.execute_reply": "2021-11-16T12:28:12.154374Z",
     "shell.execute_reply.started": "2021-11-16T12:28:12.140606Z"
    }
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed there are still missing values for the following (similarly for the test data as well):\n",
    "* Layout\n",
    "* Windows\n",
    "* Phones\n",
    "* Elevator Features\n",
    "* Garbage Chutes\n",
    "\n",
    "\n",
    "For elevators we decided to combine the elevator features into a new feature based on different categories and then fill the nan values of that engineerewd values. The other features are how ever dropped by us for various reasons. For instance layout had over 70% missing values and phones do not add any particular importance to the price predictions. For Windows and Garbage Chutes, we didnt find a clever way to find the nan values of these so we decided to drop them all together. There is always the possibility of assigning a new category for which stands for \"unknown values\" but we decided that these features were not significant enough to put much effort into them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "# 5. Feature Engineering\n",
    "The information available in the dataset is not necessarily presented in a way a machine learning model is able to make good use of. In this section, we use the data available to engineer features that are more meaningful and more strongly correlated to the outcome. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.1\"></a> <br>\n",
    "## 5.1 Euclidean Distance From City Center\n",
    "Adding the distance from city center. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:29.402398Z",
     "iopub.status.busy": "2021-11-16T12:28:29.402110Z",
     "iopub.status.idle": "2021-11-16T12:28:29.412301Z",
     "shell.execute_reply": "2021-11-16T12:28:29.411594Z",
     "shell.execute_reply.started": "2021-11-16T12:28:29.402363Z"
    }
   },
   "outputs": [],
   "source": [
    "origin_coordinates = (37.621390,55.753098)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.2\"></a> <br>\n",
    "## 5.2 Combining Elevator Features\n",
    "***The Logic behind this feature***\n",
    "Which could be interpreted as the following:\n",
    "* elevator_without == 1 & elevator_passenger == 0 & elevator_service == 0 => <b>No elevator access for all apartments => Category 0</b>\n",
    "* elevator_without == 1 & elevator_passenger == 0 & elevator_service == 1 => <b>Some apartments have service elevators and some do not => Category 1</b>\n",
    "* elevator_without == 1 & elevator_passenger == 1 & elevator_service == 0 => <b>Some apartments have passenger elevators some do not => Category 2</b>\n",
    "* elevator_without == 0 & elevator_passenger == 0 & elevator_service == 1 => <b>All apartments have service elevator => Category 3</b>\n",
    "* elevator_without == 0 & elevator_passenger == 1 & elevator_service == 0 => <b>All apartments have passenger elevator => Category 4</b>\n",
    "* elevator_without == 0 & elevator_passenger == 1 & elevator_service == 1 => <b>All apartments have passenger elevator and service elevator => Category 5</b>\n",
    "* elevator_without == 1 & elevator_passenger == 1 & elevator_service == 1 => <b>Some apartments have service elevators and passenger elevators and some do not => Category 6</b>\n",
    "* elevator_without == 0 & elevator_passenger == 0 & elevator_service == 0 => <b>Doesn't make sense but this doesnt exist either</b>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:32.478859Z",
     "iopub.status.busy": "2021-11-16T12:28:32.478564Z",
     "iopub.status.idle": "2021-11-16T12:28:33.884042Z",
     "shell.execute_reply": "2021-11-16T12:28:33.883063Z",
     "shell.execute_reply.started": "2021-11-16T12:28:32.478823Z"
    }
   },
   "outputs": [],
   "source": [
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                )    \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "mod = data['elevatern'].mode()\n",
    "data['elevatern'] = data['elevatern'].fillna(mod[0])\n",
    "data_test['elevatern'] = data_test['elevatern'].fillna(mod[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.3\"></a> <br>\n",
    "## 5.3 Projection-based Distance From City Center\n",
    "Adding the distance to city center as a feature. A projection-based distance is used rather than euclidean distance in order to account for the curvature of the earth's surface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:33.983730Z",
     "iopub.status.busy": "2021-11-16T12:28:33.982690Z",
     "iopub.status.idle": "2021-11-16T12:28:34.628166Z",
     "shell.execute_reply": "2021-11-16T12:28:34.627283Z",
     "shell.execute_reply.started": "2021-11-16T12:28:33.983667Z"
    }
   },
   "outputs": [],
   "source": [
    "lon1 =  37.621390\n",
    "lat1 = 55.753098\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data[\"longitude\"][i], data[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data['fwd_azi'] = fwd_azimuth_arr\n",
    "data['distance'] = distance_arr\n",
    "data['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data_test[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data_test[\"longitude\"][i], data_test[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data_test['fwd_azi'] = fwd_azimuth_arr\n",
    "data_test['distance'] = distance_arr\n",
    "data_test['back_azi'] = back_azimuth_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.4\"></a> <br>\n",
    "## 5.4 Area_per_room\n",
    "Dividing the total are by the number of rooms to get average area per room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:34.971397Z",
     "iopub.status.busy": "2021-11-16T12:28:34.971112Z",
     "iopub.status.idle": "2021-11-16T12:28:34.977896Z",
     "shell.execute_reply": "2021-11-16T12:28:34.977332Z",
     "shell.execute_reply.started": "2021-11-16T12:28:34.971368Z"
    }
   },
   "outputs": [],
   "source": [
    "data['area_per_room'] = data['area_total']/data['rooms']\n",
    "data_test['area_per_room'] = data_test['area_total']/data_test['rooms']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.5\"></a> <br>\n",
    "## 5.5 Bathrooms_total\n",
    "Adding the 'bathrooms_shared' and 'bathrooms_private' features to get the total number of bathrooms of the apartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:35.871221Z",
     "iopub.status.busy": "2021-11-16T12:28:35.870779Z",
     "iopub.status.idle": "2021-11-16T12:28:35.878164Z",
     "shell.execute_reply": "2021-11-16T12:28:35.877291Z",
     "shell.execute_reply.started": "2021-11-16T12:28:35.871174Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.6\"></a> <br>\n",
    "## 5.6 Balconies_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining loggias and balconies to a common feature makes sense as these two features have somewhat the same functionality in an apartment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:37.246694Z",
     "iopub.status.busy": "2021-11-16T12:28:37.245985Z",
     "iopub.status.idle": "2021-11-16T12:28:37.254166Z",
     "shell.execute_reply": "2021-11-16T12:28:37.253251Z",
     "shell.execute_reply.started": "2021-11-16T12:28:37.246645Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.7\"></a> <br>\n",
    "## 5.7 Distance to Financial City Center\n",
    "\n",
    "Moscow has also a financial center and since the domain knowledge speaks of the importance of location, it is therefore a good idea to include it in the feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:38.236074Z",
     "iopub.status.busy": "2021-11-16T12:28:38.235770Z",
     "iopub.status.idle": "2021-11-16T12:28:38.245594Z",
     "shell.execute_reply": "2021-11-16T12:28:38.244942Z",
     "shell.execute_reply.started": "2021-11-16T12:28:38.236039Z"
    }
   },
   "outputs": [],
   "source": [
    "financial_coords = (37.535497858, 55.741330368)\n",
    "distance_from_city_center = np.sqrt((financial_coords[0] - data[\"longitude\"])**2+(financial_coords[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_financial_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((financial_coords[0] - data_test[\"longitude\"])**2+(financial_coords[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_financial_center\"] = distance_from_city_center_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.8\"></a> <br>\n",
    "## 5.8 Floor_per_stories Ratio\n",
    "\n",
    "\n",
    "It is interesting to give the floor/stories to communicate to the model how high the apartment is in a building in a scale between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:39.678581Z",
     "iopub.status.busy": "2021-11-16T12:28:39.677707Z",
     "iopub.status.idle": "2021-11-16T12:28:39.684641Z",
     "shell.execute_reply": "2021-11-16T12:28:39.683979Z",
     "shell.execute_reply.started": "2021-11-16T12:28:39.678515Z"
    }
   },
   "outputs": [],
   "source": [
    "data[\"floor/stories\"] = data[\"floor\"]/data[\"stories\"]\n",
    "data_test[\"floor/stories\"] = data_test[\"floor\"]/data_test[\"stories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.10\"></a> <br>\n",
    "## 5.9 Log Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide log features to the model for some selected features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:47.408181Z",
     "iopub.status.busy": "2021-11-16T12:28:47.407381Z",
     "iopub.status.idle": "2021-11-16T12:28:47.422176Z",
     "shell.execute_reply": "2021-11-16T12:28:47.421562Z",
     "shell.execute_reply.started": "2021-11-16T12:28:47.408138Z"
    }
   },
   "outputs": [],
   "source": [
    "data['area_per_room_log']        = np.log1p(data['area_per_room'])\n",
    "data['area_total_log']           = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log']         = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log']          = np.log1p(data['area_living'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_test['area_per_room_log']   = np.log1p(data_test['area_per_room'])\n",
    "data_test['area_total_log']      = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log']    = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log']     = np.log1p(data_test['area_living'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.11\"></a> <br>\n",
    "## 5.10 Squared Features\n",
    "\n",
    "It is also possible to square some selected numerical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:47.428394Z",
     "iopub.status.busy": "2021-11-16T12:28:47.427880Z",
     "iopub.status.idle": "2021-11-16T12:28:47.439868Z",
     "shell.execute_reply": "2021-11-16T12:28:47.438735Z",
     "shell.execute_reply.started": "2021-11-16T12:28:47.428357Z"
    }
   },
   "outputs": [],
   "source": [
    "data['area_per_room_squared']        = data['area_per_room']*data['area_per_room']\n",
    "data['area_total_squared']           = data['area_total']*data['area_total']\n",
    "data['area_kitchen_squared']         = data['area_kitchen']*data['area_kitchen']\n",
    "data['area_living_squared']          = data['area_living']*data['area_living']\n",
    "\n",
    "\n",
    "\n",
    "data_test['area_per_room_squared']   = data_test['area_per_room']*data_test['area_per_room']\n",
    "data_test['area_total_squared']      = data_test['area_total']*data_test['area_total']\n",
    "data_test['area_kitchen_squared']    = data_test['area_kitchen']*data_test['area_kitchen']\n",
    "data_test['area_living_squared']     = data_test['area_living']*data_test['area_living']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5.12\"></a> <br>\n",
    "## 5.11 Other Possible Features\n",
    "\n",
    "Other possible features could be taking the squareroot of the original numerical values or taking the 1/x of the feature. These are all transformations that could possibly help further minimze the rmsle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "# 6. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.1\"></a> <br>\n",
    "## 6.1 Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a method for partitioning of the variation in the data. This feature extraction method could help discover important relationship in the moscow dataset and help create even more informative features. There are two ways this method could be used, the first method is to discover relationships between different features such that one could manually combine them to a new more informative feature. The second method is to use the PCA feature(s) as part of the dataset directly because the PCA may better describe the variational structure of the data. \n",
    "\n",
    "It is good practice to consider the following in PCA:\n",
    "* PCA only works with numerical features\n",
    "* PCA is sensitive to scale and one should therefore consider standardizing the data before applying PCA\n",
    "* Consider removing or constraining outliers, since they can have an undue influence on the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:47.476199Z",
     "iopub.status.busy": "2021-11-16T12:28:47.475765Z",
     "iopub.status.idle": "2021-11-16T12:28:47.524424Z",
     "shell.execute_reply": "2021-11-16T12:28:47.523079Z",
     "shell.execute_reply.started": "2021-11-16T12:28:47.476167Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [\"area_total\", \"distance_from_financial_center\", \"distance_from_city_center\", 'distance','back_azi','fwd_azi']\n",
    "\n",
    "\n",
    "#normally distribute the data\n",
    "data_scaled = (data[features] - data[features].mean(axis=0)) / data[features].std(axis=0)\n",
    "\n",
    "# Create principal components\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "\n",
    "# Convert to dataframe\n",
    "component_names = [f\"PC{i+1}\" for i in range(data_pca.shape[1])]\n",
    "data_pca = pd.DataFrame(data_pca, columns=component_names)\n",
    "\n",
    "data_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:48.565942Z",
     "iopub.status.busy": "2021-11-16T12:28:48.565674Z",
     "iopub.status.idle": "2021-11-16T12:28:48.581626Z",
     "shell.execute_reply": "2021-11-16T12:28:48.580725Z",
     "shell.execute_reply.started": "2021-11-16T12:28:48.565915Z"
    }
   },
   "outputs": [],
   "source": [
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # transpose the matrix of loadings\n",
    "    columns=component_names,  # so the columns are the principal components\n",
    "    index=data[features].columns,  # and the rows are the original features\n",
    ")\n",
    "loadings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signs and magnitude of a PCA component loadings tell us what kind of variation it's captured. The first component shows a contrast between area_total and the different distance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:28:50.348403Z",
     "iopub.status.busy": "2021-11-16T12:28:50.347624Z",
     "iopub.status.idle": "2021-11-16T12:28:50.813017Z",
     "shell.execute_reply": "2021-11-16T12:28:50.812114Z",
     "shell.execute_reply.started": "2021-11-16T12:28:50.348364Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_variance(pca) #Shows the explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:30:09.892236Z",
     "iopub.status.busy": "2021-11-16T12:30:09.891924Z",
     "iopub.status.idle": "2021-11-16T12:30:12.435394Z",
     "shell.execute_reply": "2021-11-16T12:30:12.434719Z",
     "shell.execute_reply.started": "2021-11-16T12:30:09.892205Z"
    }
   },
   "outputs": [],
   "source": [
    "mi_scores = make_mi_scores(data_pca, data.price/data.area_total, discrete_features=False)\n",
    "plt.figure(dpi=100, figsize=(5, 5))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the MI score we can see that PC1 and PC2 seems to be highly informative and therefore could be considered to be taken as features. MI scores are explained and further emphasized in section 7.1\n",
    "\n",
    "There are other Feature Extraction techniques that would've been interesting to check out if there was more time:\n",
    "\n",
    "For Linear cases:\n",
    "* <b>ICA</b>\n",
    "* <b>LDA</b>\n",
    "\n",
    "For Non-linear cases:\n",
    "* <b>LLE</b>\n",
    "* <b>t-SNE</b>\n",
    "* <b>Autoencoders</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6.2\"></a> <br>\n",
    "## 6.2 Target Encoding\n",
    "\n",
    "Target encoding is a way of labeling categorical features. This method much like one-hot encoding or label encoding tries to transform a categorical value into numbers, just that target encoding utilizes the a target to create the encoding. This technique could be a useful way to categorize the features street and address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:30:14.275488Z",
     "iopub.status.busy": "2021-11-16T12:30:14.274848Z",
     "iopub.status.idle": "2021-11-16T12:30:15.198236Z",
     "shell.execute_reply": "2021-11-16T12:30:15.197346Z",
     "shell.execute_reply.started": "2021-11-16T12:30:14.275448Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data.copy()\n",
    "y = X.pop('area_total')\n",
    "\n",
    "X_encode = X.sample(frac=0.25)\n",
    "y_encode = y[X_encode.index]\n",
    "X_pretrain = X.drop(X_encode.index)\n",
    "y_train = y[X_pretrain.index]\n",
    "\n",
    "\n",
    "from category_encoders import MEstimateEncoder\n",
    "\n",
    "# Create the encoder instance. Choose m to control noise.\n",
    "encoder = MEstimateEncoder(cols=[\"street\"], m=5.0,random_state=20)\n",
    "\n",
    "# Fit the encoder on the encoding split.\n",
    "encoder.fit(X_encode, y_encode)\n",
    "\n",
    "# Encode the Zipcode column to create the final training data\n",
    "X_train = encoder.transform(X_pretrain)\n",
    "\n",
    "plt.figure(dpi=90)\n",
    "ax = sns.distplot(y, kde=False, norm_hist=True)\n",
    "ax = sns.kdeplot(X_train.street, color='r', ax=ax)\n",
    "ax.set_xlabel(\"area_total\")\n",
    "ax.legend(labels=['street', 'area_total']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:30:15.260448Z",
     "iopub.status.busy": "2021-11-16T12:30:15.260138Z",
     "iopub.status.idle": "2021-11-16T12:30:15.556921Z",
     "shell.execute_reply": "2021-11-16T12:30:15.555850Z",
     "shell.execute_reply.started": "2021-11-16T12:30:15.260408Z"
    }
   },
   "outputs": [],
   "source": [
    "make_mi_scores(X_train[[\"street\"]], X_train.price,discrete_features=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "# 7. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.1\"></a> <br>\n",
    "## 7.1 Mutual Information (MI)\n",
    "\n",
    "\n",
    "Mutual information is similar to correlation where it is a metric to measure a relationship between two quantities. The advantage of Mutual Information is that it can detect any kind of of relationship, whilst correlation detects linear relationships only. This way it measures the extent to which how much a feature can help us better predict another feature. In our case we would like to know how does each feature in our dataset measure in terms of MI paired with price. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:32:19.268275Z",
     "iopub.status.busy": "2021-11-16T12:32:19.267712Z",
     "iopub.status.idle": "2021-11-16T12:32:19.315977Z",
     "shell.execute_reply": "2021-11-16T12:32:19.314889Z",
     "shell.execute_reply.started": "2021-11-16T12:32:19.268218Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\",\n",
    "            \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'distance','back_azi','fwd_azi',\"floor/stories\",\n",
    "           \"distance_from_financial_center\", \"distance_from_city_center\"]\n",
    "\n",
    "# All categorical features are treated differently in MI\n",
    "categorical = [\"new\", \"parking\", \"heating\", \"constructed\", \"condition\", \"seller\", \"material\"]\n",
    "data_mi = data.copy()\n",
    "data_mi[categorical] = data_mi[categorical].convert_dtypes(int)\n",
    "discrete_features = data_mi[features].dtypes == int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:32:21.113253Z",
     "iopub.status.busy": "2021-11-16T12:32:21.112223Z",
     "iopub.status.idle": "2021-11-16T12:32:49.551961Z",
     "shell.execute_reply": "2021-11-16T12:32:49.551060Z",
     "shell.execute_reply.started": "2021-11-16T12:32:21.113207Z"
    }
   },
   "outputs": [],
   "source": [
    "mi_scores = make_mi_scores(data_mi[features], data_mi.price,discrete_features)\n",
    "plt.figure(dpi=100, figsize=(10, 12))\n",
    "plot_mi_scores(mi_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this it looks like heating is the least important feature of all the features. It does make sense that area is the most important features. This is very common domain knowledge in real estate, it is how ever interesting to see that MI is able to capture the importance as it does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.2\"></a> <br>\n",
    "## 7.2 ANOVA F-value\n",
    "\n",
    "This method estimates degree of linearity between the input feature and output feature. A high F-value means a high degree of linearity and vice versa, this metric only captures linear relationships between the pair of features which is the main disadvantage of this selection method as it cannot detect nonlinear relationships that might appear, something that MI could."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:32:49.553698Z",
     "iopub.status.busy": "2021-11-16T12:32:49.553483Z",
     "iopub.status.idle": "2021-11-16T12:32:51.138086Z",
     "shell.execute_reply": "2021-11-16T12:32:51.137256Z",
     "shell.execute_reply.started": "2021-11-16T12:32:49.553672Z"
    }
   },
   "outputs": [],
   "source": [
    "f_value = f_classif(data[features], data.price)\n",
    "\n",
    "# Create a bar chart for visualizing the F-values\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(x=features, height=f_value[0])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('F-value')\n",
    "plt.title('F-value Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see here area_total gives the highest linear relationship with price but there are obviously many relationships that are left out because this is only linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:32:55.636913Z",
     "iopub.status.busy": "2021-11-16T12:32:55.636010Z",
     "iopub.status.idle": "2021-11-16T12:32:57.853403Z",
     "shell.execute_reply": "2021-11-16T12:32:57.852793Z",
     "shell.execute_reply.started": "2021-11-16T12:32:55.636870Z"
    }
   },
   "outputs": [],
   "source": [
    "f_value = f_classif(data[features], data.price/data.area_total)\n",
    "\n",
    "# Create a bar chart for visualizing the F-values\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(x=features, height=f_value[0])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('F-value')\n",
    "plt.title('F-value Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price/area_total gives higher F-values for many more features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.3\"></a> <br>\n",
    "## 7.3 Variance Threshold\n",
    "\n",
    "This method tries to simply look at individual features and remove those that are below a certain set threshold. Obviously this filtering method assumes that features that do not vary a lot must themselves have low predictive power. But this method does not consider the connection between input and output features and therefore it should be used in combination with either F-value or MI-score selection methods as well as general domain knowledge such that one does not eliminate important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:32:57.855024Z",
     "iopub.status.busy": "2021-11-16T12:32:57.854706Z",
     "iopub.status.idle": "2021-11-16T12:32:58.289295Z",
     "shell.execute_reply": "2021-11-16T12:32:58.288423Z",
     "shell.execute_reply.started": "2021-11-16T12:32:57.854997Z"
    }
   },
   "outputs": [],
   "source": [
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\",\n",
    "            \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'back_azi','fwd_azi']\n",
    "# Create VarianceThreshold object to perform variance thresholding\n",
    "selector = VarianceThreshold()\n",
    "\n",
    "# Perform variance thresholding\n",
    "selector.fit_transform(data[features])\n",
    "\n",
    "\n",
    "# Create a bar chart for visualizing the variances\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(x=features, height=selector.variances_)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Variance')\n",
    "plt.title('Variance Comparison')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see that bathrooms, longitude and latitude, district, material as well as other features vary very little but this si because these are categorical features and as such one should only take the variance of numerical values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7.4\"></a> <br>\n",
    "## 7.4 Conclusion Of Feature Selection\n",
    "\n",
    "The methods that are explored by us are called Filter Methods, there are how ever other methods called Wrapper and Embeddded methods, where one tries to look at for instance feature importance to decide upon the feature selection.\n",
    "\n",
    "Wrapper Methods:\n",
    "* <b>Exhaustive Feature Selection</b>\n",
    "* <b>Sequential Forward Selection</b>\n",
    "* <b>Sequential Backward Selection</b>\n",
    "\n",
    "Embedded Methods:\n",
    "* <b>Feature Importance From Model</b>\n",
    "* <b>Using Selector Object for Selecting Features</b>\n",
    "\n",
    "From the observation of the methods that were explored, it seems like that MI gave the best overall information in terms of significance of the features in relation to the domain knowledge. The F-value whilst gave good information it is limited to linear relationships and thus it must be used in combination with other methods or omited entirely. Finally Variance Thresholding for a small portion of the feature set shows that area_total has the highest variation of the selected features, but even the low variance features are important as shown in the MI scores and as such it is only used to verify which features are really significant. What is worth noting is that all filter methods used MI, F-value and Variance thresholding showed that area_total is a really important feature, which resonates well with the domain knowledge of real estate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "# 8. Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.1\"></a> <br>\n",
    "## 8.1 Model Overview\n",
    "\n",
    "Using the Lazypredict package we can train a lot of different models included in the package and get a quick overview of the training time, the RMSE performance as well as other metrics to quickly get an idea of what models are potentially a good fit for the problem we are working on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:33:03.850362Z",
     "iopub.status.busy": "2021-11-16T12:33:03.850080Z",
     "iopub.status.idle": "2021-11-16T12:33:03.853847Z",
     "shell.execute_reply": "2021-11-16T12:33:03.852989Z",
     "shell.execute_reply.started": "2021-11-16T12:33:03.850330Z"
    }
   },
   "outputs": [],
   "source": [
    "# data_train, data_valid = model_selection.train_test_split(data, test_size=0.33, stratify=np.log(data.price).round())\n",
    "\n",
    "# X_train = data_train[features]\n",
    "# y_train = np.log1p(data_train['price'])\n",
    "# X_test = data_valid[features]\n",
    "# y_test = np.log1p(data_valid['price'])\n",
    "\n",
    "# reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n",
    "# models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n",
    "# print(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result it is easy to see that Xgboost and LGBM are both a good start. The only draw back is that this package does not contain Catboost as it is also a SOTA model worth checking out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.2\"></a> <br>\n",
    "## 8.2 Choosing Prediction Target\n",
    "\n",
    "In terms of setting up a model it is also worth mapping what types of prediction targets one should use. In particular there are a few targets to consider:\n",
    "* <b>Price Target</b>\n",
    "* <b>Log_Price Target</b>\n",
    "* <b>Price/sqm Target</b>\n",
    "* <b>Log_Price/sqm Target</b>\n",
    "\n",
    "The reasoning is motivated in the EDA, but a short version is that taking the log of the price gives it more of a normal distribution by making the target less skewed. How ever for the price/sqm, it generally makes more sense to predict that than the price itself as a house could have a high price because of various reasons, for example a lot of footage, rooms, etc but the price per squaremeter metric indicates the value of the apartment regardless of the size of it. And in a lot of cases there are really expensive apartments in the city center because of the square meter price but this is not very well emphasized if we look at the flat pricing - as a really cheap but big apartment could seem to be more of the expensive one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.3\"></a> <br>\n",
    "## 8.3 Validation Split\n",
    "In terms of validation split there are two possibilities:\n",
    "* <b>Split based on apartments</b>\n",
    "* <b>Split based on buildings</b>\n",
    "\n",
    "Obviously the apartment split will give a data leakage in the validation set vs the test set but while we do it in this notebook it is worth noting that before submitting models to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:33:11.296881Z",
     "iopub.status.busy": "2021-11-16T12:33:11.296564Z",
     "iopub.status.idle": "2021-11-16T12:33:11.320303Z",
     "shell.execute_reply": "2021-11-16T12:33:11.319641Z",
     "shell.execute_reply.started": "2021-11-16T12:33:11.296845Z"
    }
   },
   "outputs": [],
   "source": [
    "gs = model_selection.GroupShuffleSplit(n_splits=2, test_size=.33, random_state=0)\n",
    "train_index, valid_index = next(gs.split(data, groups=data.building_id))\n",
    "\n",
    "data_train = data.loc[train_index]\n",
    "data_valid = data.loc[valid_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:33:11.642568Z",
     "iopub.status.busy": "2021-11-16T12:33:11.641959Z",
     "iopub.status.idle": "2021-11-16T12:33:11.654836Z",
     "shell.execute_reply": "2021-11-16T12:33:11.653962Z",
     "shell.execute_reply.started": "2021-11-16T12:33:11.642509Z"
    }
   },
   "outputs": [],
   "source": [
    "#features that are used\n",
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\",\n",
    "            \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'distance','back_azi','fwd_azi',\"floor/stories\",\n",
    "           \"distance_from_financial_center\", \"distance_from_city_center\"]\n",
    "\n",
    "\n",
    "train_x = data_train[features]\n",
    "train_y = np.log1p(data_train['price']/data_train[\"area_total\"])\n",
    "test_x = data_valid[features]\n",
    "test_y = data_valid['price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.4\"></a> <br>\n",
    "## 8.4 Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:33:14.551099Z",
     "iopub.status.busy": "2021-11-16T12:33:14.550811Z",
     "iopub.status.idle": "2021-11-16T12:34:45.115003Z",
     "shell.execute_reply": "2021-11-16T12:34:45.114070Z",
     "shell.execute_reply.started": "2021-11-16T12:33:14.551067Z"
    }
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "        'base_score' : 0.5,\n",
    "        'booster' : 'gbtree',\n",
    "        'colsample_bylevel' : 1,\n",
    "        'gamma' : 0,\n",
    "        'max_delta_step' : 0,\n",
    "        'n_jobs' : -1,\n",
    "        'nthread' : None,\n",
    "        'objective' : 'reg:squarederror',\n",
    "        'scale_pos_weight' : 1,\n",
    "        'seed' : None,\n",
    "        'lambda': 0.0024064014952485785, \n",
    "         'alpha': 0.001541503784279617, \n",
    "        'colsample_bytree': 0.43152225018148443, \n",
    "       'subsample': 0.8078473020517652, \n",
    "       'learning_rate': 0.013367834721822036, \n",
    "       'n_estimators': 5235, \n",
    "     'random_state': 291, \n",
    "      'max_depth': 9, \n",
    "    'min_child_weight': 13\n",
    "}\n",
    "\n",
    "model_xgb = XGBRegressor(**param)\n",
    "\n",
    "model_xgb.fit(train_x,train_y)\n",
    "\n",
    "xgb_preds = model_xgb.predict(test_x)\n",
    "root_mean_squared_log_error(test_y, y_pred=np.expm1(xgb_preds)*data_valid[\"area_total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.5\"></a> <br>\n",
    "## 8.5 Lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:34:45.117404Z",
     "iopub.status.busy": "2021-11-16T12:34:45.116904Z",
     "iopub.status.idle": "2021-11-16T12:35:10.894775Z",
     "shell.execute_reply": "2021-11-16T12:35:10.893892Z",
     "shell.execute_reply.started": "2021-11-16T12:34:45.117359Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params = {'objective' : 'regression',\n",
    "    \"metric\": \"root_mean_squared_error\",\n",
    "    'random_state': 2020,\n",
    "    \"n_estimators\": 3000,\n",
    "    'boosting_type': 'gbdt', #better than dart\n",
    "    \"n_jobs\": -1,\n",
    " 'learning_rate': 0.009902216010560466, \n",
    " 'num_iterations': 9853, \n",
    " 'n_estimators': 2200, \n",
    " 'max_bin': 1145, \n",
    " 'num_leaves': 992, \n",
    " 'min_data_in_leaf': 21, \n",
    " 'min_sum_hessian_in_leaf': 6, \n",
    " 'bagging_fraction': 0.7553160099162841, \n",
    " 'bagging_freq': 1, \n",
    " 'max_depth': 5, \n",
    " 'lambda_l1': 0.001047756084491848, \n",
    " 'lambda_l2': 0.5231817241800534, \n",
    " 'min_gain_to_split': 0.01715842845568677\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_lgbm = LGBMRegressor(**best_params)  \n",
    "\n",
    "model_lgbm.fit(train_x,train_y,verbose=False)\n",
    "\n",
    "lgbm_preds = model_lgbm.predict(test_x)\n",
    "root_mean_squared_log_error(test_y, y_pred=np.expm1(lgbm_preds)*data_valid[\"area_total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.6\"></a> <br>\n",
    "## 8.6 Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:35:10.896616Z",
     "iopub.status.busy": "2021-11-16T12:35:10.896299Z",
     "iopub.status.idle": "2021-11-16T12:38:04.201722Z",
     "shell.execute_reply": "2021-11-16T12:38:04.201051Z",
     "shell.execute_reply.started": "2021-11-16T12:35:10.896575Z"
    }
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "\"objective\": \"RMSE\",\n",
    "'depth': 8, \n",
    " 'reg_lambda': 0.6424630162452156, \n",
    " 'learning_rate': 0.008856338969505724, \n",
    " 'n_estimators': 5356, \n",
    " 'max_bin': 1042, \n",
    " 'random_state': 1695, \n",
    " 'subsample': 0.4474582804576312}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_catb = CatBoostRegressor(**param)  \n",
    "\n",
    "model_catb.fit(train_x,train_y, verbose=False)\n",
    "\n",
    "catb_preds = model_catb.predict(test_x)\n",
    "root_mean_squared_log_error(test_y, y_pred=np.expm1(catb_preds)*data_valid[\"area_total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.7\"></a> <br>\n",
    "## 8.7 Stacking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:38:04.204100Z",
     "iopub.status.busy": "2021-11-16T12:38:04.203650Z",
     "iopub.status.idle": "2021-11-16T12:38:04.208328Z",
     "shell.execute_reply": "2021-11-16T12:38:04.207616Z",
     "shell.execute_reply.started": "2021-11-16T12:38:04.204066Z"
    }
   },
   "outputs": [],
   "source": [
    "#Takes 1 hour and 40 minutes to run so commented out\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "stacked_model = StackingCVRegressor(regressors=(model_xgb,model_lgbm,model_catb),\n",
    "                                meta_regressor=model_xgb, #our best individual model becomes the META\n",
    "                                use_features_in_secondary=True,\n",
    "                                   verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "#stacked_model.fit(train_x,train_y)\n",
    "\n",
    "#stacked_preds = stacked_model.predict(test_x)\n",
    "#root_mean_squared_log_error(test_y, y_pred=np.expm1(stacked_preds)*data_valid[\"area_total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.8\"></a> <br>\n",
    "## 8.8 Weight Averaging\n",
    "\n",
    "Weighting is based off of the base performance of each model, in this case the stacked_predictions is also perceived as a base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:38:04.210073Z",
     "iopub.status.busy": "2021-11-16T12:38:04.209679Z",
     "iopub.status.idle": "2021-11-16T12:38:04.224946Z",
     "shell.execute_reply": "2021-11-16T12:38:04.223972Z",
     "shell.execute_reply.started": "2021-11-16T12:38:04.210044Z"
    }
   },
   "outputs": [],
   "source": [
    "weighting_preds = np.average(\n",
    "    [np.expm1(xgb_preds)*data_valid[\"area_total\"],\n",
    "     np.expm1(lgbm_preds)*data_valid[\"area_total\"],\n",
    "     np.expm1(catb_preds)*data_valid[\"area_total\"]\n",
    "     #np.expm1(stacked_preds)*data_valid[\"area_total\"]\n",
    "    ],\n",
    "    weights = 1 / np.array([0.1932,  0.1961, 0.1968]) ** 6,   \n",
    "    axis=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:38:04.226603Z",
     "iopub.status.busy": "2021-11-16T12:38:04.226259Z",
     "iopub.status.idle": "2021-11-16T12:38:04.247903Z",
     "shell.execute_reply": "2021-11-16T12:38:04.246719Z",
     "shell.execute_reply.started": "2021-11-16T12:38:04.226570Z"
    }
   },
   "outputs": [],
   "source": [
    "rmsle_xgb = root_mean_squared_log_error(test_y, y_pred=np.expm1(xgb_preds)*data_valid[\"area_total\"])\n",
    "rmsle_lgb = root_mean_squared_log_error(test_y, y_pred=np.expm1(lgbm_preds)*data_valid[\"area_total\"])\n",
    "rmsle_cat = root_mean_squared_log_error(test_y, y_pred=np.expm1(catb_preds)*data_valid[\"area_total\"])\n",
    "#rmsle_stacking = root_mean_squared_log_error(test_y, y_pred=np.expm1(stacked_preds)*data_valid[\"area_total\"])\n",
    "rmsle_weighting = root_mean_squared_log_error(test_y, y_pred=weighting_preds)\n",
    "print(\"RMSLE xgb:\", rmsle_xgb)\n",
    "print(\"RMSLE lgbm:\", rmsle_lgb)\n",
    "print(\"RMSLE catb:\", rmsle_cat)\n",
    "#print(\"RMSLE stacking:\", rmsle_stacking)\n",
    "print(\"RMSLE weighting:\", rmsle_weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that weighting and obviously if stacking is ran that these do give improvements on top of the base models. Stacking takes a whole lot of time run how ever. So it is commented out but one is welcome to uncomment it and try it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8.9\"></a> <br>\n",
    "## 8.9 Other Tried Models\n",
    "There are a few other models we tried but that did not have much success in our limited time of trying them out. Here is an exhaustive list of that:\n",
    "* <b>Snapboost</b>\n",
    "* <b>Bagging a model (for example xgboost)</b>\n",
    "* <b>Random Forest</b>\n",
    "* <b>Extra Trees</b>\n",
    "* <b>Support Vector Machine Regressor</b>\n",
    "* <b>Lasso and Ridge Regressors</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:12.125413Z",
     "iopub.status.busy": "2021-11-16T12:39:12.125087Z",
     "iopub.status.idle": "2021-11-16T12:39:12.131333Z",
     "shell.execute_reply": "2021-11-16T12:39:12.130375Z",
     "shell.execute_reply.started": "2021-11-16T12:39:12.125377Z"
    }
   },
   "outputs": [],
   "source": [
    "#Snapboost\n",
    "param = {\"objective\": \"mse\",\n",
    "          'random_state': 830, \n",
    "          'learning_rate': 0.0036201545027461845,\n",
    "          'num_round': 4988, \n",
    "          'hist_nbins': 246, \n",
    "          'colsample_bytree': 0.9028656800598056, \n",
    "          'subsample': 0.2080280241767715, \n",
    "          'tree_select_probability': 0.5206241285414699, \n",
    "          'lambda_l2': 0.06806535901164175, \n",
    "          'regularizer': 0.7392140536568328, \n",
    "          'max_depth': 241}\n",
    "\n",
    "#model_snap = BoostingMachineRegressor(**param)\n",
    "#model_snap.fit(np.array(train_x),np.array(train_y))\n",
    "\n",
    "#snap_preds = model_snap.predict(np.array(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:13.062984Z",
     "iopub.status.busy": "2021-11-16T12:39:13.062695Z",
     "iopub.status.idle": "2021-11-16T12:39:13.067469Z",
     "shell.execute_reply": "2021-11-16T12:39:13.066748Z",
     "shell.execute_reply.started": "2021-11-16T12:39:13.062954Z"
    }
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "param = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": 2000,\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 4,\n",
    "    'min_samples_split': 4, #from 2 or error\n",
    "    'min_samples_leaf': 2,\n",
    "    }\n",
    "    \n",
    "\n",
    "#model =RandomForestRegressor(**param)\n",
    "#model.fit(train_x,train_y)\n",
    "\n",
    "#preds = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:13.937156Z",
     "iopub.status.busy": "2021-11-16T12:39:13.936546Z",
     "iopub.status.idle": "2021-11-16T12:39:13.942355Z",
     "shell.execute_reply": "2021-11-16T12:39:13.941411Z",
     "shell.execute_reply.started": "2021-11-16T12:39:13.937065Z"
    }
   },
   "outputs": [],
   "source": [
    "#Bagging Model\n",
    "\n",
    "param = {\n",
    "   \"base_estimator\":  model_xgb,\n",
    "    'random_state': 2020,\n",
    "\n",
    "    \"n_estimators\": 2,\n",
    "    \"max_samples\": 0.5, #higher variance (0.1-0.99), higher bias (1, inf)\n",
    "    #\"bootstrap\": False,\n",
    "    #\"max_features\": train_x.shape[1],\n",
    "    #\"oob_score\": False,\n",
    "    #\"bootstrap_features\": False\n",
    "}\n",
    "\n",
    "#model = BaggingRegressor(**param)  \n",
    "\n",
    "#model.fit(train_x,train_y)\n",
    "\n",
    "#preds = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:14.770550Z",
     "iopub.status.busy": "2021-11-16T12:39:14.769605Z",
     "iopub.status.idle": "2021-11-16T12:39:14.776105Z",
     "shell.execute_reply": "2021-11-16T12:39:14.775293Z",
     "shell.execute_reply.started": "2021-11-16T12:39:14.770499Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extra Trees\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "et_model = ExtraTreesRegressor(n_estimators=1000, max_depth=10, max_features=0.3, n_jobs=-1, random_state=0)\n",
    "\n",
    "#et_model.fit(train_x,train_y)\n",
    "\n",
    "##preds = et_model.predict(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a> <br>\n",
    "# 9. Optuna Optimization\n",
    "\n",
    "As one can observe in section 6 all the models have very odd/specific hyperparamters. This is because this is a result of the hyperparameter optimization that was done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9.1\"></a> <br>\n",
    "## 9.1 Hyperparameter Tuning Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:16.023299Z",
     "iopub.status.busy": "2021-11-16T12:39:16.022895Z",
     "iopub.status.idle": "2021-11-16T12:39:16.032259Z",
     "shell.execute_reply": "2021-11-16T12:39:16.031332Z",
     "shell.execute_reply.started": "2021-11-16T12:39:16.023269Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial,data=data):\n",
    "    \n",
    "    train_x = data_train[features]\n",
    "    train_y = np.log1p(data_train['price']/data_train[\"area_total\"])\n",
    "    test_x = data_valid[features]\n",
    "    test_y = data_valid['price']\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    param = {\n",
    "      'base_score' : 0.5,\n",
    "        'booster' : 'gbtree',\n",
    "        'colsample_bylevel' : 1,\n",
    "        'gamma' : 0,\n",
    "        'max_delta_step' : 0,\n",
    "        'n_jobs' : -1,\n",
    "        'nthread' : None,\n",
    "        'objective' : 'reg:squarederror',\n",
    "        'scale_pos_weight' : 1,\n",
    "        'seed' : None,\n",
    "\n",
    "\n",
    "\n",
    "        #'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process        \n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 1),\n",
    "        'alpha': trial.suggest_loguniform('alpha',  1e-3, 1),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.3,1.0),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.3,1.0),\n",
    "        'learning_rate': trial.suggest_uniform('learning_rate', 0.008, 0.018),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 500,1000), #should be between 500-4000 taken down for faster running in the long notebook\n",
    "        'random_state': trial.suggest_int('random_state',0, 2000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 30),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**param)  \n",
    "    \n",
    "    model.fit(train_x,train_y)\n",
    "    \n",
    "    preds = model.predict(test_x)\n",
    "    \n",
    "    rmse = root_mean_squared_log_error(y_true=test_y, y_pred=np.expm1(preds)*test_x[\"area_total\"])\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:16.502159Z",
     "iopub.status.busy": "2021-11-16T12:39:16.501881Z",
     "iopub.status.idle": "2021-11-16T12:39:26.942212Z",
     "shell.execute_reply": "2021-11-16T12:39:26.941296Z",
     "shell.execute_reply.started": "2021-11-16T12:39:16.502129Z"
    }
   },
   "outputs": [],
   "source": [
    "#Trial is usually higher than 1 but for this case it is set low for kaggle to run faster\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9.2\"></a> <br>\n",
    "## 9.2 Hyperparameter Tuning Lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:26.944172Z",
     "iopub.status.busy": "2021-11-16T12:39:26.943940Z",
     "iopub.status.idle": "2021-11-16T12:39:26.954159Z",
     "shell.execute_reply": "2021-11-16T12:39:26.953538Z",
     "shell.execute_reply.started": "2021-11-16T12:39:26.944137Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial,data=data):\n",
    "    \n",
    "    train_x = data_train[features]\n",
    "    train_y = np.log1p(data_train['price']/data_train[\"area_total\"])\n",
    "    test_x = data_valid[features]\n",
    "    test_y = data_valid['price']\n",
    "    \n",
    "    \n",
    "    param = {\n",
    "    'objective' : 'regression',\n",
    "    \"metric\": \"root_mean_squared_error\",\n",
    "    'random_state': 2020,\n",
    "    \"n_estimators\": 3000,\n",
    "    'boosting_type': 'gbdt', #better than dart\n",
    "    \"n_jobs\": -1,\n",
    "        \n",
    "    #Increases accuracy:\n",
    "    'learning_rate' : trial.suggest_uniform('learning_rate',0.009, 0.01), #small learning rate and large iterations\n",
    "    \"num_iterations\": trial.suggest_int(\"num_iterations\",1000, 3000),#should be between 500-5000 taken down for faster running in the long notebook\n",
    "    \"n_estimators\": trial.suggest_int(\"n_estimators\", 500,1000), #should be between 500-5000 taken down for faster running in the long notebook\n",
    "    \"max_bin\" : trial.suggest_int(\"max_bin\",50,2000), #large max-bin - may be slower for large values\n",
    "    \"num_leaves\" : trial.suggest_int(\"num_leaves\", 50, 2000), #may cause overfitting for large values\n",
    "        \n",
    "    \n",
    "    #deal with overfitting:\n",
    "    \"min_data_in_leaf\" : trial.suggest_int(\"min_data_in_leaf\", 20, 60),\n",
    "    \"min_sum_hessian_in_leaf\" : trial.suggest_int(\"min_sum_hessian_in_leaf\", 1, 10),\n",
    "        \n",
    "    #as as set against overfitting:\n",
    "    \"bagging_fraction\" : trial.suggest_uniform(\"bagging_fraction\", 0.1, 1),\n",
    "    \"bagging_freq\" : trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "    \"max_depth\" : trial.suggest_int(\"max_depth\",5,30),\n",
    "\n",
    "    #Regularization:\n",
    "    \"lambda_l1\" :  trial.suggest_uniform(\"lambda_l1\", 0.001, 1),\n",
    "    \"lambda_l2\" :  trial.suggest_uniform(\"lambda_l2\", 0.001, 1),\n",
    "    \"min_gain_to_split\" :trial.suggest_uniform(\"min_gain_to_split\", 0.001, 0.1),\n",
    "        \n",
    "        \n",
    "\n",
    "    }\n",
    "    \n",
    "    \n",
    "\n",
    "    model =LGBMRegressor(**param)\n",
    "    model.fit(train_x,train_y)\n",
    "\n",
    "    preds = model.predict(test_x)\n",
    "    rmse = root_mean_squared_log_error(y_true=test_y, y_pred=np.expm1(preds)*test_x[\"area_total\"])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:26.955801Z",
     "iopub.status.busy": "2021-11-16T12:39:26.955419Z",
     "iopub.status.idle": "2021-11-16T12:39:46.023818Z",
     "shell.execute_reply": "2021-11-16T12:39:46.023200Z",
     "shell.execute_reply.started": "2021-11-16T12:39:26.955766Z"
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=2)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9.3\"></a> <br>\n",
    "## 9.3 Hyperparameter Tuning Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:46.025955Z",
     "iopub.status.busy": "2021-11-16T12:39:46.025484Z",
     "iopub.status.idle": "2021-11-16T12:39:46.034798Z",
     "shell.execute_reply": "2021-11-16T12:39:46.033920Z",
     "shell.execute_reply.started": "2021-11-16T12:39:46.025923Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(trial,data=data):\n",
    "\n",
    "    \n",
    "    train_x = data_train[features]\n",
    "    train_y = np.log1p(data_train['price']/data_train[\"area_total\"])\n",
    "    test_x = data_valid[features]\n",
    "    test_y = data_valid['price']\n",
    "\n",
    "\n",
    "    param = {\n",
    "    \"objective\": \"RMSE\",\n",
    "    'random_state': 2020,\n",
    "    #\"bootstrap_type\": \"Bayesian\",\n",
    "    \"thread_count\": -1, #basically n_jobs\n",
    "        \n",
    "    #regularization/preventing overfitting\n",
    "    'depth': trial.suggest_int('depth',4,10), \n",
    "    'reg_lambda': trial.suggest_loguniform('lambda', 0.008, 2),\n",
    "\n",
    "    #Increases accuracy:\n",
    "    'learning_rate' : trial.suggest_uniform('learning_rate',0.0001,0.01),\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 500,1000), #should be between 500-4000 taken down for faster running in the long notebook\n",
    "    \"max_bin\": trial.suggest_int(\"max_bin\",50,2000),\n",
    "    'random_state': trial.suggest_int('random_state', 0,2000),\n",
    "        \n",
    "     #misc:   \n",
    "    'subsample': trial.suggest_uniform('subsample', 0.3,1.0),\n",
    "    \n",
    "    }\n",
    "\n",
    "        \n",
    "\n",
    "    model = CatBoostRegressor(**param)  \n",
    "\n",
    "    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100, verbose=False)\n",
    "\n",
    "    preds = model.predict(test_x)\n",
    "\n",
    "    rmse = root_mean_squared_log_error(y_true=test_y, y_pred=np.expm1(preds)*test_x[\"area_total\"])\n",
    "       \n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:46.036480Z",
     "iopub.status.busy": "2021-11-16T12:39:46.036184Z",
     "iopub.status.idle": "2021-11-16T12:39:50.668298Z",
     "shell.execute_reply": "2021-11-16T12:39:50.667409Z",
     "shell.execute_reply.started": "2021-11-16T12:39:46.036448Z"
    }
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"9.4\"></a> <br>\n",
    "## 9.4 Other Model Tuning\n",
    "\n",
    "Here are some basic setups used for Snapboost and Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:50.670155Z",
     "iopub.status.busy": "2021-11-16T12:39:50.669826Z",
     "iopub.status.idle": "2021-11-16T12:39:50.680673Z",
     "shell.execute_reply": "2021-11-16T12:39:50.679748Z",
     "shell.execute_reply.started": "2021-11-16T12:39:50.670111Z"
    }
   },
   "outputs": [],
   "source": [
    "#Snapboost\n",
    "def objective(trial,data=data):\n",
    "\n",
    "    train_x = data_train[features]\n",
    "    train_y = np.log1p(data_train['price'])\n",
    "    test_x = data_valid[features]\n",
    "    test_y = np.log1p(data_valid['price'])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    param = {\n",
    "    \"objective\": \"mse\",\n",
    "    \"n_jobs\": 1,\n",
    "        \n",
    "    #increase accuracy\n",
    "    \"random_state\":  trial.suggest_int(\"random_state\",1,1000),\n",
    "    \"learning_rate\": trial.suggest_uniform(\"learning_rate\",0.0001, 0.01),\n",
    "    \"num_round\": trial.suggest_int(\"num_round\",500,5000),\n",
    "\n",
    "    #misc  \n",
    "    \"hist_nbins\": trial.suggest_int(\"hist_nbins\",1,256),\n",
    "    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\",0,0.99),\n",
    "    \"subsample\": trial.suggest_uniform(\"subsample\",0.1,1.0),\n",
    "    \"tree_select_probability\": trial.suggest_uniform(\"tree_select_probability\", 0.0,1.0),\n",
    "    \n",
    "   \n",
    "        \n",
    "    #regularization\n",
    "    \"lambda_l2\": trial.suggest_uniform(\"lambda_l2\",0.0,1.0),\n",
    "    \"regularizer\": trial.suggest_uniform(\"regularizer\",0.0,10.0),\n",
    "     \"max_depth\": trial.suggest_int(\"max_depth\",1,1000),\n",
    "\n",
    "    }\n",
    "    \n",
    "\n",
    "    model = BoostingMachineRegressor(**param)\n",
    "    model.fit(np.array(train_x),np.array(train_y))\n",
    "\n",
    "    preds = model.predict(np.array(test_x))\n",
    "    rmse = root_mean_squared_log_error(y_true=np.expm1(test_y), y_pred=np.expm1(preds))\n",
    "\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:50.682677Z",
     "iopub.status.busy": "2021-11-16T12:39:50.682174Z",
     "iopub.status.idle": "2021-11-16T12:39:50.701462Z",
     "shell.execute_reply": "2021-11-16T12:39:50.700344Z",
     "shell.execute_reply.started": "2021-11-16T12:39:50.682626Z"
    }
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "def objective(trial,data=data):\n",
    "\n",
    "    train_x = data_train[features]\n",
    "    train_y = np.log1p(data_train['price'])\n",
    "    test_x = data_valid[features]\n",
    "    test_y = np.log1p(data_valid['price'])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    param = {\n",
    "    \"n_jobs\": -1,\n",
    "    \"random_state\": trial.suggest_int(\"random_state\", 0, 2000),\n",
    "    'n_estimators': trial.suggest_int('n_estimators', 50, 10000),\n",
    "    'max_depth': trial.suggest_int('max_depth', 4, 50),\n",
    "    'min_samples_split': trial.suggest_int('min_samples_split', 2, 150), #from 2 or error\n",
    "    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100),\n",
    "    }\n",
    "    \n",
    "\n",
    "    model =RandomForestRegressor(**param)\n",
    "    model.fit(train_x,train_y)\n",
    "\n",
    "    preds = model.predict(test_x)\n",
    "    rmse = root_mean_squared_log_error(y_true=np.expm1(test_y), y_pred=np.expm1(preds))\n",
    "    \n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a> <br>\n",
    "# 10. Attempted and Documented Model Pipelines\n",
    "\n",
    "NOTE: the fitting in the attemps have been commented out such that the notebook runs faster - either skip this part and go to the model interpretation or uncoment the models to run the different attempts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.1\"></a> <br>\n",
    "## 10.1 Attempt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:50.704084Z",
     "iopub.status.busy": "2021-11-16T12:39:50.703449Z",
     "iopub.status.idle": "2021-11-16T12:39:54.261673Z",
     "shell.execute_reply": "2021-11-16T12:39:54.260932Z",
     "shell.execute_reply.started": "2021-11-16T12:39:50.704034Z"
    }
   },
   "outputs": [],
   "source": [
    "#Attempt 1 - 11.10.2021 - 0.16080 on test set\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "# features = [ \"area_total\", \"rooms\", \"floor\",\"new\",\"distance_from_city_center\",\n",
    "#         \"latitude\", \"longitude\",\"district\", \"constructed\", \"material\", \"stories\"]\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\", \"elevatern\",\"distance_from_city_center\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]\n",
    "#TDONE floor, celing, rooms   \"new\" , constructed condition\n",
    "# TODO area kitchen , area living\n",
    "\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1) \n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "        \n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "                        \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "    else:\n",
    "        mean = data[feature].mean()\n",
    "        data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "        data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "data['area_total'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "import sklearn.model_selection as model_selection\n",
    "data_train, data_valid = model_selection.train_test_split(data, test_size=0.33, stratify=np.log(data.price).round())\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "X_train = data_train[features]\n",
    "y_train = np.log1p(data_train['price'])\n",
    "X_valid = data_valid[features]\n",
    "y_valid = np.log1p(data_valid['price'])\n",
    "#Trial 26 finished with value: 0.12977325584218719 and parameters: {'lambda': 0.2287729489989326, 'alpha': 0.021096319890667407, 'colsample_bytree': 0.5144984086781564, \n",
    "     #'subsample': 0.42023355655422495, 'learning_rate': 0.01693820796093592, 'n_estimators': 3977, 'max_depth': 23, 'random_state': 2020, 'min_child_weight': 5}. Best is trial 26 with value: 0.12977325584218719.\n",
    "    \n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5144984086781564, gamma=0, learning_rate=0.01693820796093592, max_delta_step=0,\n",
    "       max_depth=23, min_child_weight=5, n_estimators=3977,\n",
    "       n_jobs=1, nthread=None, objective='reg:squarederror', random_state=2020, # squarederror  reg:squaredlogerror   reg:squarederror\n",
    "       reg_alpha=0.021096319890667407, reg_lambda=0.2287729489989326, scale_pos_weight=1, seed=None, subsample=0.42023355655422495)\n",
    "\n",
    "\n",
    "\n",
    "#model.fit(X_train, y_train)\n",
    "#y_train_hat = model.predict(X_train)\n",
    "#y_valid_hat = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.2\"></a> <br>\n",
    "## 10.2 Attempt 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:39:54.264563Z",
     "iopub.status.busy": "2021-11-16T12:39:54.264061Z",
     "iopub.status.idle": "2021-11-16T12:40:09.710247Z",
     "shell.execute_reply": "2021-11-16T12:40:09.709192Z",
     "shell.execute_reply.started": "2021-11-16T12:39:54.264527Z"
    }
   },
   "outputs": [],
   "source": [
    "#Attempt 2 - 12.10.2021 - 0.15712 on test set\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "# features = [ \"area_total\", \"rooms\", \"floor\",\"new\",\"distance_from_city_center\",\n",
    "#         \"latitude\", \"longitude\",\"district\", \"constructed\", \"material\", \"stories\"]\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\", \"elevatern\",\"distance_from_city_center\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]\n",
    "#TDONE floor, celing, rooms   \"new\" , constructed condition\n",
    "# TODO area kitchen , area living\n",
    "\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1) \n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "        \n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "                        \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "    else:\n",
    "        mean = data[feature].mean()\n",
    "        data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "        data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "data['area_total'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "#best xgboost model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "    \n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5144984086781564, gamma=0, learning_rate=0.01693820796093592, max_delta_step=0,\n",
    "       max_depth=23, min_child_weight=5, n_estimators=3977,\n",
    "       n_jobs=1, nthread=None, objective='reg:squarederror', random_state=2020, # squarederror  reg:squaredlogerror   reg:squarederror\n",
    "       reg_alpha=0.021096319890667407, reg_lambda=0.2287729489989326, scale_pos_weight=1, seed=None, subsample=0.42023355655422495)\n",
    "\n",
    "\n",
    "#model.fit(train_x,train_y)\n",
    "#xgb_preds = model.predict(test_x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#next model\n",
    "#Second best Xgboost model pipeline\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "# SIGMA\n",
    "# Filling missing long lat in the Test set\n",
    "#55.568139, 37.481831 - fixing nans\n",
    "\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#55.544066, 37.482317 - Fixing negative numbers\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Blown up coordinates outside moscow fixed:\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "#fixing test_data districts:\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "# data[[\"area_total\",\"area_kitchen\",\"area_living\",\"bathrooms_private\", \"bathrooms_shared\",\"balconies\",\"loggias\"]][data.area_living + data.area_kitchen > data.area_total]\n",
    "\n",
    "# Add a new feature of distance from center\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "# Modify/ add the eleveator feature for both test and train\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "# features = [ \"area_total\", \"rooms\", \"floor\",\"new\",\"distance_from_city_center\",\n",
    "#         \"latitude\", \"longitude\",\"district\", \"constructed\", \"material\", \"stories\"]\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\", \"elevatern\",\"distance_from_city_center\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]\n",
    "#TDONE floor, celing, rooms   \"new\" , constructed condition\n",
    "# TODO area kitchen , area living\n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "\n",
    "        \n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "\n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "\n",
    "        \n",
    "    elif feature == 'stories':\n",
    "        \n",
    "        idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss.size):\n",
    "            max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "            data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "        \n",
    "        idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss_test.size):\n",
    "            max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "            data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "            \n",
    "    elif feature == 'material':\n",
    "        data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "        data.material[data.material==6] = 5.0 #stalin to 5\n",
    "        \n",
    "        data_test.material[data_test.material==5] = 2.0\n",
    "        data_test.material[data_test.material==6] = 5.0\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "            \n",
    "            \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "\n",
    "    else:\n",
    "        mean = data[feature].mean()\n",
    "        #print('Not Categorical',feature)\n",
    "        data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "        data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "\n",
    "data['area_total'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.6185033815828214, gamma=0, learning_rate=0.015430795026041527, max_delta_step=0,\n",
    "       max_depth=12, min_child_weight=2, n_estimators=3210,\n",
    "       n_jobs=1, nthread=None, objective='reg:squarederror', random_state=2020, # squarederror  reg:squaredlogerror   reg:squarederror\n",
    "       reg_alpha=0.13226917041287767, reg_lambda=0.9825323223526006, scale_pos_weight=1, seed=None, subsample=0.7155648654032974)\n",
    "\n",
    " \n",
    "#model.fit(train_x,train_y)\n",
    "#xgb_preds_scnd_best_model = model.predict(test_x)\n",
    "\n",
    "\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from numpy.random import choice\n",
    "\n",
    "# Filling missing long lat in the Test set\n",
    "#55.568139, 37.481831 - fixing nans\n",
    "\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#55.544066, 37.482317 - Fixing negative numbers\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Blown up coordinates outside moscow fixed:\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "#fixing test_data districts:\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "# data[[\"area_total\",\"area_kitchen\",\"area_living\",\"bathrooms_private\", \"bathrooms_shared\",\"balconies\",\"loggias\"]][data.area_living + data.area_kitchen > data.area_total]\n",
    "\n",
    "# Add a new feature of distance from center\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "# Modify/ add the eleveator feature for both test and train\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"elevatern\",\"distance_from_city_center\", \"bathrooms_total\", \"bathrooms_shared\", \n",
    "            \"bathrooms_private\", 'parking',\"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"material\", \"stories\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Revisit\n",
    "# We removed windows court and windows street as they did not seem to corelate with price,\n",
    "# total_balconies == 0 instead 1 .\n",
    "# \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "    \n",
    "    elif feature == 'seller':\n",
    "        \n",
    "        list_of_candidates = [0,1,2,3]\n",
    "        # 14455 , owener 0, \n",
    "        probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "        number_of_items_to_pick = data['seller'].isna().sum()\n",
    "        number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "                      p=probability_distribution)\n",
    "        draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "                      p=probability_distribution)\n",
    "\n",
    "        data['seller'][data.seller.isna()] = draw\n",
    "        data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "        \n",
    "    elif feature == 'area_kitchen' or feature == 'area_living':\n",
    "        if feature == 'area_kitchen':\n",
    "           \n",
    "            percentage_area_data = pd.DataFrame()\n",
    "            percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "            percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "            mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "            mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "            #to omit bugs\n",
    "            data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "            data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "            data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "            data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "            data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "            data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "            #test_set\n",
    "            data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "            data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "            data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "            data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "            data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "            data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "        data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "\n",
    "        \n",
    "    elif feature == 'stories':\n",
    "        \n",
    "        idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss.size):\n",
    "            max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "            data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "        \n",
    "        idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss_test.size):\n",
    "            max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "            data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "            \n",
    "    elif feature == 'material':\n",
    "        data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "        data.material[data.material==6] = 5.0 #stalin to 5\n",
    "        \n",
    "        data_test.material[data_test.material==5] = 2.0\n",
    "        data_test.material[data_test.material==6] = 5.0\n",
    "        \n",
    "        data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "        data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "            \n",
    "            \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "\n",
    "    elif feature == 'condition': # Can also merge with 0.0 i.e. the undecorated class. but for now we created new class\n",
    "        data[\"condition\"][data.condition.isna() ] = 4.0 \n",
    "        data_test[\"condition\"][data_test.condition.isna() ] = 4.0 \n",
    "    else:\n",
    "        pass\n",
    "        #         mean = data[feature].mean()\n",
    "#         #print('Not Categorical',feature)\n",
    "#         data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "#         data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Submission\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "best_params = {'objective' : 'regression',\n",
    "                \"metric\": \"root_mean_squared_error\",\n",
    "                'random_state': 2020,\n",
    "                \"n_estimators\": 3000,\n",
    "                'boosting_type': 'gbdt',\n",
    "               'learning_rate': 0.009545503382688678, \n",
    "               'num_iterations': 6000, \n",
    "               'n_estimators': 3467, \n",
    "               'max_bin': 1329, \n",
    "               'num_leaves': 84, \n",
    "               'min_data_in_leaf': 21, \n",
    "               'min_sum_hessian_in_leaf': 10, \n",
    "               'bagging_fraction': 0.6138769224842795, \n",
    "               'bagging_freq': 1, \n",
    "               'max_depth': 24, \n",
    "               'lambda_l1': 0.03828165214380662, \n",
    "               'lambda_l2': 0.9439756453034776, \n",
    "               'min_gain_to_split': 0.006068058433178841}\n",
    "\n",
    "\n",
    "\n",
    "model = LGBMRegressor(**best_params)  \n",
    "\n",
    "#model.fit(train_x,train_y,verbose=False)\n",
    "\n",
    "#lgbm_preds = model.predict(test_x)\n",
    "\n",
    "\n",
    "\n",
    "#model 3\n",
    "\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "from numpy.random import choice\n",
    "\n",
    "# Filling missing long lat in the Test set\n",
    "#55.568139, 37.481831 - fixing nans\n",
    "\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#55.544066, 37.482317 - Fixing negative numbers\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Blown up coordinates outside moscow fixed:\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "#fixing test_data districts:\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "# data[[\"area_total\",\"area_kitchen\",\"area_living\",\"bathrooms_private\", \"bathrooms_shared\",\"balconies\",\"loggias\"]][data.area_living + data.area_kitchen > data.area_total]\n",
    "\n",
    "# Add a new feature of distance from center\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "# Modify/ add the eleveator feature for both test and train\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\", \"floor\", \"new\", \"elevatern\",\"distance_from_city_center\", \n",
    "            \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\"]\n",
    "\n",
    "\n",
    "# Revisit\n",
    "# We removed windows court and windows street as they did not seem to corelate with price,\n",
    "# total_balconies == 0 instead 1 .\n",
    "# \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "    \n",
    "    elif feature == 'seller':\n",
    "        \n",
    "        list_of_candidates = [0,1,2,3]\n",
    "        # 14455 , owener 0, \n",
    "        probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "        number_of_items_to_pick = data['seller'].isna().sum()\n",
    "        number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "                      p=probability_distribution)\n",
    "        draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "                      p=probability_distribution)\n",
    "\n",
    "        data['seller'][data.seller.isna()] = draw\n",
    "        data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "        \n",
    "    elif feature == 'area_kitchen' or feature == 'area_living':\n",
    "        if feature == 'area_kitchen':\n",
    "           \n",
    "            percentage_area_data = pd.DataFrame()\n",
    "            percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "            percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "            mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "            mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "            #to omit bugs\n",
    "            data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "            data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "            data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "            data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "            data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "            data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "            #test_set\n",
    "            data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "            data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "            data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "            data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "            data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "            data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "        data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "\n",
    "        \n",
    "    elif feature == 'stories':\n",
    "        \n",
    "        idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss.size):\n",
    "            max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "            data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "        \n",
    "        idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss_test.size):\n",
    "            max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "            data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "            \n",
    "    elif feature == 'material':\n",
    "        data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "        data.material[data.material==6] = 5.0 #stalin to 5\n",
    "        \n",
    "        data_test.material[data_test.material==5] = 2.0\n",
    "        data_test.material[data_test.material==6] = 5.0\n",
    "        \n",
    "        data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "        data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "            \n",
    "            \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "\n",
    "    elif feature == 'condition': # Can also merge with 0.0 i.e. the undecorated class. but for now we created new class\n",
    "        data[\"condition\"][data.condition.isna() ] = 4.0 \n",
    "        data_test[\"condition\"][data_test.condition.isna() ] = 4.0 \n",
    "    else:\n",
    "        pass\n",
    "        #         mean = data[feature].mean()\n",
    "#         #print('Not Categorical',feature)\n",
    "#         data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "#         data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "#model\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "\n",
    "param = {\n",
    "\"objective\": \"RMSE\",\n",
    "'random_state': 2263, \n",
    "'learning_rate': 0.025133301103588284, \n",
    "'n_estimators': 3326, \n",
    "'reg_lambda': 0.01621262044795105, \n",
    "'subsample': 0.909304956841248, \n",
    "'depth': 9}\n",
    "\n",
    "\n",
    "model = CatBoostRegressor(**param)  \n",
    "\n",
    "#model.fit(train_x,train_y,early_stopping_rounds=100,verbose=False)\n",
    "#catboost_preds = model.predict(test_x)\n",
    "\n",
    "\n",
    "#  final_preds = np.average(\n",
    "#     [np.expm1(xgb_preds),\n",
    "#      np.expm1(lgbm_preds),\n",
    "#      np.expm1(xgb_preds_scnd_best_model),\n",
    "#      np.expm1(catboost_preds)\n",
    "#     ],\n",
    "#     weights = 1 / np.array([0.16080,0.16638,0.16084,0.16460]) ** 6,  #Should be 4 by standard and then increase to 6 to squeeze more juice\n",
    "#     axis=0\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.3\"></a> <br>\n",
    "## 10.3 Attempt 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:40:09.714829Z",
     "iopub.status.busy": "2021-11-16T12:40:09.714576Z",
     "iopub.status.idle": "2021-11-16T12:40:28.965357Z",
     "shell.execute_reply": "2021-11-16T12:40:28.964385Z",
     "shell.execute_reply.started": "2021-11-16T12:40:09.714802Z"
    }
   },
   "outputs": [],
   "source": [
    "#Attempt 3 - 17.10.2021 - 0.15570 on test set\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "# features = [ \"area_total\", \"rooms\", \"floor\",\"new\",\"distance_from_city_center\",\n",
    "#         \"latitude\", \"longitude\",\"district\", \"constructed\", \"material\", \"stories\"]\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\", \"elevatern\",\"distance_from_city_center\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]\n",
    "#TDONE floor, celing, rooms   \"new\" , constructed condition\n",
    "# TODO area kitchen , area living\n",
    "\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1) \n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "        \n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "                        \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "    else:\n",
    "        mean = data[feature].mean()\n",
    "        data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "        data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "data['area_total'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "#best xgboost model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "    \n",
    "model_xgb1 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5144984086781564, gamma=0, learning_rate=0.01693820796093592, max_delta_step=0,\n",
    "       max_depth=23, min_child_weight=5, n_estimators=3977,\n",
    "       n_jobs=1, nthread=None, objective='reg:squarederror', random_state=2020, # squarederror  reg:squaredlogerror   reg:squarederror\n",
    "       reg_alpha=0.021096319890667407, reg_lambda=0.2287729489989326, scale_pos_weight=1, seed=None, subsample=0.42023355655422495)\n",
    "\n",
    "\n",
    "#model_xgb1.fit(train_x,train_y)\n",
    "#xgb_preds = model_xgb1.predict(test_x)\n",
    "\n",
    "\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "# SIGMA\n",
    "# Filling missing long lat in the Test set\n",
    "#55.568139, 37.481831 - fixing nans\n",
    "\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#55.544066, 37.482317 - Fixing negative numbers\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Blown up coordinates outside moscow fixed:\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "#fixing test_data districts:\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "# data[[\"area_total\",\"area_kitchen\",\"area_living\",\"bathrooms_private\", \"bathrooms_shared\",\"balconies\",\"loggias\"]][data.area_living + data.area_kitchen > data.area_total]\n",
    "\n",
    "# Add a new feature of distance from center\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "# Modify/ add the eleveator feature for both test and train\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "# features = [ \"area_total\", \"rooms\", \"floor\",\"new\",\"distance_from_city_center\",\n",
    "#         \"latitude\", \"longitude\",\"district\", \"constructed\", \"material\", \"stories\"]\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\", \"elevatern\",\"distance_from_city_center\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]\n",
    "#TDONE floor, celing, rooms   \"new\" , constructed condition\n",
    "# TODO area kitchen , area living\n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "\n",
    "        \n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "\n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "\n",
    "        \n",
    "    elif feature == 'stories':\n",
    "        \n",
    "        idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss.size):\n",
    "            max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "            data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "        \n",
    "        idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss_test.size):\n",
    "            max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "            data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "            \n",
    "    elif feature == 'material':\n",
    "        data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "        data.material[data.material==6] = 5.0 #stalin to 5\n",
    "        \n",
    "        data_test.material[data_test.material==5] = 2.0\n",
    "        data_test.material[data_test.material==6] = 5.0\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "            \n",
    "            \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "\n",
    "    else:\n",
    "        mean = data[feature].mean()\n",
    "        #print('Not Categorical',feature)\n",
    "        data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "        data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "\n",
    "data['area_total'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "model_xgb2 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.6185033815828214, gamma=0, learning_rate=0.015430795026041527, max_delta_step=0,\n",
    "       max_depth=12, min_child_weight=2, n_estimators=3210,\n",
    "       n_jobs=1, nthread=None, objective='reg:squarederror', random_state=2020, # squarederror  reg:squaredlogerror   reg:squarederror\n",
    "       reg_alpha=0.13226917041287767, reg_lambda=0.9825323223526006, scale_pos_weight=1, seed=None, subsample=0.7155648654032974)\n",
    "\n",
    " \n",
    "#model_xgb2.fit(train_x,train_y)\n",
    "#xgb_preds_scnd_best_model = model_xgb2.predict(test_x)\n",
    "\n",
    "\n",
    "#lightgbm pipeline\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from numpy.random import choice\n",
    "\n",
    "# Filling missing long lat in the Test set\n",
    "#55.568139, 37.481831 - fixing nans\n",
    "\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#55.544066, 37.482317 - Fixing negative numbers\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Blown up coordinates outside moscow fixed:\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "#fixing test_data districts:\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "# data[[\"area_total\",\"area_kitchen\",\"area_living\",\"bathrooms_private\", \"bathrooms_shared\",\"balconies\",\"loggias\"]][data.area_living + data.area_kitchen > data.area_total]\n",
    "\n",
    "# Add a new feature of distance from center\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "# Modify/ add the eleveator feature for both test and train\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"elevatern\",\"distance_from_city_center\", \"bathrooms_total\", \"bathrooms_shared\", \n",
    "            \"bathrooms_private\", 'parking',\"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"material\", \"stories\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Revisit\n",
    "# We removed windows court and windows street as they did not seem to corelate with price,\n",
    "# total_balconies == 0 instead 1 .\n",
    "# \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "    \n",
    "    elif feature == 'seller':\n",
    "        \n",
    "        list_of_candidates = [0,1,2,3]\n",
    "        # 14455 , owener 0, \n",
    "        probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "        number_of_items_to_pick = data['seller'].isna().sum()\n",
    "        number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "                      p=probability_distribution)\n",
    "        draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "                      p=probability_distribution)\n",
    "\n",
    "        data['seller'][data.seller.isna()] = draw\n",
    "        data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "        \n",
    "    elif feature == 'area_kitchen' or feature == 'area_living':\n",
    "        if feature == 'area_kitchen':\n",
    "           \n",
    "            percentage_area_data = pd.DataFrame()\n",
    "            percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "            percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "            mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "            mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "            #to omit bugs\n",
    "            data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "            data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "            data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "            data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "            data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "            data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "            #test_set\n",
    "            data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "            data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "            data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "            data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "            data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "            data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "        data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "\n",
    "        \n",
    "    elif feature == 'stories':\n",
    "        \n",
    "        idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss.size):\n",
    "            max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "            data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "        \n",
    "        idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss_test.size):\n",
    "            max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "            data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "            \n",
    "    elif feature == 'material':\n",
    "        data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "        data.material[data.material==6] = 5.0 #stalin to 5\n",
    "        \n",
    "        data_test.material[data_test.material==5] = 2.0\n",
    "        data_test.material[data_test.material==6] = 5.0\n",
    "        \n",
    "        data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "        data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "            \n",
    "            \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "\n",
    "    elif feature == 'condition': # Can also merge with 0.0 i.e. the undecorated class. but for now we created new class\n",
    "        data[\"condition\"][data.condition.isna() ] = 4.0 \n",
    "        data_test[\"condition\"][data_test.condition.isna() ] = 4.0 \n",
    "    else:\n",
    "        pass\n",
    "        #         mean = data[feature].mean()\n",
    "#         #print('Not Categorical',feature)\n",
    "#         data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "#         data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Submission\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "best_params = {'objective' : 'regression',\n",
    "                \"metric\": \"root_mean_squared_error\",\n",
    "                'random_state': 2020,\n",
    "                \"n_estimators\": 3000,\n",
    "                'boosting_type': 'gbdt',\n",
    "               'learning_rate': 0.009545503382688678, \n",
    "               'num_iterations': 6000, \n",
    "               'n_estimators': 3467, \n",
    "               'max_bin': 1329, \n",
    "               'num_leaves': 84, \n",
    "               'min_data_in_leaf': 21, \n",
    "               'min_sum_hessian_in_leaf': 10, \n",
    "               'bagging_fraction': 0.6138769224842795, \n",
    "               'bagging_freq': 1, \n",
    "               'max_depth': 24, \n",
    "               'lambda_l1': 0.03828165214380662, \n",
    "               'lambda_l2': 0.9439756453034776, \n",
    "               'min_gain_to_split': 0.006068058433178841}\n",
    "\n",
    "\n",
    "\n",
    "model_lgbm = LGBMRegressor(**best_params)  \n",
    "\n",
    "#model_lgbm.fit(train_x,train_y,verbose=False)\n",
    "#lgbm_preds = model_lgbm.predict(test_x)\n",
    "\n",
    "\n",
    "\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "from numpy.random import choice\n",
    "\n",
    "# Filling missing long lat in the Test set\n",
    "#55.568139, 37.481831 - fixing nans\n",
    "\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#55.544066, 37.482317 - Fixing negative numbers\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Blown up coordinates outside moscow fixed:\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "#fixing test_data districts:\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "# data[[\"area_total\",\"area_kitchen\",\"area_living\",\"bathrooms_private\", \"bathrooms_shared\",\"balconies\",\"loggias\"]][data.area_living + data.area_kitchen > data.area_total]\n",
    "\n",
    "# Add a new feature of distance from center\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "# Modify/ add the eleveator feature for both test and train\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\", \"floor\", \"new\", \"elevatern\",\"distance_from_city_center\", \n",
    "            \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\"]\n",
    "\n",
    "\n",
    "# Revisit\n",
    "# We removed windows court and windows street as they did not seem to corelate with price,\n",
    "# total_balconies == 0 instead 1 .\n",
    "# \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "    \n",
    "    elif feature == 'seller':\n",
    "        \n",
    "        list_of_candidates = [0,1,2,3]\n",
    "        # 14455 , owener 0, \n",
    "        probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "        number_of_items_to_pick = data['seller'].isna().sum()\n",
    "        number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "                      p=probability_distribution)\n",
    "        draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "                      p=probability_distribution)\n",
    "\n",
    "        data['seller'][data.seller.isna()] = draw\n",
    "        data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "        \n",
    "    elif feature == 'area_kitchen' or feature == 'area_living':\n",
    "        if feature == 'area_kitchen':\n",
    "           \n",
    "            percentage_area_data = pd.DataFrame()\n",
    "            percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "            percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "            mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "            mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "            #to omit bugs\n",
    "            data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "            data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "            data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "            data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "            data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "            data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "            #test_set\n",
    "            data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "            data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "            data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "            data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "            data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "            data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "        data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "\n",
    "        \n",
    "    elif feature == 'stories':\n",
    "        \n",
    "        idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss.size):\n",
    "            max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "            data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "        \n",
    "        idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss_test.size):\n",
    "            max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "            data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "            \n",
    "    elif feature == 'material':\n",
    "        data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "        data.material[data.material==6] = 5.0 #stalin to 5\n",
    "        \n",
    "        data_test.material[data_test.material==5] = 2.0\n",
    "        data_test.material[data_test.material==6] = 5.0\n",
    "        \n",
    "        data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "        data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "            \n",
    "            \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "\n",
    "    elif feature == 'condition': # Can also merge with 0.0 i.e. the undecorated class. but for now we created new class\n",
    "        data[\"condition\"][data.condition.isna() ] = 4.0 \n",
    "        data_test[\"condition\"][data_test.condition.isna() ] = 4.0 \n",
    "    else:\n",
    "        pass\n",
    "        #         mean = data[feature].mean()\n",
    "#         #print('Not Categorical',feature)\n",
    "#         data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "#         data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "#model\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "\n",
    "param = {\n",
    "\"objective\": \"RMSE\",\n",
    "'random_state': 2263, \n",
    "'learning_rate': 0.025133301103588284, \n",
    "'n_estimators': 3326, \n",
    "'reg_lambda': 0.01621262044795105, \n",
    "'subsample': 0.909304956841248, \n",
    "'depth': 9}\n",
    "\n",
    "\n",
    "model_cat = CatBoostRegressor(**param)  \n",
    "\n",
    "#model_cat.fit(train_x,train_y,early_stopping_rounds=100,verbose=False)\n",
    "#catboost_preds = model_cat.predict(test_x)\n",
    "\n",
    "\n",
    "apartments = pd.read_csv('./data/apartments_train.csv')\n",
    "buildings = pd.read_csv('./data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('./data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('./data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "def clean_NaN_values(data, features):\n",
    "    for feature in features:\n",
    "        if data[feature].max() == 1.0 or feature == \"elevator_service\" or feature == \"condition\" or feature == \"constructed\" or feature == \"material\" or feature == \"seller\" :\n",
    "            #print('Categorical',feature)\n",
    "            mod = data[feature].mode()\n",
    "            data[feature] = data[feature].fillna(mod[0])\n",
    "        else:\n",
    "            mean = data[feature].mean()\n",
    "            #print('Not Categorical',feature)\n",
    "            data[feature] = data[feature].fillna(mean)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def add_euclidean_distance_feature(data):\n",
    "    origin_coordinates = (37.6, 55.75)\n",
    "    X, Y = 0,1\n",
    "    distance_from_city_center = np.sqrt((origin_coordinates[X] - data[\"longitude\"])**2+(origin_coordinates[Y] - data[\"latitude\"])**2)\n",
    "    data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "features = [\"ceiling\",\"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"balconies\", \"material\", \"stories\"]\n",
    "\n",
    "data      = clean_NaN_values(data, features)\n",
    "data_test = clean_NaN_values(data_test, features)\n",
    "\n",
    "\n",
    "data      = add_euclidean_distance_feature(data)\n",
    "data_test = add_euclidean_distance_feature(data_test)\n",
    "\n",
    "features = [\"ceiling\",\"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"balconies\", \"material\", \"stories\", \"distance_from_city_center\"]\n",
    "\n",
    "\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "#validation: 0.12496821678247572  \n",
    " \n",
    "\n",
    "param = {\n",
    "\"objective\": \"RMSE\",\n",
    "'random_state': 2020, \n",
    "'learning_rate': 0.027775682386650822, \n",
    "'n_estimators': 9561, \n",
    "'reg_lambda': 0.02942773134248745, \n",
    "'subsample': 0.6452052083779029,\n",
    "'depth': 8,\n",
    "'bagging_temperature': 56.77037557663241}\n",
    "\n",
    "\n",
    "model_cat2 = CatBoostRegressor(**param)  \n",
    "\n",
    "#model_cat2.fit(train_x,train_y,early_stopping_rounds=100,verbose=False)\n",
    "#catboost2_preds = model_cat2.predict(test_x)\n",
    "\n",
    "\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "# features = [ \"area_total\", \"rooms\", \"floor\",\"new\",\"distance_from_city_center\",\n",
    "#         \"latitude\", \"longitude\",\"district\", \"constructed\", \"material\", \"stories\"]\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\", \"elevatern\",\"distance_from_city_center\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]\n",
    "#TDONE floor, celing, rooms   \"new\" , constructed condition\n",
    "# TODO area kitchen , area living\n",
    "\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1) \n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "        \n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "                        \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "    else:\n",
    "        mean = data[feature].mean()\n",
    "        data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "        data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "data['area_total'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "stacked_model = StackingCVRegressor(regressors=(model_xgb1, model_xgb2, model_lgbm, model_cat, model_cat2),\n",
    "                                meta_regressor=model_xgb1, #our best individual mode becomes the META\n",
    "                                use_features_in_secondary=True,\n",
    "                                   verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "#stacked_model.fit(train_x,train_y)\n",
    "#stacked_preds = stacked_model.predict(test_x)\n",
    "\n",
    "# final_preds = np.average(\n",
    "#     [np.expm1(xgb_preds),\n",
    "#      np.expm1(lgbm_preds),\n",
    "#      np.expm1(xgb_preds_scnd_best_model),\n",
    "#      np.expm1(catboost_preds),\n",
    "#      np.expm1(stacked_preds),\n",
    "#      np.expm1(catboost2_preds)\n",
    "#     ],\n",
    "#     weights = 1 / np.array([0.12887,  0.13364,  0.12631,  0.13242,  0.12871,0.12496]) ** 6,  #Should be 4 by standard and then increase to 6 to squeeze more juice\n",
    "#     axis=0\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.4\"></a> <br>\n",
    "## 10.4 Attempt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:40:28.968155Z",
     "iopub.status.busy": "2021-11-16T12:40:28.967800Z",
     "iopub.status.idle": "2021-11-16T12:40:33.614434Z",
     "shell.execute_reply": "2021-11-16T12:40:33.613419Z",
     "shell.execute_reply.started": "2021-11-16T12:40:28.968113Z"
    }
   },
   "outputs": [],
   "source": [
    "#Attempt 4 - 27.10.2021 - 0.15178 on test set\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "import pyproj\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "#FEATURE CLEANING\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "#cleaning/engineering all elevators as one feature\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                )    \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "mod = data['elevatern'].mode()\n",
    "data['elevatern'] = data['elevatern'].fillna(mod[0])\n",
    "data_test['elevatern'] = data_test['elevatern'].fillna(mod[0])\n",
    "\n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "#engineering and cleaning a feature\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "#seller\n",
    "list_of_candidates = [0,1,2,3]\n",
    "probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "number_of_items_to_pick = data['seller'].isna().sum()\n",
    "number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "              p=probability_distribution)\n",
    "draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "              p=probability_distribution)\n",
    "\n",
    "data['seller'][data.seller.isna()] = draw\n",
    "data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "\n",
    "\n",
    "#area_kitchen and living\n",
    "percentage_area_data = pd.DataFrame()\n",
    "percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "#to omit bugs\n",
    "data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "#test_set\n",
    "data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "\n",
    "#ceiling    \n",
    "maxc = 9\n",
    "minc = 1\n",
    "data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)  \n",
    "\n",
    "data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "\n",
    "\n",
    "#condition\n",
    "var =  4.0\n",
    "data['condition'] = data['condition'].fillna(var)\n",
    "data_test['condition'] = data_test['condition'].fillna(var)\n",
    "\n",
    "\n",
    "#stories\n",
    "idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss.size):\n",
    "    max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "    data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "\n",
    "idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss_test.size):\n",
    "    max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "    data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "\n",
    "\n",
    "#material\n",
    "data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "data.material[data.material==6] = 5.0 #stalin to 5\n",
    "\n",
    "data_test.material[data_test.material==5] = 2.0\n",
    "data_test.material[data_test.material==6] = 5.0\n",
    "\n",
    "data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "\n",
    "#constructed, new:\n",
    "data['constructed'] = data.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data['new'] = data.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")      \n",
    "\n",
    "\n",
    "data_test['constructed'] = data_test.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data_test['new'] = data_test.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FEATURE ENGINEERING:\n",
    "\n",
    "lon1 =  37.621390\n",
    "lat1 = 55.753098\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data[\"longitude\"][i], data[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data['fwd_azi'] = fwd_azimuth_arr\n",
    "data['distance'] = distance_arr\n",
    "data['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data_test[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data_test[\"longitude\"][i], data_test[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data_test['fwd_azi'] = fwd_azimuth_arr\n",
    "data_test['distance'] = distance_arr\n",
    "data_test['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['area_per_room'] = data['area_total']/data['rooms']\n",
    "data_test['area_per_room'] = data_test['area_total']/data_test['rooms']\n",
    "\n",
    "data['area_per_room_log'] = np.log1p(data['area_per_room'])\n",
    "data_test['area_per_room_log'] = np.log1p(data_test['area_per_room'])\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#floor/stories\n",
    "data[\"floor/stories\"] = data[\"floor\"]/data[\"stories\"]\n",
    "data_test[\"floor/stories\"] = data_test[\"floor\"]/data_test[\"stories\"]\n",
    "\n",
    "\n",
    "#euclidean financial distance from city center\n",
    "financial_coords = (37.535497858, 55.741330368)\n",
    "distance_from_city_center = np.sqrt((financial_coords[0] - data[\"longitude\"])**2+(financial_coords[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_financial_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((financial_coords[0] - data_test[\"longitude\"])**2+(financial_coords[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_financial_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "#euclidean distance from city center\n",
    "origin_coordinates = (37.621390,55.753098)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "#FEATURES INCLUDED:\n",
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\",\n",
    "            \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'distance','back_azi','fwd_azi',\"floor/stories\",\n",
    "           \"distance_from_financial_center\", \"distance_from_city_center\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MODEL:\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price']/data[\"area_total\"])\n",
    "test_x = data_test[features]\n",
    "\n",
    "param = {\n",
    "        'base_score' : 0.5,\n",
    "        'booster' : 'gbtree',\n",
    "        'colsample_bylevel' : 1,\n",
    "        'gamma' : 0,\n",
    "        'max_delta_step' : 0,\n",
    "        'n_jobs' : -1,\n",
    "        'nthread' : None,\n",
    "        'objective' : 'reg:squarederror',\n",
    "        'scale_pos_weight' : 1,\n",
    "        'seed' : None,\n",
    "        'lambda': 0.0024064014952485785, \n",
    "         'alpha': 0.001541503784279617, \n",
    "        'colsample_bytree': 0.43152225018148443, \n",
    "       'subsample': 0.8078473020517652, \n",
    "       'learning_rate': 0.013367834721822036, \n",
    "       'n_estimators': 5235, \n",
    "     'random_state': 291, \n",
    "      'max_depth': 9, \n",
    "    'min_child_weight': 13\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model_xgb3 = XGBRegressor(**param)\n",
    "\n",
    "#model_xgb3.fit(train_x,train_y)\n",
    "#xgb3_preds = model_xgb3.predict(test_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.5\"></a> <br>\n",
    "## 10.5 Attempt 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:40:33.616772Z",
     "iopub.status.busy": "2021-11-16T12:40:33.616515Z",
     "iopub.status.idle": "2021-11-16T12:40:46.573280Z",
     "shell.execute_reply": "2021-11-16T12:40:46.572404Z",
     "shell.execute_reply.started": "2021-11-16T12:40:33.616743Z"
    }
   },
   "outputs": [],
   "source": [
    "#Attempt 5 - 08.11.2021 - 0.14871 on test set\n",
    "#Xgboost 1 PIPELINE  \n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "#Feature Cleaning\n",
    "maxc = 9\n",
    "minc = 1\n",
    "data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)     \n",
    "\n",
    "\n",
    "var =  4.0\n",
    "data['condition'] = data['condition'].fillna(var)\n",
    "data_test['condition'] = data_test['condition'].fillna(var)\n",
    "\n",
    "\n",
    "data['constructed'] = data.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data['new'] = data.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")      \n",
    "\n",
    "data_test['constructed'] = data_test.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data_test['new'] = data_test.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")     \n",
    "\n",
    "        \n",
    "for feature in [\"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\",\"latitude\", \n",
    "                \"longitude\",\"district\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]:\n",
    "    mean = data[feature].mean()\n",
    "    data[feature] = data[feature].fillna(mean)    \n",
    "    data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "#FEATURE ENGINEERING DONE:\n",
    "data['area_total'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1) \n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                )      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "mod = data['elevatern'].mode()\n",
    "data['elevatern'] = data['elevatern'].fillna(mod[0])\n",
    "data_test['elevatern'] = data_test['elevatern'].fillna(mod[0])\n",
    "\n",
    "\n",
    "#Adding city center as origin\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FEATURES INCLUDED:\n",
    "features = [\"ceiling\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\", \"elevatern\",\"distance_from_city_center\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"windows_court\", \"balconies\", \"material\", \"stories\"]\n",
    "\n",
    "\n",
    "\n",
    "#Model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    "    \n",
    "model_xgb1 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.5144984086781564, gamma=0, learning_rate=0.01693820796093592, max_delta_step=0,\n",
    "       max_depth=23, min_child_weight=5, n_estimators=3977,\n",
    "       n_jobs=1, nthread=None, objective='reg:squarederror', random_state=2020, # squarederror  reg:squaredlogerror   reg:squarederror\n",
    "       reg_alpha=0.021096319890667407, reg_lambda=0.2287729489989326, scale_pos_weight=1, seed=None, subsample=0.42023355655422495)\n",
    "\n",
    "\n",
    "#model_xgb1.fit(train_x,train_y)\n",
    "#xgb_preds = model_xgb1.predict(test_x)\n",
    "\n",
    "#Catboost PIPELINE  \n",
    "apartments = pd.read_csv('./data/apartments_train.csv')\n",
    "buildings = pd.read_csv('./data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('./data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('./data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "#FEATURE CLEANING:\n",
    "for feature in [\"ceiling\",\"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"balconies\", \"material\", \"stories\"]:\n",
    "        if data[feature].max() == 1.0 or feature == \"elevator_service\" or feature == \"condition\" or feature == \"constructed\" or feature == \"material\" or feature == \"seller\" :\n",
    "            #print('Categorical',feature)\n",
    "            mod = data[feature].mode()\n",
    "            data[feature] = data[feature].fillna(mod[0])\n",
    "            \n",
    "            mod_t = data_test[feature].mode()\n",
    "            data_test[feature] = data_test[feature].fillna(mod_t[0])          \n",
    "        else:\n",
    "            mean = data[feature].mean()\n",
    "            data[feature] = data[feature].fillna(mean)\n",
    "            \n",
    "            mean_t = data_test[feature].mean()\n",
    "            data_test[feature] = data_test[feature].fillna(mean_t)\n",
    "\n",
    "\n",
    "\n",
    "#FEATURE ENGINEERING:\n",
    "origin_coordinates = (37.6, 55.75)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "#FEATURES INCLUDED:\n",
    "features = [\"ceiling\",\"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"floor\", \"condition\",\"new\",\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"seller\", \"balconies\", \"material\", \"stories\", \"distance_from_city_center\"]\n",
    "\n",
    "\n",
    "#Model\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price'])\n",
    "test_x = data_test[features]\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "param = {\n",
    "\"objective\": \"RMSE\",\n",
    "'random_state': 2020, \n",
    "'learning_rate': 0.027775682386650822, \n",
    "'n_estimators': 9561, \n",
    "'reg_lambda': 0.02942773134248745, \n",
    "'subsample': 0.6452052083779029,\n",
    "'depth': 8,\n",
    "'bagging_temperature': 56.77037557663241}\n",
    "\n",
    "\n",
    "model_cat2 = CatBoostRegressor(**param)  \n",
    "\n",
    "#model_cat2.fit(train_x,train_y,early_stopping_rounds=100,verbose=False)\n",
    "#catboost2_preds = model_cat2.predict(test_x)\n",
    "\n",
    "\n",
    "#LGBM 2 PIPELINE\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "import pyproj\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "#FEATURE CLEANING\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "#cleaning/engineering all elevators as one feature\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                )    \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "mod = data['elevatern'].mode()\n",
    "data['elevatern'] = data['elevatern'].fillna(mod[0])\n",
    "data_test['elevatern'] = data_test['elevatern'].fillna(mod[0])\n",
    "\n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "#engineering and cleaning a feature\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "#seller\n",
    "list_of_candidates = [0,1,2,3]\n",
    "probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "number_of_items_to_pick = data['seller'].isna().sum()\n",
    "number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "              p=probability_distribution)\n",
    "draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "              p=probability_distribution)\n",
    "\n",
    "data['seller'][data.seller.isna()] = draw\n",
    "data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "\n",
    "\n",
    "#area_kitchen and living\n",
    "percentage_area_data = pd.DataFrame()\n",
    "percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "#to omit bugs\n",
    "data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "#test_set\n",
    "data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "\n",
    "#ceiling    \n",
    "maxc = 9\n",
    "minc = 1\n",
    "data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)  \n",
    "\n",
    "data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "\n",
    "\n",
    "#condition\n",
    "var =  4.0\n",
    "data['condition'] = data['condition'].fillna(var)\n",
    "data_test['condition'] = data_test['condition'].fillna(var)\n",
    "\n",
    "\n",
    "#stories\n",
    "idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss.size):\n",
    "    max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "    data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "\n",
    "idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss_test.size):\n",
    "    max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "    data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "\n",
    "\n",
    "#material\n",
    "data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "data.material[data.material==6] = 5.0 #stalin to 5\n",
    "\n",
    "data_test.material[data_test.material==5] = 2.0\n",
    "data_test.material[data_test.material==6] = 5.0\n",
    "\n",
    "data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "\n",
    "#constructed, new:\n",
    "data['constructed'] = data.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data['new'] = data.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")      \n",
    "\n",
    "\n",
    "data_test['constructed'] = data_test.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data_test['new'] = data_test.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FEATURE ENGINEERING:\n",
    "\n",
    "lon1 =  37.621390\n",
    "lat1 = 55.753098\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data[\"longitude\"][i], data[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data['fwd_azi'] = fwd_azimuth_arr\n",
    "data['distance'] = distance_arr\n",
    "data['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data_test[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data_test[\"longitude\"][i], data_test[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data_test['fwd_azi'] = fwd_azimuth_arr\n",
    "data_test['distance'] = distance_arr\n",
    "data_test['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['area_per_room'] = data['area_total']/data['rooms']\n",
    "data_test['area_per_room'] = data_test['area_total']/data_test['rooms']\n",
    "\n",
    "data['area_per_room_log'] = np.log1p(data['area_per_room'])\n",
    "data_test['area_per_room_log'] = np.log1p(data_test['area_per_room'])\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "\n",
    "#FEATURES INCLUDED:\n",
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\", \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'distance','back_azi','fwd_azi']\n",
    "\n",
    "    \n",
    "#Model\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price']/data[\"area_total\"])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "\n",
    "param = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'num_leaves': 35,\n",
    "            'min_child_weight': 1,\n",
    "            'subsample': 0.32415121173658534,\n",
    "            'colsample_bytree':  0.4768205472451884,\n",
    "            'reg_lambda': 0.14916991373512928,\n",
    "            'reg_alpha': 0.006696476138868112,\n",
    "            'learning_rate': 0.01747572661694792,\n",
    "            'max_depth': 46,\n",
    "            'n_estimators': 9775,\n",
    "            'n_jobs' : 1,\n",
    "            'objective' : 'regression',\n",
    "\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "model_lgbm2 = LGBMRegressor(**param)\n",
    "\n",
    "\n",
    "#model_lgbm2.fit(train_x,train_y)\n",
    "#lgbm2_preds = model_lgbm2.predict(test_x)\n",
    "\n",
    "\n",
    "#Xgb2 Pipeline\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price']/data[\"area_total\"])\n",
    "test_x = data_test[features]\n",
    "\n",
    "model_xgb2 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "colsample_bytree=0.5728285533310635, gamma=0, learning_rate=0.016003653491882115, max_delta_step=0,\n",
    "max_depth=8, min_child_weight=1, n_estimators=3515,\n",
    "n_jobs=1, nthread=None, objective='reg:squarederror', random_state=5, # squarederror reg:squaredlogerror reg:squarederror\n",
    "reg_alpha=0.005800171239325761, reg_lambda=0.48110648627756064, scale_pos_weight=1, seed=None, subsample=0.7155884414918227)\n",
    "\n",
    "\n",
    "#model_xgb2.fit(train_x,train_y)\n",
    "#xgb2_preds = model_xgb2.predict(test_x)\n",
    "\n",
    "\n",
    "#XGB3 (0.15178) on its own\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "\n",
    "import pyproj\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "#FEATURE CLEANING\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "#cleaning/engineering all elevators as one feature\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                )    \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "mod = data['elevatern'].mode()\n",
    "data['elevatern'] = data['elevatern'].fillna(mod[0])\n",
    "data_test['elevatern'] = data_test['elevatern'].fillna(mod[0])\n",
    "\n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "#engineering and cleaning a feature\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "#seller\n",
    "list_of_candidates = [0,1,2,3]\n",
    "probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "number_of_items_to_pick = data['seller'].isna().sum()\n",
    "number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "              p=probability_distribution)\n",
    "draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "              p=probability_distribution)\n",
    "\n",
    "data['seller'][data.seller.isna()] = draw\n",
    "data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "\n",
    "\n",
    "#area_kitchen and living\n",
    "percentage_area_data = pd.DataFrame()\n",
    "percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "#to omit bugs\n",
    "data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "#test_set\n",
    "data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "\n",
    "#ceiling    \n",
    "maxc = 9\n",
    "minc = 1\n",
    "data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)  \n",
    "\n",
    "data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "\n",
    "\n",
    "#condition\n",
    "var =  4.0\n",
    "data['condition'] = data['condition'].fillna(var)\n",
    "data_test['condition'] = data_test['condition'].fillna(var)\n",
    "\n",
    "\n",
    "#stories\n",
    "idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss.size):\n",
    "    max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "    data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "\n",
    "idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss_test.size):\n",
    "    max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "    data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "\n",
    "\n",
    "#material\n",
    "data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "data.material[data.material==6] = 5.0 #stalin to 5\n",
    "\n",
    "data_test.material[data_test.material==5] = 2.0\n",
    "data_test.material[data_test.material==6] = 5.0\n",
    "\n",
    "data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "\n",
    "#constructed, new:\n",
    "data['constructed'] = data.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data['new'] = data.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")      \n",
    "\n",
    "\n",
    "data_test['constructed'] = data_test.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data_test['new'] = data_test.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FEATURE ENGINEERING:\n",
    "\n",
    "lon1 =  37.621390\n",
    "lat1 = 55.753098\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data[\"longitude\"][i], data[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data['fwd_azi'] = fwd_azimuth_arr\n",
    "data['distance'] = distance_arr\n",
    "data['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data_test[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data_test[\"longitude\"][i], data_test[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data_test['fwd_azi'] = fwd_azimuth_arr\n",
    "data_test['distance'] = distance_arr\n",
    "data_test['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['area_per_room'] = data['area_total']/data['rooms']\n",
    "data_test['area_per_room'] = data_test['area_total']/data_test['rooms']\n",
    "\n",
    "data['area_per_room_log'] = np.log1p(data['area_per_room'])\n",
    "data_test['area_per_room_log'] = np.log1p(data_test['area_per_room'])\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#floor/stories\n",
    "data[\"floor/stories\"] = data[\"floor\"]/data[\"stories\"]\n",
    "data_test[\"floor/stories\"] = data_test[\"floor\"]/data_test[\"stories\"]\n",
    "\n",
    "\n",
    "#euclidean financial distance from city center\n",
    "financial_coords = (37.535497858, 55.741330368)\n",
    "distance_from_city_center = np.sqrt((financial_coords[0] - data[\"longitude\"])**2+(financial_coords[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_financial_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((financial_coords[0] - data_test[\"longitude\"])**2+(financial_coords[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_financial_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "#euclidean distance from city center\n",
    "origin_coordinates = (37.621390,55.753098)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "#FEATURES INCLUDED:\n",
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\",\n",
    "            \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'distance','back_azi','fwd_azi',\"floor/stories\",\n",
    "           \"distance_from_financial_center\", \"distance_from_city_center\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MODEL:\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price']/data[\"area_total\"])\n",
    "test_x = data_test[features]\n",
    "\n",
    "param = {\n",
    "        'base_score' : 0.5,\n",
    "        'booster' : 'gbtree',\n",
    "        'colsample_bylevel' : 1,\n",
    "        'gamma' : 0,\n",
    "        'max_delta_step' : 0,\n",
    "        'n_jobs' : -1,\n",
    "        'nthread' : None,\n",
    "        'objective' : 'reg:squarederror',\n",
    "        'scale_pos_weight' : 1,\n",
    "        'seed' : None,\n",
    "        'lambda': 0.0024064014952485785, \n",
    "         'alpha': 0.001541503784279617, \n",
    "        'colsample_bytree': 0.43152225018148443, \n",
    "       'subsample': 0.8078473020517652, \n",
    "       'learning_rate': 0.013367834721822036, \n",
    "       'n_estimators': 5235, \n",
    "     'random_state': 291, \n",
    "      'max_depth': 9, \n",
    "    'min_child_weight': 13\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model_xgb3 = XGBRegressor(**param)\n",
    "\n",
    "#model_xgb3.fit(train_x,train_y)\n",
    "#xgb3_preds = model_xgb3.predict(test_x)\n",
    "\n",
    "\n",
    "#Stacked Model Pipeline\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price']/data[\"area_total\"])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "stacked_model = StackingCVRegressor(regressors=(model_xgb1, model_xgb2, model_cat2, model_lgbm2,model_xgb3),\n",
    "                                meta_regressor=model_xgb3, #our best individual model becomes the META\n",
    "                                use_features_in_secondary=True,\n",
    "                                   verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "#stacked_model.fit(train_x,train_y)\n",
    "#stacked_preds = stacked_model.predict(test_x)\n",
    "\n",
    "\n",
    "#Weighted Averaging/Blending Model Pipeline\n",
    "# final_preds = np.average(\n",
    "#     [np.expm1(xgb_preds),\n",
    "#      np.expm1(xgb2_preds)*data_test[\"area_total\"],\n",
    "#      np.expm1(stacked_preds)*data_test[\"area_total\"],\n",
    "#      np.expm1(catboost2_preds),\n",
    "#      np.expm1(lgbm2_preds)*data_test[\"area_total\"],\n",
    "#      np.expm1(xgb3_preds)*data_test[\"area_total\"]\n",
    "#     ],\n",
    "#     weights = 1 / np.array([0.12887,  0.12631, 0.12,0.12492,0.12866,0.1225]) ** 6,  #Should be 4 by standard and then increase to 6 to squeeze more juice\n",
    "#     axis=0\n",
    "# )\n",
    "\n",
    "#Submission\n",
    "\n",
    "# Construct submission dataframe\n",
    "# submission = pd.DataFrame()\n",
    "# submission['id'] = data_test.id\n",
    "# submission['price_prediction'] = final_preds # *0.99839    \n",
    "# print(f'Generated {len(submission)} predictions')\n",
    "\n",
    "# # Export submission to csv with headers\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# # Look at submitted csv\n",
    "# print('\\nLine count of submission')\n",
    "# !wc -l submission.csv\n",
    "\n",
    "# print('\\nFirst 5 rows of submission')\n",
    "# !head -n 10 submission.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.6\"></a> <br>\n",
    "## 10.6 Group KFolding Based on Building Split\n",
    "This method is used to calculate the weights in the averaging method. Note models from section 10.5 are used here for demonstration. The reason why cross validation is important is to reveal possible overfitting that might be happening on the test set. If the models perform both well on the test set and the cross validation then we can be safe that there is a smaller risk for us to overfit on the public part of the test set. So this is a very important step, and also it is important to to this split based on the building_id split as well as extract out the cross validation predictions to make sure to account for the fact that the different models have been trained on different targets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random cross validation - does not work very well but worth demonstrating\n",
    "from sklearn.model_selection import KFold, cross_val_score, GroupShuffleSplit, cross_val_predict\n",
    "def cv_rmsle(model):\n",
    "    rmset = 0\n",
    "    splits = 20\n",
    "    #cross validates on 20 different random splits\n",
    "    for i in range(splits):\n",
    "        gs = GroupShuffleSplit(n_splits=2, test_size=.33)\n",
    "        train_index, valid_index = next(gs.split(data, groups=data.building_id))\n",
    "\n",
    "        data_train = data.loc[train_index]\n",
    "        data_valid = data.loc[valid_index]\n",
    "\n",
    "        train_x = data_train[features]\n",
    "        train_y = np.log1p(data_train['price']/data_train[\"area_total\"])\n",
    "        test_x = data_valid[features]\n",
    "        test_y = data_valid['price']\n",
    "\n",
    "\n",
    "        model.fit(train_x,train_y)\n",
    "\n",
    "        preds = model.predict(test_x)\n",
    "\n",
    "        rmse = root_mean_squared_log_error(y_true=test_y, y_pred=np.expm1(preds)*test_x.area_total)\n",
    "\n",
    "        rmset = rmset + rmse\n",
    "    \n",
    "    return rmset / splits\n",
    "\n",
    "print(\"Get dem weights bro\", 0.0)\n",
    "print(\"CROSS VALIDATED XGBOOST1:\", cv_rmsle(model_xgb1))\n",
    "print(\"CROSS VALIDATED XGBOOST2:\", cv_rmsle(model_xgb2))\n",
    "print(\"CROSS VALIDATED CATBOOST2:\", cv_rmsle(model_cat2))\n",
    "print(\"CROSS VALIDATED LGBM3:\", cv_rmsle(model_lgbm3)) \n",
    "print(\"CROSS VALIDATED XGBOOST3:\", cv_rmsle(model_xgb3)) \n",
    "print(\"CROSS VALIDATED CATBOOST3:\", cv_rmsle(model_catb3)) \n",
    "#Group KFold Cross_validation based on building split\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, cross_val_predict\n",
    "\n",
    "\n",
    "kfolds = 10\n",
    "gkf = GroupKFold( n_splits=kfolds)\n",
    "groups = data.building_id\n",
    "\n",
    "def group_cv(model, y, data=data[features], cv=gkf, groups=groups):\n",
    "    model_preds = cross_val_predict(model, X=data, y=y, cv=gkf , groups =groups)\n",
    "    return model_preds\n",
    "\n",
    "\n",
    "print(\"LOGGING GROUP KFOLDING 10 SPLITS\")\n",
    "\n",
    "#uncomment to run\n",
    "#xgb1_cv_preds = group_cv(model_xgb1, np.log1p(data.price))\n",
    "#print(\"XGBOOST1 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(xgb1_cv_preds)))\n",
    "\n",
    "\n",
    "#cat2_cv_preds = group_cv(model_cat2, np.log1p(data.price))\n",
    "#print(\"CATBOOST2 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(cat2_cv_preds)))\n",
    "\n",
    "#xgb2_cv_preds = group_cv(model_xgb2, np.log1p(data.price/data.area_total))\n",
    "#print(\"XGBOOST2 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(xgb2_cv_preds)*data.area_total))\n",
    "\n",
    "#lgbm3_cv_preds = group_cv(model_lgbm3, np.log1p(data.price/data.area_total))\n",
    "#print(\"LGBM3 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(lgbm3_cv_preds)*data.area_total))\n",
    "\n",
    "#xgb3_cv_preds = group_cv(model_xgb3, np.log1p(data.price/data.area_total))\n",
    "#print(\"XGBOOST3 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(xgb3_cv_preds)*data.area_total))\n",
    "\n",
    "\n",
    "#cat3_cv_preds = group_cv(model_catb3, np.log1p(data.price/data.area_total))\n",
    "#print(\"CATBOOST3 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(cat3_cv_preds)*data.area_total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from a possible 10 Group KFold Cross Validation:\n",
    "\n",
    "* CATBOOST2 SCORE: 0.19819805228618034\n",
    "* CATBOOST3 SCORE: 0.19596734972841498\n",
    "* XGBOOST1 SCORE : 0.1955884082590347\n",
    "* XGBOOST3 SCORE : 0.1908350746986002\n",
    "* LGBM3 SCORE : 0.18979521325245455\n",
    "* XGBOOST2 SCORE : 0.18874589461384148\n",
    "\n",
    "Sorting the scores from lowest to best performing, it looks pretty consistent that the latest models are performing better than their earlier counter parts atleast if one looks at each model. XGBOOST3 and XGBOOST 2 are pretty close even though XGBOOST3 performs better in the test set (maybe an indication of overfitting, but it is so small that it is hard to conclude that), how ever CATBOOST3 does better on cross validation than CATBOOST2 which is an indication that CATBOOST2 is overfitting on the public part of the test set as it is performing better than CATBOOST3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.7\"></a> <br>\n",
    "## 10.7 Final Submission Pipeline 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"10.8\"></a> <br>\n",
    "## 10.8 Final Submission Pipeline 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11\"></a> <br>\n",
    "# 11. Model Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11.1\"></a> <br>\n",
    "## 11.1 LIME Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lime explaination help us in performing a check on the reasoning behind the various model decisions. Based on the previous feature engineering we expect that distance from city center and area total are the most important features, therefore, we will expect that the most influential feature for model decision should be one of these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:40:46.575806Z",
     "iopub.status.busy": "2021-11-16T12:40:46.575487Z",
     "iopub.status.idle": "2021-11-16T12:43:03.454487Z",
     "shell.execute_reply": "2021-11-16T12:43:03.453409Z",
     "shell.execute_reply.started": "2021-11-16T12:40:46.575766Z"
    }
   },
   "outputs": [],
   "source": [
    "#CURRENTLY BEST MODEL - 22.10.2021 - 0.15831\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "\n",
    "# Formation of Objective for Optuna\n",
    "import pyproj\n",
    "from numpy.random import choice\n",
    "\n",
    "# Filling missing long lat in the Test set\n",
    "#55.568139, 37.481831 - fixing nans\n",
    "\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "\n",
    "\n",
    "#55.544066, 37.482317 - Fixing negative numbers\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "\n",
    "\n",
    "#Blown up coordinates outside moscow fixed:\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "\n",
    "\n",
    "\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "#fixing test_data districts:\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "\n",
    "\n",
    "\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "# Fixing training data districts\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "# data[[\"area_total\",\"area_kitchen\",\"area_living\",\"bathrooms_private\", \"bathrooms_shared\",\"balconies\",\"loggias\"]][data.area_living + data.area_kitchen > data.area_total]\n",
    "\n",
    "# Add a new feature of distance from center\n",
    "#Adding city center as origin\n",
    "#origin_coordinates = (37.6, 55.75)\n",
    "\n",
    "# https://stackoverflow.com/questions/24617013/convert-latitude-and-longitude-to-x-and-y-grid-system-using-python\n",
    "# data[\"dx\"]= (lon1-data[\"longitude\"])*40000*np.cos((lat1+data[\"latitude\"])*np.pi/360)/360\n",
    "# data[\"dy\"] = (lat1-data[\"latitude\"])*40000/360\n",
    "\n",
    "\n",
    "lon1 =  37.621390\n",
    "lat1 = 55.753098\n",
    "\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data[\"longitude\"][i], data[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data['fwd_azi'] = fwd_azimuth_arr\n",
    "data['distance'] = distance_arr\n",
    "data['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data_test[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data_test[\"longitude\"][i], data_test[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data_test['fwd_azi'] = fwd_azimuth_arr\n",
    "data_test['distance'] = distance_arr\n",
    "data_test['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Modify/ add the eleveator feature for both test and train\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) # expesnive      0 1 4 6 expensive     2 3 5   cheap     ,   E    1, 0 , 2 ,6  ,,,,   3,      \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "\n",
    "\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "\n",
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\", \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'distance','back_azi','fwd_azi']\n",
    "\n",
    "\n",
    "# Revisit\n",
    "# We removed windows court and windows street as they did not seem to corelate with price,\n",
    "# total_balconies == 0 instead 1 .\n",
    "# \n",
    "for feature in features:\n",
    "    if   feature == 'elevatern': #or feature == \"elevator_service\" or features == \"condition\" or feature == \"constructed\" or features == \"material\" or features == \"seller\"\n",
    "        #print('Categorical',feature)\n",
    "        mod = data[feature].mode()\n",
    "        data[feature] = data[feature].fillna(mod[0])\n",
    "        data_test[feature] = data_test[feature].fillna(mod[0])\n",
    "    \n",
    "    elif feature == 'seller':\n",
    "        \n",
    "        list_of_candidates = [0,1,2,3]\n",
    "        # 14455 , owener 0, \n",
    "        probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "        number_of_items_to_pick = data['seller'].isna().sum()\n",
    "        number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "                      p=probability_distribution)\n",
    "        draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "                      p=probability_distribution)\n",
    "\n",
    "        data['seller'][data.seller.isna()] = draw\n",
    "        data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "        \n",
    "    elif feature == 'area_kitchen' or feature == 'area_living':\n",
    "        if feature == 'area_kitchen':\n",
    "           \n",
    "            percentage_area_data = pd.DataFrame()\n",
    "            percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "            percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "            mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "            mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "            #to omit bugs\n",
    "            data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "            data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "            data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "            data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "            data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "            data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "            #test_set\n",
    "            data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "            data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "            data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "            data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "            data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "            data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "    elif feature == 'ceiling':\n",
    "        maxc = 9\n",
    "        minc = 1\n",
    "        data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "        data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)  \n",
    "        \n",
    "        data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "        \n",
    "    elif feature == 'condition' :\n",
    "        var =  4.0\n",
    "        data[feature] = data[feature].fillna(var)\n",
    "        data_test[feature] = data_test[feature].fillna(var)\n",
    "\n",
    "        \n",
    "    elif feature == 'stories':\n",
    "        \n",
    "        idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss.size):\n",
    "            max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "            data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "        \n",
    "        idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "        #data['storiesnew'] = data['stories'].copy()\n",
    "        for i in range(idss_test.size):\n",
    "            max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "            data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "            \n",
    "    elif feature == 'material':\n",
    "        data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "        data.material[data.material==6] = 5.0 #stalin to 5\n",
    "        \n",
    "        data_test.material[data_test.material==5] = 2.0\n",
    "        data_test.material[data_test.material==6] = 5.0\n",
    "        \n",
    "        data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "        data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "\n",
    "        \n",
    "    elif feature == 'constructed' or feature == 'new':\n",
    "        if feature == 'new':\n",
    "            pass\n",
    "        else:\n",
    "            data['constructed'] = data.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data['new'] = data.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )      \n",
    "            \n",
    "            \n",
    "            data_test['constructed'] = data_test.apply(\n",
    "                lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "                axis=1\n",
    "            )  \n",
    "            data_test['new'] = data_test.apply(\n",
    "                lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "                axis=1\n",
    "            )     \n",
    "\n",
    "    elif feature == 'condition': # Can also merge with 0.0 i.e. the undecorated class. but for now we created new class\n",
    "        data[\"condition\"][data.condition.isna() ] = 4.0 \n",
    "        data_test[\"condition\"][data_test.condition.isna() ] = 4.0 \n",
    "    else:\n",
    "        pass\n",
    "        #         mean = data[feature].mean()\n",
    "#         #print('Not Categorical',feature)\n",
    "#         data[feature] = data[feature].fillna(mean)\n",
    "        \n",
    "#         data_test[feature] = data_test[feature].fillna(mean)\n",
    "\n",
    "data['area_per_room'] = data['area_total']/data['rooms']\n",
    "data_test['area_per_room'] = data_test['area_total']/data_test['rooms']\n",
    "\n",
    "data['area_per_room_log'] = np.log1p(data['area_per_room'])\n",
    "data_test['area_per_room_log'] = np.log1p(data_test['area_per_room'])\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "\n",
    "# train_x = data[features]\n",
    "# train_y = np.log1p(data['price']/data[\"area_total\"])\n",
    "# test_x = data_test[features]\n",
    "\n",
    "model_xgb2 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "colsample_bytree=0.5728285533310635, gamma=0, learning_rate=0.016003653491882115, max_delta_step=0,\n",
    "max_depth=8, min_child_weight=1, n_estimators=3515,\n",
    "n_jobs=1, nthread=None, objective='reg:squarederror', random_state=5, # squarederror reg:squaredlogerror reg:squarederror\n",
    "reg_alpha=0.005800171239325761, reg_lambda=0.48110648627756064, scale_pos_weight=1, seed=None, subsample=0.7155884414918227)\n",
    "\n",
    "\n",
    "data_train, data_valid = model_selection.train_test_split(data, test_size=0.33, random_state = 0, stratify=np.log(data.price).round())\n",
    "train_X = data_train[features]\n",
    "train_y = np.log1p(data_train['price']/data_train[\"area_total\"]) #data['price']/data[\"area_total\"]\n",
    "test_X = data_valid[features]\n",
    "test_y = np.log1p(data_valid['price']/data_valid[\"area_total\"])\n",
    "        \n",
    "model_xgb2.fit(train_X,train_y)\n",
    "\n",
    "test_pred = model_xgb2.predict(test_X)\n",
    "# final_preds = final_preds *data_test[\"area_total\"]\n",
    "\n",
    "\n",
    "errors = test_pred - test_y\n",
    "errors = errors.to_numpy()\n",
    "sorted_errors = np.argsort(abs(errors))\n",
    "\n",
    "errors = test_pred - test_y\n",
    "errors = errors.to_numpy()\n",
    "sorted_errors = np.argsort(abs(errors))\n",
    "worse_5 = sorted_errors[-5:]\n",
    "best_5 = sorted_errors[:5]\n",
    "\n",
    "print(pd.DataFrame({'worse':errors[worse_5]}))\n",
    "print()\n",
    "print(pd.DataFrame({'best':errors[best_5]}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lime first creates an explainer object. Then we can visualize each instance of the dataset under consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:43:03.455974Z",
     "iopub.status.busy": "2021-11-16T12:43:03.455740Z",
     "iopub.status.idle": "2021-11-16T12:43:15.331175Z",
     "shell.execute_reply": "2021-11-16T12:43:15.330253Z",
     "shell.execute_reply.started": "2021-11-16T12:43:03.455947Z"
    }
   },
   "outputs": [],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "train_Xnp = train_X.to_numpy()\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(train_Xnp, feature_names=train_X.columns, class_names=['Price'], verbose=True, mode='regression')\n",
    "\n",
    "i = worse_5[0]\n",
    "print('Error =', errors[i])\n",
    "test_Xnp = test_X.to_numpy()\n",
    "exp = explainer.explain_instance(test_Xnp[i], model_xgb2.predict, num_features=10)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can observe that for one of the bad performing data point, the important features are stated in the table. Those features are distance, latitude and so on. This makes sense that the model is relying on these features to make a decision and makes the model more interpretable. We also notice that the prediction can be brought closer to the groundtruth by increase the weight of the distance feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:43:15.338981Z",
     "iopub.status.busy": "2021-11-16T12:43:15.332633Z",
     "iopub.status.idle": "2021-11-16T12:43:26.862039Z",
     "shell.execute_reply": "2021-11-16T12:43:26.860919Z",
     "shell.execute_reply.started": "2021-11-16T12:43:15.338924Z"
    }
   },
   "outputs": [],
   "source": [
    "i = worse_5[1]\n",
    "print('Error =', errors[i])\n",
    "exp = explainer.explain_instance(test_Xnp[i], model_xgb2.predict, num_features=10)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can observe that for one of the bad performing data point, the important features are stated in the table. Those features are distance, parking and so on. The interpretation of this point is also similar to the previous data point selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:43:26.864051Z",
     "iopub.status.busy": "2021-11-16T12:43:26.863503Z",
     "iopub.status.idle": "2021-11-16T12:43:38.616726Z",
     "shell.execute_reply": "2021-11-16T12:43:38.615727Z",
     "shell.execute_reply.started": "2021-11-16T12:43:26.864003Z"
    }
   },
   "outputs": [],
   "source": [
    "i = best_5[0]\n",
    "print('Error =', errors[i])\n",
    "exp = explainer.explain_instance(test_Xnp[i], model_xgb2.predict, num_features=10)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can observe that for one of the best performing data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:43:38.618725Z",
     "iopub.status.busy": "2021-11-16T12:43:38.618305Z",
     "iopub.status.idle": "2021-11-16T12:43:50.469846Z",
     "shell.execute_reply": "2021-11-16T12:43:50.468511Z",
     "shell.execute_reply.started": "2021-11-16T12:43:38.618689Z"
    }
   },
   "outputs": [],
   "source": [
    "i = best_5[1]\n",
    "print('Error =', errors[i])\n",
    "exp = explainer.explain_instance(test_Xnp[i], model_xgb2.predict, num_features=10)\n",
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from LIME we also try out the Feature Importance and find that the most important features are intuitively the same which we consider the most important in our feature engineering and data analyisis stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"11.2\"></a> <br>\n",
    "## 11.2 Model Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME provides an instance level idea of importance of each feature towards the prediction of a model. We also perform feature importance to check the importance of all the features for the data set as a whole. \n",
    "We find that the most important features are intuitively more or less the same which we consider the most important in our feature engineering and data analyisis stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-16T12:49:23.365103Z",
     "iopub.status.busy": "2021-11-16T12:49:23.364822Z",
     "iopub.status.idle": "2021-11-16T12:49:39.220204Z",
     "shell.execute_reply": "2021-11-16T12:49:39.219357Z",
     "shell.execute_reply.started": "2021-11-16T12:49:23.365075Z"
    }
   },
   "outputs": [],
   "source": [
    "col_sorted_by_importance=model_xgb2.feature_importances_.argsort()\n",
    "feat_imp=pd.DataFrame({\n",
    "    'cols':train_X.columns[col_sorted_by_importance],\n",
    "    'imps':model_xgb2.feature_importances_[col_sorted_by_importance]\n",
    "})\n",
    "\n",
    "import plotly_express as px\n",
    "px.bar(feat_imp, x='cols', y='imps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a> <br>\n",
    "## 12 Second Final Submission- Short Notebook\n",
    "\n",
    "In the following sections, we document and provide the code for the second final submission. This submission takes 40 minutes to run. \n",
    "The short notebook has the other final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "np.random.seed(123)\n",
    "sns.set_style('darkgrid')\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "!ln -s /kaggle/input/moscow-housing-tdt4173 ./data\n",
    "!ls ./data | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_log_error(y_true, y_pred):\n",
    "    # Alternatively: sklearn.metrics.mean_squared_log_error(y_true, y_pred) ** 0.5\n",
    "    assert (y_true >= 0).all() \n",
    "    assert (y_pred >= 0).all()\n",
    "    log_error = np.log1p(y_pred) - np.log1p(y_true)  # Note: log1p(x) = log(1 + x)\n",
    "    return np.mean(log_error ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apartments = pd.read_csv('data/apartments_train.csv')\n",
    "buildings = pd.read_csv('data/buildings_train.csv')\n",
    "data = pd.merge(apartments, buildings.set_index('id'), how='left', left_on='building_id', right_index=True)\n",
    "\n",
    "apartments_test = pd.read_csv('data/apartments_test.csv')\n",
    "buildings_test = pd.read_csv('data/buildings_test.csv')\n",
    "data_test = pd.merge(apartments_test, buildings_test.set_index('id'), how='left', left_on='building_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyproj\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "#FEATURE CLEANING\n",
    "data_test.latitude.iloc[90] = 55.568139\n",
    "data_test.longitude.iloc[90]= 37.481831\n",
    "data_test.latitude.iloc[23] = 55.568139\n",
    "data_test.longitude.iloc[23]= 37.481831\n",
    "data_test.latitude.iloc[2511] = 55.544066\n",
    "data_test.longitude.iloc[2511]= 37.482317\n",
    "data_test.latitude.iloc[6959] = 55.544066\n",
    "data_test.longitude.iloc[6959]= 37.482317\n",
    "data_test.latitude.iloc[5090] = 55.544066\n",
    "data_test.longitude.iloc[5090]= 37.482317\n",
    "data_test.latitude.iloc[8596] = 55.544066\n",
    "data_test.longitude.iloc[8596]= 37.482317\n",
    "data_test.latitude.iloc[2529] = 55.764335\n",
    "data_test.longitude.iloc[2529]= 37.907556\n",
    "data_test.latitude.iloc[4719] = 55.765430\n",
    "data_test.longitude.iloc[4719]= 37.928284\n",
    "data_test.latitude.iloc[9547] = 55.765430\n",
    "data_test.longitude.iloc[9547]= 37.928284\n",
    "\n",
    "data_test.district[data_test.building_id == 3803] = 11\n",
    "data_test.district[data_test.building_id == 4636] = 11\n",
    "data_test.district[data_test.building_id == 4412] = 11\n",
    "data_test.district[data_test.building_id == 926] = 3\n",
    "data_test.district[data_test.building_id == 4202] = 3\n",
    "data_test.district[data_test.building_id == 8811] = 3\n",
    "data_test.district[data_test.building_id == 6879] = 3\n",
    "data_test.district[data_test.building_id == 5667] = 3\n",
    "data_test.district[data_test.building_id == 2265] = 5\n",
    "data_test.district[data_test.building_id == 6403] = 5\n",
    "data_test.district[data_test.building_id == 7317] = 5\n",
    "data_test.district[data_test.building_id == 1647] = 5\n",
    "data_test.district[data_test.building_id == 183] = 5\n",
    "\n",
    "data.district[data.building_id == 2029] = 0\n",
    "data.district[data.building_id == 1255] = 0\n",
    "data.district[data.building_id == 4162] = 5\n",
    "\n",
    "\n",
    "#cleaning/engineering all elevators as one feature\n",
    "data['elevatern'] = data.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                )    \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)\n",
    "data_test['elevatern'] = data_test.apply(lambda row: 0 if (row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 0.0 ) # \n",
    "                               else( 1 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(2 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(3 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 0.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(4 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 0.0) # \n",
    "                                else(5 if(row[\"elevator_without\"] == 0 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0) # \n",
    "                                else(6 if(row[\"elevator_without\"] == 1 and row[\"elevator_passenger\"] == 1.0 and row[\"elevator_service\"] == 1.0)\n",
    "                                else(np.nan)\n",
    "                                ) \n",
    "                                )\n",
    "                                )\n",
    "                                )   \n",
    "                                )\n",
    "                                )\n",
    "                                ,axis=1)  \n",
    "mod = data['elevatern'].mode()\n",
    "data['elevatern'] = data['elevatern'].fillna(mod[0])\n",
    "data_test['elevatern'] = data_test['elevatern'].fillna(mod[0])\n",
    "\n",
    "\n",
    "data[\"bathrooms_shared\"] = data[\"bathrooms_shared\"].fillna(1)\n",
    "data[\"bathrooms_private\"] = data[\"bathrooms_private\"].fillna(1)\n",
    "data_test[\"bathrooms_shared\"] = data_test[\"bathrooms_shared\"].fillna(1)\n",
    "data_test[\"bathrooms_private\"] = data_test[\"bathrooms_private\"].fillna(1)\n",
    "\n",
    "\n",
    "\n",
    "data[\"parking\"] =  data[\"parking\"].fillna(3.0)\n",
    "data_test[\"parking\"] =  data_test[\"parking\"].fillna(3.0)\n",
    "data[\"heating\"] =  data[\"heating\"].fillna(0.0)\n",
    "data_test[\"heating\"] =  data_test[\"heating\"].fillna(0.0)\n",
    "\n",
    "#engineering and cleaning a feature\n",
    "data[\"total_balconies\"] = data[\"balconies\"] + data[\"loggias\"]\n",
    "data_test[\"total_balconies\"] = data_test[\"balconies\"] + data_test[\"loggias\"]\n",
    "data[\"total_balconies\"] =  data[\"total_balconies\"].fillna(1.0)\n",
    "data_test[\"total_balconies\"] =  data_test[\"total_balconies\"].fillna(1.0)\n",
    "\n",
    "\n",
    "#seller\n",
    "list_of_candidates = [0,1,2,3]\n",
    "probability_distribution  = [0.11, 0.33, 0.13, 0.43]\n",
    "number_of_items_to_pick = data['seller'].isna().sum()\n",
    "number_of_items_to_pick_test = data_test['seller'].isna().sum()\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "draw = choice(list_of_candidates, number_of_items_to_pick,\n",
    "              p=probability_distribution)\n",
    "draw_test = choice(list_of_candidates, number_of_items_to_pick_test,\n",
    "              p=probability_distribution)\n",
    "\n",
    "data['seller'][data.seller.isna()] = draw\n",
    "data_test['seller'][data_test.seller.isna()] = draw_test\n",
    "\n",
    "\n",
    "#area_kitchen and living\n",
    "percentage_area_data = pd.DataFrame()\n",
    "percentage_area_data[\"area_kitchen\"] = data[\"area_kitchen\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "percentage_area_data[\"area_living\"] = data[\"area_living\"][data.area_living + data.area_kitchen < data.area_total]/data[\"area_total\"][data.area_living + data.area_kitchen < data.area_total]\n",
    "\n",
    "mean_kitchen = percentage_area_data[\"area_kitchen\"].mean()\n",
    "mean_living = percentage_area_data[\"area_living\"].mean()\n",
    "\n",
    "#to omit bugs\n",
    "data[\"area_kitchen_edit\"] = data[\"area_kitchen\"].copy()\n",
    "data[\"area_living_edit\"] = data[\"area_living\"].copy()\n",
    "\n",
    "data[\"area_kitchen_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_kitchen\n",
    "data[\"area_living_edit\"][(data.area_living + data.area_kitchen >= data.area_total) | (data.area_living.isna() | data.area_kitchen.isna())] = data.area_total*mean_living\n",
    "\n",
    "data[\"area_kitchen\"] = data[\"area_kitchen_edit\"].copy()\n",
    "data[\"area_living\"] = data[\"area_living_edit\"].copy()\n",
    "\n",
    "#test_set\n",
    "data_test[\"area_kitchen_edit\"] = data_test[\"area_kitchen\"].copy()\n",
    "data_test[\"area_living_edit\"] = data_test[\"area_living\"].copy()\n",
    "\n",
    "data_test[\"area_kitchen_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_kitchen\n",
    "data_test[\"area_living_edit\"][(data_test.area_living + data_test.area_kitchen >= data_test.area_total) | (data_test.area_living.isna() | data_test.area_kitchen.isna())] = data_test.area_total*mean_living\n",
    "\n",
    "\n",
    "data_test[\"area_kitchen\"] = data_test[\"area_kitchen_edit\"].copy()\n",
    "data_test[\"area_living\"] = data_test[\"area_living_edit\"].copy()\n",
    "        \n",
    "\n",
    "#ceiling    \n",
    "maxc = 9\n",
    "minc = 1\n",
    "data['ceiling'] = data.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1) \n",
    "data_test['ceiling'] = data_test.apply(lambda row: data[\"ceiling\"].mode()[0] if (row[\"ceiling\"] < minc or row[\"ceiling\"] > maxc ) else( row[\"ceiling\"]) ,axis=1)  \n",
    "\n",
    "data['ceiling'][data.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "data_test['ceiling'][data_test.ceiling.isna() ] = data[\"ceiling\"].mode()[0]\n",
    "\n",
    "\n",
    "#condition\n",
    "var =  4.0\n",
    "data['condition'] = data['condition'].fillna(var)\n",
    "data_test['condition'] = data_test['condition'].fillna(var)\n",
    "\n",
    "\n",
    "#stories\n",
    "idss = data[[\"building_id\"]][data.floor > data.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss.size):\n",
    "    max_floor = data['floor'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]].max()\n",
    "    data['stories'][data[\"building_id\"] == idss[\"building_id\"].iloc[i]] =  max_floor\n",
    "\n",
    "idss_test = data_test[[\"building_id\"]][data_test.floor > data_test.stories].sort_values(\"building_id\").drop_duplicates()\n",
    "for i in range(idss_test.size):\n",
    "    max_floor_test = data_test['floor'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]].max()\n",
    "    data_test['stories'][data_test[\"building_id\"] == idss_test[\"building_id\"].iloc[i]] =  max_floor_test\n",
    "\n",
    "\n",
    "#material\n",
    "data.material[data.material==5] = 2.0 #merging monlith brick with monolith\n",
    "data.material[data.material==6] = 5.0 #stalin to 5\n",
    "\n",
    "data_test.material[data_test.material==5] = 2.0\n",
    "data_test.material[data_test.material==6] = 5.0\n",
    "\n",
    "data['material'][data.material.isna() ] = data['material'].mode()[0]\n",
    "data_test['material'][data_test.material.isna() ] = data['material'].mode()[0]\n",
    "\n",
    "#constructed, new:\n",
    "data['constructed'] = data.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data['new'] = data.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")      \n",
    "\n",
    "\n",
    "data_test['constructed'] = data_test.apply(\n",
    "    lambda row: 2019 if (np.isnan(row['constructed']) and ~np.isnan(row['new']) and row['new'] == 0.0) else( 2021 if (np.isnan(row['constructed'])) else row['constructed']),\n",
    "    axis=1\n",
    ")  \n",
    "data_test['new'] = data_test.apply(\n",
    "    lambda row: 0.0 if (np.isnan(row['new']) and row['constructed'] < 2020) else( 1.0 if (np.isnan(row['new'])) else row['new']),\n",
    "    axis=1\n",
    ")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FEATURE ENGINEERING:\n",
    "\n",
    "lon1 =  37.621390\n",
    "lat1 = 55.753098\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data[\"longitude\"][i], data[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data['fwd_azi'] = fwd_azimuth_arr\n",
    "data['distance'] = distance_arr\n",
    "data['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "geodesic = pyproj.Geod(ellps='WGS84')\n",
    "distance_arr = []\n",
    "back_azimuth_arr = []\n",
    "fwd_azimuth_arr = []\n",
    "for i in range(len(data_test[\"longitude\"])):\n",
    "    fwd_azimuth,back_azimuth,distance = geodesic.inv(lon1, lat1, data_test[\"longitude\"][i], data_test[\"latitude\"][i])\n",
    "    distance_arr.append(distance)\n",
    "    back_azimuth_arr.append(back_azimuth)\n",
    "    fwd_azimuth_arr.append(fwd_azimuth)\n",
    "\n",
    "data_test['fwd_azi'] = fwd_azimuth_arr\n",
    "data_test['distance'] = distance_arr\n",
    "data_test['back_azi'] = back_azimuth_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data['area_per_room'] = data['area_total']/data['rooms']\n",
    "data_test['area_per_room'] = data_test['area_total']/data_test['rooms']\n",
    "\n",
    "data['area_per_room_log'] = np.log1p(data['area_per_room'])\n",
    "data_test['area_per_room_log'] = np.log1p(data_test['area_per_room'])\n",
    "\n",
    "data['area_total_log'] = np.log1p(data['area_total'])\n",
    "data['area_kitchen_log'] = np.log1p(data['area_kitchen'])\n",
    "data['area_living_log'] = np.log1p(data['area_living'])\n",
    "\n",
    "data_test['area_total_log'] = np.log1p(data_test['area_total'])\n",
    "data_test['area_kitchen_log'] = np.log1p(data_test['area_kitchen'])\n",
    "data_test['area_living_log'] = np.log1p(data_test['area_living'])\n",
    "\n",
    "\n",
    "data[\"bathrooms_total\"] = data.bathrooms_shared + data.bathrooms_private\n",
    "data_test[\"bathrooms_total\"] = data_test.bathrooms_shared + data_test.bathrooms_private\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#floor/stories\n",
    "data[\"floor/stories\"] = data[\"floor\"]/data[\"stories\"]\n",
    "data_test[\"floor/stories\"] = data_test[\"floor\"]/data_test[\"stories\"]\n",
    "\n",
    "\n",
    "#euclidean financial distance from city center\n",
    "financial_coords = (37.535497858, 55.741330368)\n",
    "distance_from_city_center = np.sqrt((financial_coords[0] - data[\"longitude\"])**2+(financial_coords[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_financial_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((financial_coords[0] - data_test[\"longitude\"])**2+(financial_coords[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_financial_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "#euclidean distance from city center\n",
    "origin_coordinates = (37.621390,55.753098)\n",
    "distance_from_city_center = np.sqrt((origin_coordinates[0] - data[\"longitude\"])**2+(origin_coordinates[1] - data[\"latitude\"])**2)\n",
    "data[\"distance_from_city_center\"] = distance_from_city_center\n",
    "\n",
    "distance_from_city_center_t = np.sqrt((origin_coordinates[0] - data_test[\"longitude\"])**2+(origin_coordinates[1] - data_test[\"latitude\"])**2)\n",
    "data_test[\"distance_from_city_center\"] = distance_from_city_center_t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FEATURES INCLUDED:\n",
    "features = [\"ceiling\", \"area_per_room\" ,  \"area_per_room_log\", \"rooms\", \"area_total\", \"area_kitchen\", \"area_living\", \"area_total_log\", \"area_kitchen_log\", \"area_living_log\",\n",
    "            \"floor\", \"new\", \"elevatern\", \"bathrooms_total\", \"bathrooms_shared\", \"bathrooms_private\", 'parking', 'heating',\n",
    "            \"latitude\", \"longitude\",\"district\", \"constructed\", \"condition\", \"seller\", \"total_balconies\", \"material\", \"stories\",'distance','back_azi','fwd_azi',\"floor/stories\",\n",
    "           \"distance_from_financial_center\", \"distance_from_city_center\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GroupKfold cross validation based on building split\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, cross_val_predict\n",
    "\n",
    "kfolds = 10\n",
    "gkf = GroupKFold( n_splits=kfolds)\n",
    "groups = data.building_id\n",
    "\n",
    "def group_cv(model, y, data=data[features], cv=gkf, groups=groups):\n",
    "    model_preds = cross_val_predict(model, X=data, y=y, cv=gkf , groups =groups)\n",
    "    return model_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS VALIDATION IS DONE HERE:\n",
    "#LGBM\n",
    "from lightgbm import LGBMRegressor\n",
    "best_params = {'objective' : 'regression',\n",
    "    \"metric\": \"root_mean_squared_error\",\n",
    "    'random_state': 2020,\n",
    "    \"n_estimators\": 3000,\n",
    "    'boosting_type': 'gbdt', #better than dart\n",
    "    \"n_jobs\": -1,\n",
    " 'learning_rate': 0.009902216010560466, \n",
    " 'num_iterations': 9853, \n",
    " 'n_estimators': 2200, \n",
    " 'max_bin': 1145, \n",
    " 'num_leaves': 992, \n",
    " 'min_data_in_leaf': 21, \n",
    " 'min_sum_hessian_in_leaf': 6, \n",
    " 'bagging_fraction': 0.7553160099162841, \n",
    " 'bagging_freq': 1, \n",
    " 'max_depth': 5, \n",
    " 'lambda_l1': 0.001047756084491848, \n",
    " 'lambda_l2': 0.5231817241800534, \n",
    " 'min_gain_to_split': 0.01715842845568677\n",
    "    }\n",
    "model_lgbm3 = LGBMRegressor(**best_params)\n",
    "\n",
    "\n",
    "\n",
    "#Catboost\n",
    "from catboost import CatBoostRegressor\n",
    "param = {\n",
    "\"objective\": \"RMSE\",\n",
    "'depth': 8, \n",
    " 'reg_lambda': 0.6424630162452156, \n",
    " 'learning_rate': 0.008856338969505724, \n",
    " 'n_estimators': 5356, \n",
    " 'max_bin': 1042, \n",
    " 'random_state': 1695, \n",
    " 'subsample': 0.4474582804576312,\n",
    "    \"verbose\": False}\n",
    "model_catb3 = CatBoostRegressor(**param)  \n",
    "\n",
    "\n",
    "\n",
    "#Xgboost\n",
    "from xgboost import XGBRegressor\n",
    "param = {\n",
    "        'base_score' : 0.5,\n",
    "        'booster' : 'gbtree',\n",
    "        'colsample_bylevel' : 1,\n",
    "        'gamma' : 0,\n",
    "        'max_delta_step' : 0,\n",
    "        'n_jobs' : -1,\n",
    "        'nthread' : None,\n",
    "        'objective' : 'reg:squarederror',\n",
    "        'scale_pos_weight' : 1,\n",
    "        'seed' : None,\n",
    "        'lambda': 0.0024064014952485785, \n",
    "         'alpha': 0.001541503784279617, \n",
    "        'colsample_bytree': 0.43152225018148443, \n",
    "       'subsample': 0.8078473020517652, \n",
    "       'learning_rate': 0.013367834721822036, \n",
    "       'n_estimators': 5235, \n",
    "     'random_state': 291, \n",
    "      'max_depth': 9, \n",
    "    'min_child_weight': 13\n",
    "}\n",
    "model_xgb3 = XGBRegressor(**param)\n",
    "\n",
    "\n",
    "\n",
    "#From GroupKFolding on 10 ksplits on building_id\n",
    "#LGBM3 SCORE: 0.18979521325245455\n",
    "#XGBOOST3 SCORE: 0.1908350746986002\n",
    "#CATBOOST3 SCORE: 0.19596734972841498\n",
    "\n",
    "#comment these out when submitting:\n",
    "#print(\"LOGGING GROUP KFOLDING 10 SPLITS\")\n",
    "\n",
    "#lgbm3_cv_preds = group_cv(model_lgbm3, np.log1p(data.price/data.area_total))\n",
    "#print(\"LGBM3 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(lgbm3_cv_preds)*data.area_total))\n",
    "\n",
    "#xgb3_cv_preds = group_cv(model_xgb3, np.log1p(data.price/data.area_total))\n",
    "#print(\"XGBOOST3 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(xgb3_cv_preds)*data.area_total))\n",
    "\n",
    "#cat3_cv_preds = group_cv(model_catb3, np.log1p(data.price/data.area_total))\n",
    "#print(\"CATBOOST3 SCORE:\", root_mean_squared_log_error(data.price, np.expm1(cat3_cv_preds)*data.area_total))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STACKED MODEL\n",
    "train_x = data[features]\n",
    "train_y = np.log1p(data['price']/data[\"area_total\"])\n",
    "test_x = data_test[features]\n",
    "\n",
    "\n",
    "\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "stacked_model = StackingCVRegressor(regressors=(model_lgbm3, model_catb3, model_xgb3),\n",
    "                                meta_regressor=model_xgb3, #our best individual model becomes the META\n",
    "                                use_features_in_secondary=True,\n",
    "                                   verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "stacked_model.fit(train_x,train_y)\n",
    "stacked_preds = stacked_model.predict(test_x)\n",
    "\n",
    "final_preds = np.expm1(stacked_preds)*data_test[\"area_total\"]\n",
    "\n",
    "\n",
    "#submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test.id\n",
    "submission['price_prediction'] = final_preds     \n",
    "print(f'Generated {len(submission)} predictions')\n",
    "\n",
    "# Export submission to csv with headers\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# Look at submitted csv\n",
    "print('\\nLine count of submission')\n",
    "!wc -l submission.csv\n",
    "\n",
    "print('\\nFirst 5 rows of submission')\n",
    "!head -n 10 submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"13\"></a> <br>\n",
    "# 13. Conclusion\n",
    "\n",
    "In conclusion domain knowledge is a very important starting point for any machine learning project. Cleaning features and generating new ones should be motivated by the domain knowledge as this from experience has lead to better performance for similar models. This goes to show that a datadriven approach has greater value than a modeldriven approach. This is because investigating the dataset, finding out if it is consistent and understanding different patterns in the data will both help give a better understanding of how the data was generated but also how it needs to be altered such that a model understands it. In terms of modeling, it has shown that the SoTA methods Xgboost, LGBM and Catboost has given the best performances and have been performing even better after a optuna optimization pipeline. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
